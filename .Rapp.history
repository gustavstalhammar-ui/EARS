# Define the required packages#
packages <- c("broom", "crr", "cmprsk", "dplyr", "forcats", "ggplot2", #
              "ggridges", "gtable", "grid", "gridExtra", "purrr", #
              "survival", "survminer", "tidyr")#
#
# Install any missing packages#
new_packages <- packages[!(packages %in% installed.packages()[,"Package"])]#
if(length(new_packages)) install.packages(new_packages, dependencies = TRUE)#
#
# Load the packages#
lapply(packages, require, character.only = TRUE)
# Ensure required libraries are installed#
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")#
if (!requireNamespace("openxlsx", quietly = TRUE)) install.packages("openxlsx")#
#
library(dplyr)#
library(openxlsx)#
#
# Given data#
N1 <- 657  # Patients in brachytherapy group#
N2 <- 660  # Patients in enucleation group#
deaths_12yr_enucleation <- 98  # Observed deaths in enucleation#
deaths_12yr_brachytherapy <- 105  # Observed deaths in brachytherapy#
censored_12yr_enucleation <- 427  # Censored in enucleation#
censored_12yr_brachytherapy <- 412  # Censored in brachytherapy#
#
# Function to calculate additional deaths for a given target risk ratio (RR)#
# and direction ("brachytherapy" or "enucleation").#
# This version assumes that deaths in both arms may vary proportionally.#
calculate_required_deaths <- function(target_RR, direction) {#
  SE_RR <- (log(1.41) - log(0.81)) / (2 * 1.96)  # Standard error for RR#
  z_alpha <- 1.96  # For 95% CI#
  # Calculate bounds for log(RR)#
  lower_log_RR <- log(target_RR) - z_alpha * SE_RR#
  upper_log_RR <- log(target_RR) + z_alpha * SE_RR#
  if (direction == "brachytherapy") {#
    # Adjust deaths in the brachytherapy group, allowing proportional change in both arms#
    new_deaths <- target_RR * (deaths_12yr_enucleation / (N2 - censored_12yr_enucleation)) * (N1 - censored_12yr_brachytherapy)#
    lower_deaths <- exp(lower_log_RR) * (deaths_12yr_enucleation / (N2 - censored_12yr_enucleation)) * (N1 - censored_12yr_brachytherapy)#
    upper_deaths <- exp(upper_log_RR) * (deaths_12yr_enucleation / (N2 - censored_12yr_enucleation)) * (N1 - censored_12yr_brachytherapy)#
    additional_deaths <- new_deaths - deaths_12yr_brachytherapy#
    lower_additional <- lower_deaths - deaths_12yr_brachytherapy#
    upper_additional <- upper_deaths - deaths_12yr_brachytherapy#
  } else if (direction == "enucleation") {#
    # Adjust deaths in the enucleation group, allowing proportional change in both arms#
    new_deaths <- (deaths_12yr_brachytherapy * (N2 - censored_12yr_enucleation)) / (target_RR * (N1 - censored_12yr_brachytherapy))#
    lower_deaths <- (deaths_12yr_brachytherapy * (N2 - censored_12yr_enucleation)) / (exp(upper_log_RR) * (N1 - censored_12yr_brachytherapy))#
    upper_deaths <- (deaths_12yr_brachytherapy * (N2 - censored_12yr_enucleation)) / (exp(lower_log_RR) * (N1 - censored_12yr_brachytherapy))#
    additional_deaths <- new_deaths - deaths_12yr_enucleation#
    lower_additional <- lower_deaths - deaths_12yr_enucleation#
    upper_additional <- upper_deaths - deaths_12yr_enucleation#
  } else {#
    stop("Invalid direction. Use 'brachytherapy' or 'enucleation'.")#
  }#
  return(c(round(additional_deaths, 2), round(lower_additional, 2), round(upper_additional, 2)))#
}#
#
# Risk ratios to evaluate#
risk_ratios <- seq(0.5, 1.5, by = 0.05)#
#
# Generate data for sensitivity analysis#
results <- data.frame(#
  Risk_Ratio = risk_ratios,#
  Additional_Deaths_Brachytherapy = sapply(risk_ratios, function(x) calculate_required_deaths(x, "brachytherapy")[1]),#
  CI_Lower_Brachytherapy = sapply(risk_ratios, function(x) calculate_required_deaths(x, "brachytherapy")[2]),#
  CI_Upper_Brachytherapy = sapply(risk_ratios, function(x) calculate_required_deaths(x, "brachytherapy")[3]),#
  Additional_Deaths_Enucleation = sapply(risk_ratios, function(x) calculate_required_deaths(x, "enucleation")[1]),#
  CI_Lower_Enucleation = sapply(risk_ratios, function(x) calculate_required_deaths(x, "enucleation")[2]),#
  CI_Upper_Enucleation = sapply(risk_ratios, function(x) calculate_required_deaths(x, "enucleation")[3])#
)#
#
# Save to Excel (update the file path as needed)#
output_path <- "/Users/gussta/Desktop/Threshold_Analysis_Results_Aligned.xlsx"#
write.xlsx(results, output_path)#
#
cat("Sensitivity analysis results saved to:", output_path, "\n")
# Ensure required libraries are installed#
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")#
if (!requireNamespace("openxlsx", quietly = TRUE)) install.packages("openxlsx")#
#
library(dplyr)#
library(openxlsx)#
#
# Baseline data (from COMS)#
N1 <- 657  # Total patients in brachytherapy arm#
N2 <- 660  # Total patients in enucleation arm#
deaths_12yr_brachytherapy <- 105  # Observed deaths in brachytherapy arm#
deaths_12yr_enucleation <- 98     # Observed deaths in enucleation arm#
censored_12yr_brachytherapy <- 412  # Censored in brachytherapy arm#
censored_12yr_enucleation <- 427    # Censored in enucleation arm#
#
# Function to calculate required additional (or fewer) deaths when both arms are adjusted#
calculate_required_deaths_adj <- function(target_RR, direction, adj) {#
  # Adjusted death counts in both arms#
  adj_deaths_brachy <- deaths_12yr_brachytherapy + adj#
  adj_deaths_enucl <- deaths_12yr_enucleation + adj#
  # Standard error from original CI (using published upper and lower bounds)#
  SE_RR <- (log(1.41) - log(0.81)) / (2 * 1.96)#
  z_alpha <- 1.96#
  lower_log_RR <- log(target_RR) - z_alpha * SE_RR#
  upper_log_RR <- log(target_RR) + z_alpha * SE_RR#
  if (direction == "brachytherapy") {#
    # Calculate required deaths in brachytherapy arm given adjusted enucleation deaths#
    req_deaths <- target_RR * (adj_deaths_enucl / (N2 - censored_12yr_enucleation)) * (N1 - censored_12yr_brachytherapy)#
    req_lower  <- exp(lower_log_RR) * (adj_deaths_enucl / (N2 - censored_12yr_enucleation)) * (N1 - censored_12yr_brachytherapy)#
    req_upper  <- exp(upper_log_RR) * (adj_deaths_enucl / (N2 - censored_12yr_enucleation)) * (N1 - censored_12yr_brachytherapy)#
    additional <- req_deaths - adj_deaths_brachy#
    lower_add  <- req_lower - adj_deaths_brachy#
    upper_add  <- req_upper - adj_deaths_brachy#
  } else if (direction == "enucleation") {#
    # Calculate required deaths in enucleation arm given adjusted brachytherapy deaths#
    req_deaths <- (adj_deaths_brachy * (N2 - censored_12yr_enucleation)) / (target_RR * (N1 - censored_12yr_brachytherapy))#
    req_lower  <- (adj_deaths_brachy * (N2 - censored_12yr_enucleation)) / (exp(upper_log_RR) * (N1 - censored_12yr_brachytherapy))#
    req_upper  <- (adj_deaths_brachy * (N2 - censored_12yr_enucleation)) / (exp(lower_log_RR) * (N1 - censored_12yr_brachytherapy))#
    additional <- req_deaths - adj_deaths_enucl#
    lower_add  <- req_lower - adj_deaths_enucl#
    upper_add  <- req_upper - adj_deaths_enucl#
  } else {#
    stop("Invalid direction. Use 'brachytherapy' or 'enucleation'.")#
  }#
  return(c(round(additional, 2), round(lower_add, 2), round(upper_add, 2)))#
}#
#
# Define the adjustments and target risk ratios to test#
adjustments <- c(-40, -20, 20, 40)#
target_RRs <- c(0.70, 1.30)#
#
# Initialize a data frame to store results#
results_adj <- data.frame()#
#
# Loop over each combination of target risk ratio, adjustment, and direction#
for (rr in target_RRs) {#
  for (adj in adjustments) {#
    for (dir in c("brachytherapy", "enucleation")) {#
      res <- calculate_required_deaths_adj(rr, dir, adj)#
      temp <- data.frame(#
        Target_RR = rr,#
        Adjustment = adj,#
        Direction = dir,#
        Additional_Deaths = res[1],#
        CI_Lower = res[2],#
        CI_Upper = res[3]#
      )#
      results_adj <- rbind(results_adj, temp)#
    }#
  }#
}#
#
# Save the sensitivity analysis results to an Excel file on the desktop#
desktop_path <- "~/Desktop/Threshold_Analysis_Sensitivity.xlsx"  # Adjust path as needed#
write.xlsx(results_adj, desktop_path)#
#
cat("Sensitivity analysis results saved to:", desktop_path, "\n")
# Ensure required libraries are installed#
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")#
if (!requireNamespace("openxlsx", quietly = TRUE)) install.packages("openxlsx")#
#
library(dplyr)#
library(openxlsx)#
#
# Baseline data (from COMS)#
N1 <- 657  # Total patients in brachytherapy arm#
N2 <- 660  # Total patients in enucleation arm#
deaths_12yr_brachytherapy <- 105  # Observed deaths in brachytherapy arm#
deaths_12yr_enucleation <- 98     # Observed deaths in enucleation arm#
censored_12yr_brachytherapy <- 412  # Censored in brachytherapy arm#
censored_12yr_enucleation <- 427    # Censored in enucleation arm#
#
# Function to calculate required additional (or fewer) deaths when both arms are adjusted by a percentage#
# 'adj_pct' is the percentage change (e.g., -40, -20, 20, 40)#
calculate_required_deaths_pct <- function(target_RR, direction, adj_pct) {#
  # Adjusted death counts using percentage change#
  factor <- 1 + (adj_pct / 100)#
  adj_deaths_brachy <- deaths_12yr_brachytherapy * factor#
  adj_deaths_enucl <- deaths_12yr_enucleation * factor#
  # Standard error from original CI (using published upper and lower bounds)#
  SE_RR <- (log(1.41) - log(0.81)) / (2 * 1.96)#
  z_alpha <- 1.96#
  lower_log_RR <- log(target_RR) - z_alpha * SE_RR#
  upper_log_RR <- log(target_RR) + z_alpha * SE_RR#
  if (direction == "brachytherapy") {#
    # Calculate required deaths in brachytherapy arm given adjusted enucleation deaths#
    req_deaths <- target_RR * (adj_deaths_enucl / (N2 - censored_12yr_enucleation)) * (N1 - censored_12yr_brachytherapy)#
    req_lower  <- exp(lower_log_RR) * (adj_deaths_enucl / (N2 - censored_12yr_enucleation)) * (N1 - censored_12yr_brachytherapy)#
    req_upper  <- exp(upper_log_RR) * (adj_deaths_enucl / (N2 - censored_12yr_enucleation)) * (N1 - censored_12yr_brachytherapy)#
    additional <- req_deaths - adj_deaths_brachy#
    lower_add  <- req_lower - adj_deaths_brachy#
    upper_add  <- req_upper - adj_deaths_brachy#
  } else if (direction == "enucleation") {#
    # Calculate required deaths in enucleation arm given adjusted brachytherapy deaths#
    req_deaths <- (adj_deaths_brachy * (N2 - censored_12yr_enucleation)) / (target_RR * (N1 - censored_12yr_brachytherapy))#
    req_lower  <- (adj_deaths_brachy * (N2 - censored_12yr_enucleation)) / (exp(upper_log_RR) * (N1 - censored_12yr_brachytherapy))#
    req_upper  <- (adj_deaths_brachy * (N2 - censored_12yr_enucleation)) / (exp(lower_log_RR) * (N1 - censored_12yr_brachytherapy))#
    additional <- req_deaths - adj_deaths_enucl#
    lower_add  <- req_lower - adj_deaths_enucl#
    upper_add  <- req_upper - adj_deaths_enucl#
  } else {#
    stop("Invalid direction. Use 'brachytherapy' or 'enucleation'.")#
  }#
  return(c(round(additional, 2), round(lower_add, 2), round(upper_add, 2)))#
}#
#
# Define the percentage adjustments and target risk ratios to test#
adjustments_pct <- c(-40, -20, 20, 40)#
target_RRs <- c(0.70, 1.30)#
#
# Initialize a data frame to store results#
results_pct <- data.frame()#
#
# Loop over each combination of target risk ratio, adjustment percentage, and direction#
for (rr in target_RRs) {#
  for (adj in adjustments_pct) {#
    for (dir in c("brachytherapy", "enucleation")) {#
      res <- calculate_required_deaths_pct(rr, dir, adj)#
      temp <- data.frame(#
        Target_RR = rr,#
        Adjustment_pct = adj,#
        Direction = dir,#
        Additional_Deaths = res[1],#
        CI_Lower = res[2],#
        CI_Upper = res[3]#
      )#
      results_pct <- rbind(results_pct, temp)#
    }#
  }#
}#
#
# Save the sensitivity analysis results to an Excel file on the desktop#
desktop_path <- "~/Desktop/Threshold_Analysis_Sensitivity_Pct.xlsx"  # Adjust path as needed#
write.xlsx(results_pct, desktop_path)#
#
cat("Sensitivity analysis results saved to:", desktop_path, "\n")
# Check if the readxl package is installed; if not, install it#
if (!require("readxl")) {#
  install.packages("readxl")#
  library(readxl)#
} else {#
  library(readxl)#
}#
# Define the full path to your Excel file#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Evolutionary Origin of SNP rs12913832, Blue-Eye Mutation/Data/Data1.xlsx"#
# Read the data from the first sheet (adjust 'sheet' if necessary)#
df <- read_excel(file_path, sheet = 1)#
# Inspect the structure of the imported data#
str(df)#
# Rename columns for convenience (adjust if your file has different headers)#
# Expected order:#
# 1: Country#
# 2: Age-standardized ocular melanoma annual incidence/million#
# 3: Period#
# 4: Comment#
# 5: Source#
# 6: Approximate Latitude (text)#
# 7: Approximate Latitude, numerical#
# 8: Frequency of rs12913832 allele, % of population#
# 9: Latitude Source#
names(df) <- c("Country", "Incidence", "Period", "Comment", "Source",#
               "ApproxLat", "Latitude", "AlleleFreq", "LatSource")#
# Convert columns to numeric as needed#
df$Incidence <- as.numeric(df$Incidence)#
df$Latitude  <- as.numeric(df$Latitude)#
df$AlleleFreq <- as.numeric(df$AlleleFreq)#
# Plotting the relationships between incidence and predictors#
par(mfrow = c(1, 2))  # Two plots side by side#
# Plot: Incidence vs. Latitude#
plot(df$Latitude, df$Incidence,#
     main = "Incidence vs. Latitude",#
     xlab = "Latitude (°)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ Latitude, data = df), col = "red")#
# Plot: Incidence vs. Allele Frequency#
plot(df$AlleleFreq, df$Incidence,#
     main = "Incidence vs. Allele Frequency",#
     xlab = "Allele Frequency (%)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ AlleleFreq, data = df), col = "red")#
# Correlation tests for individual associations#
cor_lat <- cor.test(df$Incidence, df$Latitude)#
cor_allele <- cor.test(df$Incidence, df$AlleleFreq)#
print(cor_lat)#
print(cor_allele)#
# Fit simple linear regression models#
model_lat <- lm(Incidence ~ Latitude, data = df)#
model_allele <- lm(Incidence ~ AlleleFreq, data = df)#
# Fit a multiple regression model including both predictors#
model_both <- lm(Incidence ~ Latitude + AlleleFreq, data = df)#
# Display summaries of the models#
summary(model_lat)#
summary(model_allele)#
summary(model_both)#
# Standardize the variables to compare effect sizes directly#
df_std <- df#
df_std$Incidence <- scale(df_std$Incidence)#
df_std$Latitude  <- scale(df_std$Latitude)#
df_std$AlleleFreq <- scale(df_std$AlleleFreq)#
model_std <- lm(Incidence ~ Latitude + AlleleFreq, data = df_std)#
summary(model_std)
# Check if the readxl package is installed; if not, install it#
if (!require("readxl")) {#
  install.packages("readxl")#
  library(readxl)#
} else {#
  library(readxl)#
}#
#
# Check if the ppcor package is installed; if not, install it#
if (!require("ppcor")) {#
  install.packages("ppcor")#
  library(ppcor)#
} else {#
  library(ppcor)#
}#
#
# Check if the writexl package is installed; if not, install it#
if (!require("writexl")) {#
  install.packages("writexl")#
  library(writexl)#
} else {#
  library(writexl)#
}#
#
# Define the full path to your Excel file#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Evolutionary Origin of SNP rs12913832, Blue-Eye Mutation/Data/Data1.xlsx"#
#
# Read the data from the first sheet (adjust 'sheet' if necessary)#
df <- read_excel(file_path, sheet = 1)#
#
# Inspect the structure of the imported data#
str(df)#
#
# Rename columns for convenience (adjust if your file has different headers)#
# Expected order:#
# 1: Country#
# 2: Age-standardized ocular melanoma annual incidence/million#
# 3: Period#
# 4: Comment#
# 5: Source#
# 6: Approximate Latitude (text)#
# 7: Approximate Latitude, numerical#
# 8: Frequency of rs12913832 allele, % of population#
# 9: Latitude Source#
names(df) <- c("Country", "Incidence", "Period", "Comment", "Source",#
               "ApproxLat", "Latitude", "AlleleFreq", "LatSource")#
#
# Convert columns to numeric as needed#
df$Incidence <- as.numeric(df$Incidence)#
df$Latitude  <- as.numeric(df$Latitude)#
df$AlleleFreq <- as.numeric(df$AlleleFreq)#
#
# Plotting the relationships between incidence and predictors#
par(mfrow = c(1, 2))  # Two plots side by side#
#
# Plot: Incidence vs. Latitude#
plot(df$Latitude, df$Incidence,#
     main = "Incidence vs. Latitude",#
     xlab = "Latitude (°)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ Latitude, data = df), col = "red")#
#
# Plot: Incidence vs. Allele Frequency#
plot(df$AlleleFreq, df$Incidence,#
     main = "Incidence vs. Allele Frequency",#
     xlab = "Allele Frequency (%)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ AlleleFreq, data = df), col = "red")#
#
# Correlation tests for individual associations#
cor_lat <- cor.test(df$Incidence, df$Latitude)#
cor_allele <- cor.test(df$Incidence, df$AlleleFreq)#
print(cor_lat)#
print(cor_allele)#
#
# Fit simple linear regression models#
model_lat <- lm(Incidence ~ Latitude, data = df)#
model_allele <- lm(Incidence ~ AlleleFreq, data = df)#
#
# Fit a multiple regression model including both predictors#
model_both <- lm(Incidence ~ Latitude + AlleleFreq, data = df)#
#
# Display summaries of the models#
summary(model_lat)#
summary(model_allele)#
summary(model_both)#
#
# Standardize the variables to compare effect sizes directly#
df_std <- df#
df_std$Incidence <- scale(df_std$Incidence)#
df_std$Latitude  <- scale(df_std$Latitude)#
df_std$AlleleFreq <- scale(df_std$AlleleFreq)#
#
model_std <- lm(Incidence ~ Latitude + AlleleFreq, data = df_std)#
summary(model_std)#
#
# ----- Partial Correlations Analysis -----#
# Method 1: Compute partial correlations manually from the multiple regression model#
#
# Extract model summary and residual degrees of freedom#
model_summary <- summary(model_both)#
df_resid <- df.residual(model_both)#
#
# Calculate partial correlation for AlleleFreq controlling for Latitude#
t_allele <- model_summary$coefficients["AlleleFreq", "t value"]#
r_partial_allele <- sqrt(t_allele^2 / (t_allele^2 + df_resid))#
cat("Partial correlation for AlleleFreq (controlling for Latitude):", r_partial_allele, "\n")#
#
# Calculate partial correlation for Latitude controlling for AlleleFreq#
t_latitude <- model_summary$coefficients["Latitude", "t value"]#
r_partial_latitude <- sqrt(t_latitude^2 / (t_latitude^2 + df_resid))#
cat("Partial correlation for Latitude (controlling for AlleleFreq):", r_partial_latitude, "\n")#
#
# Method 2: Compute partial correlations using the ppcor package#
# Remove rows with missing values to avoid errors#
df_complete <- df[complete.cases(df[, c("Incidence", "Latitude", "AlleleFreq")]), ]#
partial_corr <- pcor(df_complete[, c("Incidence", "Latitude", "AlleleFreq")])#
print(partial_corr)#
#
# ----- Export data for GraphPad Prism -----#
# Create data frames for scatter plots#
df_latitude <- df[complete.cases(df[, c("Latitude", "Incidence")]), c("Latitude", "Incidence")]#
df_allele <- df[complete.cases(df[, c("AlleleFreq", "Incidence")]), c("AlleleFreq", "Incidence")]#
#
# Create a list of data frames to export as separate sheets#
export_list <- list("Latitude vs Incidence" = df_latitude,#
                    "Allele Frequency vs Incidence" = df_allele)#
#
# Define the export path for the xlsx file (adjust path if necessary)#
export_path <- "~/Desktop/graphpad_data.xlsx"#
#
# Write the list to an xlsx file on the desktop#
write_xlsx(export_list, path = export_path)#
#
cat("Data for scatter plots exported to:", export_path, "\n")
# Updated R script with sensitivity analyses#
#
# --- Load required packages ---#
if (!require("readxl")) {#
  install.packages("readxl")#
  library(readxl)#
} else {#
  library(readxl)#
}#
#
if (!require("ppcor")) {#
  install.packages("ppcor")#
  library(ppcor)#
} else {#
  library(ppcor)#
}#
#
if (!require("writexl")) {#
  install.packages("writexl")#
  library(writexl)#
} else {#
  library(writexl)#
}#
#
# --- Read in the data ---#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Evolutionary Origin of SNP rs12913832, Blue-Eye Mutation/Data/Data1.xlsx"#
df <- read_excel(file_path, sheet = 1)#
#
# Inspect the structure of the imported data#
str(df)#
#
# Rename columns for convenience (adjust if necessary)#
names(df) <- c("Country", "Incidence", "Period", "Comment", "Source",#
               "ApproxLat", "Latitude", "AlleleFreq", "LatSource")#
#
# Convert columns to numeric as needed#
df$Incidence <- as.numeric(df$Incidence)#
df$Latitude  <- as.numeric(df$Latitude)#
df$AlleleFreq <- as.numeric(df$AlleleFreq)#
#
# --- Update United States Allele Frequency ---#
# Compute the weighted average based on:#
# European Americans: 59% with 77%#
# Hispanic and Latino: 19% with 23%#
# Black or African Americans: 13% with 16%#
# (Total 91% of the US population)#
weighted_US <- (0.59 * 77 + 0.19 * 23 + 0.13 * 16) / 0.91#
cat("Weighted average allele frequency for the United States:", weighted_US, "\n")#
df$AlleleFreq[df$Country == "United States"] <- weighted_US#
#
# --- Plotting the relationships between incidence and predictors ---#
par(mfrow = c(1, 2))  # Two plots side by side#
#
# Plot: Incidence vs. Latitude#
plot(df$Latitude, df$Incidence,#
     main = "Incidence vs. Latitude",#
     xlab = "Latitude (°)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ Latitude, data = df), col = "red")#
#
# Plot: Incidence vs. Allele Frequency#
plot(df$AlleleFreq, df$Incidence,#
     main = "Incidence vs. Allele Frequency",#
     xlab = "Allele Frequency (%)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ AlleleFreq, data = df), col = "red")#
#
# --- Correlation tests for individual associations ---#
cor_lat <- cor.test(df$Incidence, df$Latitude)#
cor_allele <- cor.test(df$Incidence, df$AlleleFreq)#
print(cor_lat)#
print(cor_allele)#
#
# --- Fit simple and multiple regression models ---#
model_lat <- lm(Incidence ~ Latitude, data = df)#
model_allele <- lm(Incidence ~ AlleleFreq, data = df)#
model_both <- lm(Incidence ~ Latitude + AlleleFreq, data = df)#
#
# Display summaries of the models#
summary(model_lat)#
summary(model_allele)#
summary(model_both)#
#
# Standardize the variables to compare effect sizes directly#
df_std <- df#
df_std$Incidence <- scale(df_std$Incidence)#
df_std$Latitude  <- scale(df_std$Latitude)#
df_std$AlleleFreq <- scale(df_std$AlleleFreq)#
model_std <- lm(Incidence ~ Latitude + AlleleFreq, data = df_std)#
summary(model_std)#
#
# --- Partial Correlations Analysis ---#
# Method 1: Compute partial correlations manually from the multiple regression model#
model_summary <- summary(model_both)#
df_resid <- df.residual(model_both)#
#
t_allele <- model_summary$coefficients["AlleleFreq", "t value"]#
r_partial_allele <- sqrt(t_allele^2 / (t_allele^2 + df_resid))#
cat("Partial correlation for AlleleFreq (controlling for Latitude):", r_partial_allele, "\n")#
#
t_latitude <- model_summary$coefficients["Latitude", "t value"]#
r_partial_latitude <- sqrt(t_latitude^2 / (t_latitude^2 + df_resid))#
cat("Partial correlation for Latitude (controlling for AlleleFreq):", r_partial_latitude, "\n")#
#
# Method 2: Compute partial correlations using the ppcor package#
df_complete <- df[complete.cases(df[, c("Incidence", "Latitude", "AlleleFreq")]), ]#
partial_corr <- pcor(df_complete[, c("Incidence", "Latitude", "AlleleFreq")])#
print(partial_corr)#
#
# --- Export data for GraphPad Prism ---#
df_latitude <- df[complete.cases(df[, c("Latitude", "Incidence")]), c("Latitude", "Incidence")]#
df_allele <- df[complete.cases(df[, c("AlleleFreq", "Incidence")]), c("AlleleFreq", "Incidence")]#
export_list <- list("Latitude vs Incidence" = df_latitude,#
                    "Allele Frequency vs Incidence" = df_allele)#
export_path <- "~/Desktop/graphpad_data.xlsx"#
write_xlsx(export_list, path = export_path)#
cat("Data for scatter plots exported to:", export_path, "\n")#
#
# =============================#
# --- Sensitivity Analyses ---#
# =============================#
#
# Sensitivity Analysis 1: Leave-One-Out Analysis#
# For each country, exclude it from the dataset and fit the multiple regression model.#
# Store the coefficient estimates and P-values for both predictors.#
#
loo_results <- data.frame(Country = character(),#
                          Coef_AlleleFreq = numeric(),#
                          Pval_AlleleFreq = numeric(),#
                          Coef_Latitude = numeric(),#
                          Pval_Latitude = numeric(),#
                          stringsAsFactors = FALSE)#
#
countries <- unique(df$Country)#
for (c in countries) {#
  # Exclude the current country and remove incomplete cases#
  df_temp <- subset(df, Country != c & complete.cases(Incidence, Latitude, AlleleFreq))#
  # Proceed if a sufficient number of observations remain (e.g., n > 10)#
  if(nrow(df_temp) > 10) {#
    model_temp <- lm(Incidence ~ Latitude + AlleleFreq, data = df_temp)#
    coef_temp <- summary(model_temp)$coefficients#
    loo_results <- rbind(loo_results, data.frame(#
      Country = c,#
      Coef_AlleleFreq = coef_temp["AlleleFreq", "Estimate"],#
      Pval_AlleleFreq = coef_temp["AlleleFreq", "Pr(>|t|)"],#
      Coef_Latitude = coef_temp["Latitude", "Estimate"],#
      Pval_Latitude = coef_temp["Latitude", "Pr(>|t|)"]#
    ))#
  }#
}#
#
cat("\n--- Leave-One-Out Sensitivity Analysis Results ---\n")#
print(loo_results)#
#
# Sensitivity Analysis 2: Varying Allele Frequencies by ±20 Percentage Points#
# Create modified datasets with allele frequencies increased and decreased by 20 pp.#
# Ensure the modified frequencies remain within 0-100%.#
#
# Increase allele frequencies by 20 percentage points (max capped at 100)#
df_plus <- df#
df_plus$AlleleFreq <- pmin(df_plus$AlleleFreq + 20, 100)#
model_plus <- lm(Incidence ~ Latitude + AlleleFreq, data = df_plus)#
cat("\n--- Regression Summary with Allele Frequencies Increased by 20 pp ---\n")#
print(summary(model_plus))#
#
# Decrease allele frequencies by 20 percentage points (min capped at 0)#
df_minus <- df#
df_minus$AlleleFreq <- pmax(df_minus$AlleleFreq - 20, 0)#
model_minus <- lm(Incidence ~ Latitude + AlleleFreq, data = df_minus)#
cat("\n--- Regression Summary with Allele Frequencies Decreased by 20 pp ---\n")#
print(summary(model_minus))
# Updated R script with sensitivity analyses#
# --- Load required packages ---#
if (!require("readxl")) {#
  install.packages("readxl")#
  library(readxl)#
} else {#
  library(readxl)#
}#
if (!require("ppcor")) {#
  install.packages("ppcor")#
  library(ppcor)#
} else {#
  library(ppcor)#
}#
if (!require("writexl")) {#
  install.packages("writexl")#
  library(writexl)#
} else {#
  library(writexl)#
}#
# --- Read in the data ---#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Evolutionary Origin of SNP rs12913832, Blue-Eye Mutation/Data/Data1.xlsx"#
df <- read_excel(file_path, sheet = 1)#
# Inspect the structure of the imported data#
str(df)#
# Rename columns for convenience (adjust if necessary)#
names(df) <- c("Country", "Incidence", "Period", "Comment", "Source",#
               "ApproxLat", "Latitude", "AlleleFreq", "LatSource")#
# Convert columns to numeric as needed#
df$Incidence <- as.numeric(df$Incidence)#
df$Latitude  <- as.numeric(df$Latitude)#
df$AlleleFreq <- as.numeric(df$AlleleFreq)#
# --- Update United States Allele Frequency ---#
# Compute the weighted average based on:#
# European Americans: 59% with 77%#
# Hispanic and Latino: 19% with 23%#
# Black or African Americans: 13% with 16%#
# (Total 91% of the US population)#
weighted_US <- (0.59 * 77 + 0.19 * 23 + 0.13 * 16) / 0.91#
cat("Weighted average allele frequency for the United States:", weighted_US, "\n")#
df$AlleleFreq[df$Country == "United States"] <- weighted_US#
# --- Plotting the relationships between incidence and predictors ---#
par(mfrow = c(1, 2))  # Two plots side by side#
# Plot: Incidence vs. Latitude#
plot(df$Latitude, df$Incidence,#
     main = "Incidence vs. Latitude",#
     xlab = "Latitude (°)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ Latitude, data = df), col = "red")#
# Plot: Incidence vs. Allele Frequency#
plot(df$AlleleFreq, df$Incidence,#
     main = "Incidence vs. Allele Frequency",#
     xlab = "Allele Frequency (%)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ AlleleFreq, data = df), col = "red")#
# --- Correlation tests for individual associations ---#
cor_lat <- cor.test(df$Incidence, df$Latitude)#
cor_allele <- cor.test(df$Incidence, df$AlleleFreq)#
print(cor_lat)#
print(cor_allele)#
# --- Fit simple and multiple regression models ---#
model_lat <- lm(Incidence ~ Latitude, data = df)#
model_allele <- lm(Incidence ~ AlleleFreq, data = df)#
model_both <- lm(Incidence ~ Latitude + AlleleFreq, data = df)#
# Display summaries of the models#
summary(model_lat)#
summary(model_allele)#
summary(model_both)#
# Standardize the variables to compare effect sizes directly#
df_std <- df#
df_std$Incidence <- scale(df_std$Incidence)#
df_std$Latitude  <- scale(df_std$Latitude)#
df_std$AlleleFreq <- scale(df_std$AlleleFreq)#
model_std <- lm(Incidence ~ Latitude + AlleleFreq, data = df_std)#
summary(model_std)#
# --- Partial Correlations Analysis ---#
# Method 1: Compute partial correlations manually from the multiple regression model#
model_summary <- summary(model_both)#
df_resid <- df.residual(model_both)#
t_allele <- model_summary$coefficients["AlleleFreq", "t value"]#
r_partial_allele <- sqrt(t_allele^2 / (t_allele^2 + df_resid))#
cat("Partial correlation for AlleleFreq (controlling for Latitude):", r_partial_allele, "\n")#
t_latitude <- model_summary$coefficients["Latitude", "t value"]#
r_partial_latitude <- sqrt(t_latitude^2 / (t_latitude^2 + df_resid))#
cat("Partial correlation for Latitude (controlling for AlleleFreq):", r_partial_latitude, "\n")#
# Method 2: Compute partial correlations using the ppcor package#
df_complete <- df[complete.cases(df[, c("Incidence", "Latitude", "AlleleFreq")]), ]#
partial_corr <- pcor(df_complete[, c("Incidence", "Latitude", "AlleleFreq")])#
print(partial_corr)#
# --- Export data for GraphPad Prism ---#
df_latitude <- df[complete.cases(df[, c("Latitude", "Incidence")]), c("Latitude", "Incidence")]#
df_allele <- df[complete.cases(df[, c("AlleleFreq", "Incidence")]), c("AlleleFreq", "Incidence")]#
export_list <- list("Latitude vs Incidence" = df_latitude,#
                    "Allele Frequency vs Incidence" = df_allele)#
export_path <- "~/Desktop/graphpad_data.xlsx"#
write_xlsx(export_list, path = export_path)#
cat("Data for scatter plots exported to:", export_path, "\n")#
# =============================#
# --- Sensitivity Analyses ---#
# =============================#
# Sensitivity Analysis 1: Leave-One-Out Analysis#
# For each country, exclude it from the dataset and fit the multiple regression model.#
# Store the coefficient estimates and P-values for both predictors.#
loo_results <- data.frame(Country = character(),#
                          Coef_AlleleFreq = numeric(),#
                          Pval_AlleleFreq = numeric(),#
                          Coef_Latitude = numeric(),#
                          Pval_Latitude = numeric(),#
                          stringsAsFactors = FALSE)#
countries <- unique(df$Country)#
for (c in countries) {#
  # Exclude the current country and remove incomplete cases#
  df_temp <- subset(df, Country != c & complete.cases(Incidence, Latitude, AlleleFreq))#
  # Proceed if a sufficient number of observations remain (e.g., n > 10)#
  if(nrow(df_temp) > 10) {#
    model_temp <- lm(Incidence ~ Latitude + AlleleFreq, data = df_temp)#
    coef_temp <- summary(model_temp)$coefficients#
    loo_results <- rbind(loo_results, data.frame(#
      Country = c,#
      Coef_AlleleFreq = coef_temp["AlleleFreq", "Estimate"],#
      Pval_AlleleFreq = coef_temp["AlleleFreq", "Pr(>|t|)"],#
      Coef_Latitude = coef_temp["Latitude", "Estimate"],#
      Pval_Latitude = coef_temp["Latitude", "Pr(>|t|)"]#
    ))#
  }#
}#
cat("\n--- Leave-One-Out Sensitivity Analysis Results ---\n")#
print(loo_results)#
# Sensitivity Analysis 2: Varying Allele Frequencies by ±20 Percentage Points#
# Create modified datasets with allele frequencies increased and decreased by 20 pp.#
# Ensure the modified frequencies remain within 0-100%.#
# Increase allele frequencies by 20 percentage points (max capped at 100)#
df_plus <- df#
df_plus$AlleleFreq <- pmin(df_plus$AlleleFreq + 20, 100)#
model_plus <- lm(Incidence ~ Latitude + AlleleFreq, data = df_plus)#
cat("\n--- Regression Summary with Allele Frequencies Increased by 20 pp ---\n")#
print(summary(model_plus))#
# Decrease allele frequencies by 20 percentage points (min capped at 0)#
df_minus <- df#
df_minus$AlleleFreq <- pmax(df_minus$AlleleFreq - 20, 0)#
model_minus <- lm(Incidence ~ Latitude + AlleleFreq, data = df_minus)#
cat("\n--- Regression Summary with Allele Frequencies Decreased by 20 pp ---\n")#
print(summary(model_minus))
# Updated R script with sensitivity analyses#
# --- Load required packages ---#
if (!require("readxl")) {#
  install.packages("readxl")#
  library(readxl)#
} else {#
  library(readxl)#
}#
if (!require("ppcor")) {#
  install.packages("ppcor")#
  library(ppcor)#
} else {#
  library(ppcor)#
}#
if (!require("writexl")) {#
  install.packages("writexl")#
  library(writexl)#
} else {#
  library(writexl)#
}#
# --- Read in the data ---#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Evolutionary Origin of SNP rs12913832, Blue-Eye Mutation/Data/Data1.xlsx"#
df <- read_excel(file_path, sheet = 1)#
# Inspect the structure of the imported data#
str(df)#
# Rename columns for convenience (adjust if necessary)#
names(df) <- c("Country", "Incidence", "Period", "Comment", "Source",#
               "ApproxLat", "Latitude", "AlleleFreq", "LatSource")#
# Convert columns to numeric as needed#
df$Incidence <- as.numeric(df$Incidence)#
df$Latitude  <- as.numeric(df$Latitude)#
df$AlleleFreq <- as.numeric(df$AlleleFreq)#
# --- Update United States Allele Frequency ---#
# Compute the weighted average based on:#
# European Americans: 59% with 77%#
# Hispanic and Latino: 19% with 23%#
# Black or African Americans: 13% with 16%#
# (Total 91% of the US population)#
weighted_US <- (0.59 * 77 + 0.19 * 23 + 0.13 * 16) / 0.91#
cat("Weighted average allele frequency for the United States:", weighted_US, "\n")#
df$AlleleFreq[df$Country == "United States"] <- weighted_US#
# --- Plotting the relationships between incidence and predictors ---#
par(mfrow = c(1, 2))  # Two plots side by side#
# Plot: Incidence vs. Latitude#
plot(df$Latitude, df$Incidence,#
     main = "Incidence vs. Latitude",#
     xlab = "Latitude (°)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ Latitude, data = df), col = "red")#
# Plot: Incidence vs. Allele Frequency#
plot(df$AlleleFreq, df$Incidence,#
     main = "Incidence vs. Allele Frequency",#
     xlab = "Allele Frequency (%)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ AlleleFreq, data = df), col = "red")#
# --- Correlation tests for individual associations ---#
cor_lat <- cor.test(df$Incidence, df$Latitude)#
cor_allele <- cor.test(df$Incidence, df$AlleleFreq)#
print(cor_lat)#
print(cor_allele)#
# --- Fit simple and multiple regression models ---#
model_lat <- lm(Incidence ~ Latitude, data = df)#
model_allele <- lm(Incidence ~ AlleleFreq, data = df)#
model_both <- lm(Incidence ~ Latitude + AlleleFreq, data = df)#
# Display summaries of the models#
summary(model_lat)#
summary(model_allele)#
summary(model_both)#
# Standardize the variables to compare effect sizes directly#
df_std <- df#
df_std$Incidence <- scale(df_std$Incidence)#
df_std$Latitude  <- scale(df_std$Latitude)#
df_std$AlleleFreq <- scale(df_std$AlleleFreq)#
model_std <- lm(Incidence ~ Latitude + AlleleFreq, data = df_std)#
summary(model_std)#
# --- Partial Correlations Analysis ---#
# Method 1: Compute partial correlations manually from the multiple regression model#
model_summary <- summary(model_both)#
df_resid <- df.residual(model_both)#
t_allele <- model_summary$coefficients["AlleleFreq", "t value"]#
r_partial_allele <- sqrt(t_allele^2 / (t_allele^2 + df_resid))#
cat("Partial correlation for AlleleFreq (controlling for Latitude):", r_partial_allele, "\n")#
t_latitude <- model_summary$coefficients["Latitude", "t value"]#
r_partial_latitude <- sqrt(t_latitude^2 / (t_latitude^2 + df_resid))#
cat("Partial correlation for Latitude (controlling for AlleleFreq):", r_partial_latitude, "\n")#
# Method 2: Compute partial correlations using the ppcor package#
df_complete <- df[complete.cases(df[, c("Incidence", "Latitude", "AlleleFreq")]), ]#
partial_corr <- pcor(df_complete[, c("Incidence", "Latitude", "AlleleFreq")])#
print(partial_corr)#
# --- Export data for GraphPad Prism ---#
df_latitude <- df[complete.cases(df[, c("Latitude", "Incidence")]), c("Latitude", "Incidence")]#
df_allele <- df[complete.cases(df[, c("AlleleFreq", "Incidence")]), c("AlleleFreq", "Incidence")]#
export_list <- list("Latitude vs Incidence" = df_latitude,#
                    "Allele Frequency vs Incidence" = df_allele)#
export_path <- "~/Desktop/graphpad_data.xlsx"#
write_xlsx(export_list, path = export_path)#
cat("Data for scatter plots exported to:", export_path, "\n")#
# =============================#
# --- Sensitivity Analyses ---#
# =============================#
# Sensitivity Analysis 1: Leave-One-Out Analysis#
# For each country, exclude it from the dataset and fit the multiple regression model.#
# Store the coefficient estimates and P-values for both predictors.#
loo_results <- data.frame(Country = character(),#
                          Coef_AlleleFreq = numeric(),#
                          Pval_AlleleFreq = numeric(),#
                          Coef_Latitude = numeric(),#
                          Pval_Latitude = numeric(),#
                          stringsAsFactors = FALSE)#
countries <- unique(df$Country)#
for (c in countries) {#
  # Exclude the current country and remove incomplete cases#
  df_temp <- subset(df, Country != c & complete.cases(Incidence, Latitude, AlleleFreq))#
  # Proceed if a sufficient number of observations remain (e.g., n > 10)#
  if(nrow(df_temp) > 10) {#
    model_temp <- lm(Incidence ~ Latitude + AlleleFreq, data = df_temp)#
    coef_temp <- summary(model_temp)$coefficients#
    loo_results <- rbind(loo_results, data.frame(#
      Country = c,#
      Coef_AlleleFreq = coef_temp["AlleleFreq", "Estimate"],#
      Pval_AlleleFreq = coef_temp["AlleleFreq", "Pr(>|t|)"],#
      Coef_Latitude = coef_temp["Latitude", "Estimate"],#
      Pval_Latitude = coef_temp["Latitude", "Pr(>|t|)"]#
    ))#
  }#
}#
cat("\n--- Leave-One-Out Sensitivity Analysis Results ---\n")#
print(loo_results)#
# Sensitivity Analysis 2: Random Variation in Allele Frequencies by ±40 Percentage Points#
# For each country, add a random adjustment sampled from a uniform distribution between -40 and +40,#
# and then cap the values between 0 and 100.#
set.seed(123)  # for reproducibility#
df_random <- df#
random_adjustments <- runif(nrow(df_random), min = -40, max = 40)#
df_random$AlleleFreq <- pmin(pmax(df_random$AlleleFreq + random_adjustments, 0), 100)#
model_random <- lm(Incidence ~ Latitude + AlleleFreq, data = df_random)#
cat("\n--- Regression Summary with Allele Frequencies Randomly Varied by ±40 pp ---\n")#
print(summary(model_random))
# Updated R script with sensitivity analyses#
#
# --- Load required packages ---#
if (!require("readxl")) {#
  install.packages("readxl")#
  library(readxl)#
} else {#
  library(readxl)#
}#
#
if (!require("ppcor")) {#
  install.packages("ppcor")#
  library(ppcor)#
} else {#
  library(ppcor)#
}#
#
if (!require("writexl")) {#
  install.packages("writexl")#
  library(writexl)#
} else {#
  library(writexl)#
}#
#
# --- Read in the data ---#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Evolutionary Origin of SNP rs12913832, Blue-Eye Mutation/Data/Data1.xlsx"#
df <- read_excel(file_path, sheet = 1)#
#
# Inspect the structure of the imported data#
str(df)#
#
# Rename columns for convenience (adjust if necessary)#
names(df) <- c("Country", "Incidence", "Period", "Comment", "Source",#
               "ApproxLat", "Latitude", "AlleleFreq", "LatSource")#
#
# Convert columns to numeric as needed#
df$Incidence <- as.numeric(df$Incidence)#
df$Latitude  <- as.numeric(df$Latitude)#
df$AlleleFreq <- as.numeric(df$AlleleFreq)#
#
# --- Update United States Allele Frequency ---#
# Compute the weighted average based on:#
# European Americans: 59% with 77%#
# Hispanic and Latino: 19% with 23%#
# Black or African Americans: 13% with 16%#
# (Total 91% of the US population)#
weighted_US <- (0.59 * 77 + 0.19 * 23 + 0.13 * 16) / 0.91#
cat("Weighted average allele frequency for the United States:", weighted_US, "\n")#
df$AlleleFreq[df$Country == "United States"] <- weighted_US#
#
# --- Plotting the relationships between incidence and predictors ---#
par(mfrow = c(1, 2))  # Two plots side by side#
#
# Plot: Incidence vs. Latitude#
plot(df$Latitude, df$Incidence,#
     main = "Incidence vs. Latitude",#
     xlab = "Latitude (°)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ Latitude, data = df), col = "red")#
#
# Plot: Incidence vs. Allele Frequency#
plot(df$AlleleFreq, df$Incidence,#
     main = "Incidence vs. Allele Frequency",#
     xlab = "Allele Frequency (%)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ AlleleFreq, data = df), col = "red")#
#
# --- Correlation tests for individual associations ---#
cor_lat <- cor.test(df$Incidence, df$Latitude)#
cor_allele <- cor.test(df$Incidence, df$AlleleFreq)#
print(cor_lat)#
print(cor_allele)#
#
# --- Fit simple and multiple regression models ---#
model_lat <- lm(Incidence ~ Latitude, data = df)#
model_allele <- lm(Incidence ~ AlleleFreq, data = df)#
model_both <- lm(Incidence ~ Latitude + AlleleFreq, data = df)#
#
# Display summaries of the models#
summary(model_lat)#
summary(model_allele)#
summary(model_both)#
#
# Standardize the variables to compare effect sizes directly#
df_std <- df#
df_std$Incidence <- scale(df_std$Incidence)#
df_std$Latitude  <- scale(df_std$Latitude)#
df_std$AlleleFreq <- scale(df_std$AlleleFreq)#
model_std <- lm(Incidence ~ Latitude + AlleleFreq, data = df_std)#
summary(model_std)#
#
# --- Partial Correlations Analysis ---#
# Method 1: Compute partial correlations manually from the multiple regression model#
model_summary <- summary(model_both)#
df_resid <- df.residual(model_both)#
#
t_allele <- model_summary$coefficients["AlleleFreq", "t value"]#
r_partial_allele <- sqrt(t_allele^2 / (t_allele^2 + df_resid))#
cat("Partial correlation for AlleleFreq (controlling for Latitude):", r_partial_allele, "\n")#
#
t_latitude <- model_summary$coefficients["Latitude", "t value"]#
r_partial_latitude <- sqrt(t_latitude^2 / (t_latitude^2 + df_resid))#
cat("Partial correlation for Latitude (controlling for AlleleFreq):", r_partial_latitude, "\n")#
#
# Method 2: Compute partial correlations using the ppcor package#
df_complete <- df[complete.cases(df[, c("Incidence", "Latitude", "AlleleFreq")]), ]#
partial_corr <- pcor(df_complete[, c("Incidence", "Latitude", "AlleleFreq")])#
print(partial_corr)#
#
# --- Export data for GraphPad Prism ---#
df_latitude <- df[complete.cases(df[, c("Latitude", "Incidence")]), c("Latitude", "Incidence")]#
df_allele <- df[complete.cases(df[, c("AlleleFreq", "Incidence")]), c("AlleleFreq", "Incidence")]#
export_list <- list("Latitude vs Incidence" = df_latitude,#
                    "Allele Frequency vs Incidence" = df_allele)#
export_path <- "~/Desktop/graphpad_data.xlsx"#
write_xlsx(export_list, path = export_path)#
cat("Data for scatter plots exported to:", export_path, "\n")#
#
# =============================#
# --- Sensitivity Analyses ---#
# =============================#
#
# Sensitivity Analysis 1: Leave-One-Out Analysis#
# For each country, exclude it from the dataset and fit the multiple regression model.#
# Store the coefficient estimates and P-values for both predictors.#
#
loo_results <- data.frame(Country = character(),#
                          Coef_AlleleFreq = numeric(),#
                          Pval_AlleleFreq = numeric(),#
                          Coef_Latitude = numeric(),#
                          Pval_Latitude = numeric(),#
                          stringsAsFactors = FALSE)#
#
countries <- unique(df$Country)#
for (c in countries) {#
  # Exclude the current country and remove incomplete cases#
  df_temp <- subset(df, Country != c & complete.cases(Incidence, Latitude, AlleleFreq))#
  # Proceed if a sufficient number of observations remain (e.g., n > 10)#
  if(nrow(df_temp) > 10) {#
    model_temp <- lm(Incidence ~ Latitude + AlleleFreq, data = df_temp)#
    coef_temp <- summary(model_temp)$coefficients#
    loo_results <- rbind(loo_results, data.frame(#
      Country = c,#
      Coef_AlleleFreq = coef_temp["AlleleFreq", "Estimate"],#
      Pval_AlleleFreq = coef_temp["AlleleFreq", "Pr(>|t|)"],#
      Coef_Latitude = coef_temp["Latitude", "Estimate"],#
      Pval_Latitude = coef_temp["Latitude", "Pr(>|t|)"]#
    ))#
  }#
}#
#
cat("\n--- Leave-One-Out Sensitivity Analysis Results ---\n")#
print(loo_results)#
#
# Sensitivity Analysis 2: Random Variation in Allele Frequencies by ±40 Percentage Points (10,000 iterations)#
# In each iteration, each country's allele frequency is randomly adjusted by a value drawn from#
# a uniform distribution between -40 and +40 percentage points (capped between 0 and 100). #
# We then fit the multiple regression model and record whether the associations for AlleleFreq and Latitude are significant (P < 0.05).#
#
set.seed(123)  # for reproducibility#
n_iterations <- 10000#
pvals_allele <- numeric(n_iterations)#
pvals_latitude <- numeric(n_iterations)#
#
for(i in 1:n_iterations){#
  df_random_iter <- df  # start with the original data#
  random_adjustments <- runif(nrow(df_random_iter), min = -40, max = 40)#
  df_random_iter$AlleleFreq <- pmin(pmax(df_random_iter$AlleleFreq + random_adjustments, 0), 100)#
  model_iter <- lm(Incidence ~ Latitude + AlleleFreq, data = df_random_iter)#
  summ_iter <- summary(model_iter)#
  pvals_allele[i] <- summ_iter$coefficients["AlleleFreq", "Pr(>|t|)"]#
  pvals_latitude[i] <- summ_iter$coefficients["Latitude", "Pr(>|t|)"]#
}#
#
sig_allele <- sum(pvals_allele < 0.05, na.rm = TRUE)#
sig_latitude <- sum(pvals_latitude < 0.05, na.rm = TRUE)#
#
cat("\n--- Random Variation Sensitivity Analysis (10,000 iterations) ---\n")#
cat("Out of", n_iterations, "iterations:\n")#
cat("The association for AlleleFreq was significant (P < 0.05) in", sig_allele, "iterations.\n")#
cat("The association for Latitude was significant (P < 0.05) in", sig_latitude, "iterations.\n")
# Updated R script with multivariate sensitivity analyses#
#
# --- Load required packages ---#
if (!require("readxl")) {#
  install.packages("readxl")#
  library(readxl)#
} else {#
  library(readxl)#
}#
#
if (!require("ppcor")) {#
  install.packages("ppcor")#
  library(ppcor)#
} else {#
  library(ppcor)#
}#
#
if (!require("writexl")) {#
  install.packages("writexl")#
  library(writexl)#
} else {#
  library(writexl)#
}#
#
# --- Read in the data ---#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Evolutionary Origin of SNP rs12913832, Blue-Eye Mutation/Data/Data1.xlsx"#
df <- read_excel(file_path, sheet = 1)#
#
# Inspect the structure of the imported data#
str(df)#
#
# Rename columns for convenience (adjust if necessary)#
names(df) <- c("Country", "Incidence", "Period", "Comment", "Source",#
               "ApproxLat", "Latitude", "AlleleFreq", "LatSource")#
#
# Convert columns to numeric as needed#
df$Incidence <- as.numeric(df$Incidence)#
df$Latitude  <- as.numeric(df$Latitude)#
df$AlleleFreq <- as.numeric(df$AlleleFreq)#
#
# --- Update United States Allele Frequency ---#
# Compute the weighted average based on:#
# European Americans: 59% with 77%#
# Hispanic and Latino: 19% with 23%#
# Black or African Americans: 13% with 16%#
# (Total 91% of the US population)#
weighted_US <- (0.59 * 77 + 0.19 * 23 + 0.13 * 16) / 0.91#
cat("Weighted average allele frequency for the United States:", weighted_US, "\n")#
df$AlleleFreq[df$Country == "United States"] <- weighted_US#
#
# --- Plotting the relationships between incidence and predictors ---#
par(mfrow = c(1, 2))  # Two plots side by side#
#
# Plot: Incidence vs. Latitude#
plot(df$Latitude, df$Incidence,#
     main = "Incidence vs. Latitude",#
     xlab = "Latitude (°)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ Latitude, data = df), col = "red")#
#
# Plot: Incidence vs. Allele Frequency#
plot(df$AlleleFreq, df$Incidence,#
     main = "Incidence vs. Allele Frequency",#
     xlab = "Allele Frequency (%)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ AlleleFreq, data = df), col = "red")#
#
# --- Correlation tests for individual associations ---#
cor_lat <- cor.test(df$Incidence, df$Latitude)#
cor_allele <- cor.test(df$Incidence, df$AlleleFreq)#
print(cor_lat)#
print(cor_allele)#
#
# --- Fit simple and multiple regression models ---#
# Note: The following models are multivariate when both predictors are included.#
model_lat <- lm(Incidence ~ Latitude, data = df)#
model_allele <- lm(Incidence ~ AlleleFreq, data = df)#
model_both <- lm(Incidence ~ Latitude + AlleleFreq, data = df)#
#
# Display summaries of the models#
summary(model_lat)#
summary(model_allele)#
summary(model_both)#
#
# Standardize the variables to compare effect sizes directly#
df_std <- df#
df_std$Incidence <- scale(df_std$Incidence)#
df_std$Latitude  <- scale(df_std$Latitude)#
df_std$AlleleFreq <- scale(df_std$AlleleFreq)#
model_std <- lm(Incidence ~ Latitude + AlleleFreq, data = df_std)#
summary(model_std)#
#
# --- Partial Correlations Analysis ---#
# Method 1: Compute partial correlations manually from the multiple regression model#
model_summary <- summary(model_both)#
df_resid <- df.residual(model_both)#
#
t_allele <- model_summary$coefficients["AlleleFreq", "t value"]#
r_partial_allele <- sqrt(t_allele^2 / (t_allele^2 + df_resid))#
cat("Partial correlation for AlleleFreq (controlling for Latitude):", r_partial_allele, "\n")#
#
t_latitude <- model_summary$coefficients["Latitude", "t value"]#
r_partial_latitude <- sqrt(t_latitude^2 / (t_latitude^2 + df_resid))#
cat("Partial correlation for Latitude (controlling for AlleleFreq):", r_partial_latitude, "\n")#
#
# Method 2: Compute partial correlations using the ppcor package#
df_complete <- df[complete.cases(df[, c("Incidence", "Latitude", "AlleleFreq")]), ]#
partial_corr <- pcor(df_complete[, c("Incidence", "Latitude", "AlleleFreq")])#
print(partial_corr)#
#
# --- Export data for GraphPad Prism ---#
df_latitude <- df[complete.cases(df[, c("Latitude", "Incidence")]), c("Latitude", "Incidence")]#
df_allele <- df[complete.cases(df[, c("AlleleFreq", "Incidence")]), c("AlleleFreq", "Incidence")]#
export_list <- list("Latitude vs Incidence" = df_latitude,#
                    "Allele Frequency vs Incidence" = df_allele)#
export_path <- "~/Desktop/graphpad_data.xlsx"#
write_xlsx(export_list, path = export_path)#
cat("Data for scatter plots exported to:", export_path, "\n")#
#
# =============================#
# --- Sensitivity Analyses ---#
# =============================#
#
# Sensitivity Analysis 1: Leave-One-Out Analysis#
# Here, we iteratively exclude each country and refit the multiple regression model #
# (i.e., Incidence ~ Latitude + AlleleFreq). This analysis is multivariate because both predictors#
# are included in each model. We record the coefficient estimates and P-values for both predictors.#
loo_results <- data.frame(Country = character(),#
                          Coef_AlleleFreq = numeric(),#
                          Pval_AlleleFreq = numeric(),#
                          Coef_Latitude = numeric(),#
                          Pval_Latitude = numeric(),#
                          stringsAsFactors = FALSE)#
#
countries <- unique(df$Country)#
for (c in countries) {#
  # Exclude the current country and remove incomplete cases#
  df_temp <- subset(df, Country != c & complete.cases(Incidence, Latitude, AlleleFreq))#
  # Proceed if a sufficient number of observations remain (e.g., n > 10)#
  if(nrow(df_temp) > 10) {#
    model_temp <- lm(Incidence ~ Latitude + AlleleFreq, data = df_temp)#
    coef_temp <- summary(model_temp)$coefficients#
    loo_results <- rbind(loo_results, data.frame(#
      Country = c,#
      Coef_AlleleFreq = coef_temp["AlleleFreq", "Estimate"],#
      Pval_AlleleFreq = coef_temp["AlleleFreq", "Pr(>|t|)"],#
      Coef_Latitude = coef_temp["Latitude", "Estimate"],#
      Pval_Latitude = coef_temp["Latitude", "Pr(>|t|)"]#
    ))#
  }#
}#
#
cat("\n--- Leave-One-Out Sensitivity Analysis Results ---\n")#
print(loo_results)#
#
# Sensitivity Analysis 2: Random Variation in Allele Frequencies by ±40 Percentage Points (10,000 iterations)#
# In each iteration, we randomly adjust each country's allele frequency by a value drawn from a uniform#
# distribution between -40 and +40 percentage points (with values capped between 0 and 100). #
# We then fit the multiple regression model (multivariate: Incidence ~ Latitude + AlleleFreq) and record#
# whether the associations for AlleleFreq and Latitude are significant (P < 0.05).#
set.seed(123)  # for reproducibility#
n_iterations <- 10000#
pvals_allele <- numeric(n_iterations)#
pvals_latitude <- numeric(n_iterations)#
#
for(i in 1:n_iterations){#
  df_random_iter <- df  # start with the original data#
  random_adjustments <- runif(nrow(df_random_iter), min = -40, max = 40)#
  df_random_iter$AlleleFreq <- pmin(pmax(df_random_iter$AlleleFreq + random_adjustments, 0), 100)#
  model_iter <- lm(Incidence ~ Latitude + AlleleFreq, data = df_random_iter)#
  summ_iter <- summary(model_iter)#
  pvals_allele[i] <- summ_iter$coefficients["AlleleFreq", "Pr(>|t|)"]#
  pvals_latitude[i] <- summ_iter$coefficients["Latitude", "Pr(>|t|)"]#
}#
#
sig_allele <- sum(pvals_allele < 0.05, na.rm = TRUE)#
sig_latitude <- sum(pvals_latitude < 0.05, na.rm = TRUE)#
#
cat("\n--- Random Variation Sensitivity Analysis (10,000 iterations) ---\n")#
cat("Out of", n_iterations, "iterations:\n")#
cat("The association for AlleleFreq was significant (P < 0.05) in", sig_allele, "iterations.\n")#
cat("The association for Latitude was significant (P < 0.05) in", sig_latitude, "iterations.\n")
# Sensitivity Analysis 2: Random Variation in Allele Frequencies by ±25 Percentage Points (10,000 iterations)#
# In each iteration, we randomly adjust each country's allele frequency by a value drawn from a uniform#
# distribution between -25 and +25 percentage points (with values capped between 0 and 100). #
# We then fit the multiple regression model (multivariate: Incidence ~ Latitude + AlleleFreq) and record#
# whether the associations for AlleleFreq and Latitude are significant (P < 0.05).#
#
set.seed(123)  # for reproducibility#
n_iterations <- 10000#
pvals_allele <- numeric(n_iterations)#
pvals_latitude <- numeric(n_iterations)#
#
for(i in 1:n_iterations){#
  df_random_iter <- df  # start with the original data#
  random_adjustments <- runif(nrow(df_random_iter), min = -25, max = 25)#
  df_random_iter$AlleleFreq <- pmin(pmax(df_random_iter$AlleleFreq + random_adjustments, 0), 100)#
  model_iter <- lm(Incidence ~ Latitude + AlleleFreq, data = df_random_iter)#
  summ_iter <- summary(model_iter)#
  pvals_allele[i] <- summ_iter$coefficients["AlleleFreq", "Pr(>|t|)"]#
  pvals_latitude[i] <- summ_iter$coefficients["Latitude", "Pr(>|t|)"]#
}#
#
sig_allele <- sum(pvals_allele < 0.05, na.rm = TRUE)#
sig_latitude <- sum(pvals_latitude < 0.05, na.rm = TRUE)#
#
cat("\n--- Random Variation Sensitivity Analysis (10,000 iterations) ---\n")#
cat("Out of", n_iterations, "iterations:\n")#
cat("The association for AlleleFreq was significant (P < 0.05) in", sig_allele, "iterations.\n")#
cat("The association for Latitude was significant (P < 0.05) in", sig_latitude, "iterations.\n")
# Updated R script with sensitivity analyses (using ±25 percentage points for random variation)#
#
# --- Load required packages ---#
if (!require("readxl")) {#
  install.packages("readxl")#
  library(readxl)#
} else {#
  library(readxl)#
}#
#
if (!require("ppcor")) {#
  install.packages("ppcor")#
  library(ppcor)#
} else {#
  library(ppcor)#
}#
#
if (!require("writexl")) {#
  install.packages("writexl")#
  library(writexl)#
} else {#
  library(writexl)#
}#
#
# --- Read in the data ---#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Evolutionary Origin of SNP rs12913832, Blue-Eye Mutation/Data/Data1.xlsx"#
df <- read_excel(file_path, sheet = 1)#
#
# Inspect the structure of the imported data#
str(df)#
#
# Rename columns for convenience (adjust if necessary)#
names(df) <- c("Country", "Incidence", "Period", "Comment", "Source",#
               "ApproxLat", "Latitude", "AlleleFreq", "LatSource")#
#
# Convert columns to numeric as needed#
df$Incidence <- as.numeric(df$Incidence)#
df$Latitude  <- as.numeric(df$Latitude)#
df$AlleleFreq <- as.numeric(df$AlleleFreq)#
#
# --- Update United States Allele Frequency ---#
# Compute the weighted average based on:#
# European Americans: 59% with 77%#
# Hispanic and Latino: 19% with 23%#
# Black or African Americans: 13% with 16%#
# (Total 91% of the US population)#
weighted_US <- (0.59 * 77 + 0.19 * 23 + 0.13 * 16) / 0.91#
cat("Weighted average allele frequency for the United States:", weighted_US, "\n")#
df$AlleleFreq[df$Country == "United States"] <- weighted_US#
#
# --- Plotting the relationships between incidence and predictors ---#
par(mfrow = c(1, 2))  # Two plots side by side#
#
# Plot: Incidence vs. Latitude#
plot(df$Latitude, df$Incidence,#
     main = "Incidence vs. Latitude",#
     xlab = "Latitude (°)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ Latitude, data = df), col = "red")#
#
# Plot: Incidence vs. Allele Frequency#
plot(df$AlleleFreq, df$Incidence,#
     main = "Incidence vs. Allele Frequency",#
     xlab = "Allele Frequency (%)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ AlleleFreq, data = df), col = "red")#
#
# --- Correlation tests for individual associations ---#
cor_lat <- cor.test(df$Incidence, df$Latitude)#
cor_allele <- cor.test(df$Incidence, df$AlleleFreq)#
print(cor_lat)#
print(cor_allele)#
#
# --- Fit simple and multiple regression models ---#
model_lat <- lm(Incidence ~ Latitude, data = df)#
model_allele <- lm(Incidence ~ AlleleFreq, data = df)#
model_both <- lm(Incidence ~ Latitude + AlleleFreq, data = df)#
#
# Display summaries of the models#
summary(model_lat)#
summary(model_allele)#
summary(model_both)#
#
# Standardize the variables to compare effect sizes directly#
df_std <- df#
df_std$Incidence <- scale(df_std$Incidence)#
df_std$Latitude  <- scale(df_std$Latitude)#
df_std$AlleleFreq <- scale(df_std$AlleleFreq)#
model_std <- lm(Incidence ~ Latitude + AlleleFreq, data = df_std)#
summary(model_std)#
#
# --- Partial Correlations Analysis ---#
# Method 1: Compute partial correlations manually from the multiple regression model#
model_summary <- summary(model_both)#
df_resid <- df.residual(model_both)#
#
t_allele <- model_summary$coefficients["AlleleFreq", "t value"]#
r_partial_allele <- sqrt(t_allele^2 / (t_allele^2 + df_resid))#
cat("Partial correlation for AlleleFreq (controlling for Latitude):", r_partial_allele, "\n")#
#
t_latitude <- model_summary$coefficients["Latitude", "t value"]#
r_partial_latitude <- sqrt(t_latitude^2 / (t_latitude^2 + df_resid))#
cat("Partial correlation for Latitude (controlling for AlleleFreq):", r_partial_latitude, "\n")#
#
# Method 2: Compute partial correlations using the ppcor package#
df_complete <- df[complete.cases(df[, c("Incidence", "Latitude", "AlleleFreq")]), ]#
partial_corr <- pcor(df_complete[, c("Incidence", "Latitude", "AlleleFreq")])#
print(partial_corr)#
#
# --- Export data for GraphPad Prism ---#
df_latitude <- df[complete.cases(df[, c("Latitude", "Incidence")]), c("Latitude", "Incidence")]#
df_allele <- df[complete.cases(df[, c("AlleleFreq", "Incidence")]), c("AlleleFreq", "Incidence")]#
export_list <- list("Latitude vs Incidence" = df_latitude,#
                    "Allele Frequency vs Incidence" = df_allele)#
export_path <- "~/Desktop/graphpad_data.xlsx"#
write_xlsx(export_list, path = export_path)#
cat("Data for scatter plots exported to:", export_path, "\n")#
#
# =============================#
# --- Sensitivity Analyses ---#
# =============================#
#
# Sensitivity Analysis 1: Leave-One-Out Analysis#
# Here, we iteratively exclude each country and refit the multiple regression model #
# (i.e., Incidence ~ Latitude + AlleleFreq). This analysis is multivariate because both predictors#
# are included in each model. We record the coefficient estimates and P-values for both predictors.#
loo_results <- data.frame(Country = character(),#
                          Coef_AlleleFreq = numeric(),#
                          Pval_AlleleFreq = numeric(),#
                          Coef_Latitude = numeric(),#
                          Pval_Latitude = numeric(),#
                          stringsAsFactors = FALSE)#
#
countries <- unique(df$Country)#
for (c in countries) {#
  # Exclude the current country and remove incomplete cases#
  df_temp <- subset(df, Country != c & complete.cases(Incidence, Latitude, AlleleFreq))#
  # Proceed if a sufficient number of observations remain (e.g., n > 10)#
  if(nrow(df_temp) > 10) {#
    model_temp <- lm(Incidence ~ Latitude + AlleleFreq, data = df_temp)#
    coef_temp <- summary(model_temp)$coefficients#
    loo_results <- rbind(loo_results, data.frame(#
      Country = c,#
      Coef_AlleleFreq = coef_temp["AlleleFreq", "Estimate"],#
      Pval_AlleleFreq = coef_temp["AlleleFreq", "Pr(>|t|)"],#
      Coef_Latitude = coef_temp["Latitude", "Estimate"],#
      Pval_Latitude = coef_temp["Latitude", "Pr(>|t|)"]#
    ))#
  }#
}#
#
cat("\n--- Leave-One-Out Sensitivity Analysis Results ---\n")#
print(loo_results)#
#
# Sensitivity Analysis 2: Random Variation in Allele Frequencies by ±25 Percentage Points (10,000 iterations)#
# In each iteration, we randomly adjust each country's allele frequency by a value drawn from a uniform#
# distribution between -25 and +25 percentage points (with values capped between 0 and 100). #
# We then fit the multiple regression model (multivariate: Incidence ~ Latitude + AlleleFreq) and record#
# whether the associations for AlleleFreq and Latitude are significant (P < 0.05).#
set.seed(123)  # for reproducibility#
n_iterations <- 10000#
pvals_allele <- numeric(n_iterations)#
pvals_latitude <- numeric(n_iterations)#
#
for(i in 1:n_iterations){#
  df_random_iter <- df  # start with the original data#
  random_adjustments <- runif(nrow(df_random_iter), min = -25, max = 25)#
  df_random_iter$AlleleFreq <- pmin(pmax(df_random_iter$AlleleFreq + random_adjustments, 0), 100)#
  model_iter <- lm(Incidence ~ Latitude + AlleleFreq, data = df_random_iter)#
  summ_iter <- summary(model_iter)#
  pvals_allele[i] <- summ_iter$coefficients["AlleleFreq", "Pr(>|t|)"]#
  pvals_latitude[i] <- summ_iter$coefficients["Latitude", "Pr(>|t|)"]#
}#
#
sig_allele <- sum(pvals_allele < 0.05, na.rm = TRUE)#
sig_latitude <- sum(pvals_latitude < 0.05, na.rm = TRUE)#
#
cat("\n--- Random Variation Sensitivity Analysis (10,000 iterations) ---\n")#
cat("Out of", n_iterations, "iterations:\n")#
cat("The association for AlleleFreq was significant (P < 0.05) in", sig_allele, "iterations.\n")#
cat("The association for Latitude was significant (P < 0.05
# Updated R script with sensitivity analyses (using ±25 percentage points for random variation)#
#
# --- Load required packages ---#
if (!require("readxl")) {#
  install.packages("readxl")#
  library(readxl)#
} else {#
  library(readxl)#
}#
#
if (!require("ppcor")) {#
  install.packages("ppcor")#
  library(ppcor)#
} else {#
  library(ppcor)#
}#
#
if (!require("writexl")) {#
  install.packages("writexl")#
  library(writexl)#
} else {#
  library(writexl)#
}#
#
# --- Read in the data ---#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Evolutionary Origin of SNP rs12913832, Blue-Eye Mutation/Data/Data1.xlsx"#
df <- read_excel(file_path, sheet = 1)#
#
# Inspect the structure of the imported data#
str(df)#
#
# Rename columns for convenience (adjust if necessary)#
names(df) <- c("Country", "Incidence", "Period", "Comment", "Source",#
               "ApproxLat", "Latitude", "AlleleFreq", "LatSource")#
#
# Convert columns to numeric as needed#
df$Incidence <- as.numeric(df$Incidence)#
df$Latitude  <- as.numeric(df$Latitude)#
df$AlleleFreq <- as.numeric(df$AlleleFreq)#
#
# --- Update United States Allele Frequency ---#
# Compute the weighted average based on:#
# European Americans: 59% with 77%#
# Hispanic and Latino: 19% with 23%#
# Black or African Americans: 13% with 16%#
# (Total 91% of the US population)#
weighted_US <- (0.59 * 77 + 0.19 * 23 + 0.13 * 16) / 0.91#
cat("Weighted average allele frequency for the United States:", weighted_US, "\n")#
df$AlleleFreq[df$Country == "United States"] <- weighted_US#
#
# --- Plotting the relationships between incidence and predictors ---#
par(mfrow = c(1, 2))  # Two plots side by side#
#
# Plot: Incidence vs. Latitude#
plot(df$Latitude, df$Incidence,#
     main = "Incidence vs. Latitude",#
     xlab = "Latitude (°)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ Latitude, data = df), col = "red")#
#
# Plot: Incidence vs. Allele Frequency#
plot(df$AlleleFreq, df$Incidence,#
     main = "Incidence vs. Allele Frequency",#
     xlab = "Allele Frequency (%)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ AlleleFreq, data = df), col = "red")#
#
# --- Correlation tests for individual associations ---#
cor_lat <- cor.test(df$Incidence, df$Latitude)#
cor_allele <- cor.test(df$Incidence, df$AlleleFreq)#
print(cor_lat)#
print(cor_allele)#
#
# --- Fit simple and multiple regression models ---#
model_lat <- lm(Incidence ~ Latitude, data = df)#
model_allele <- lm(Incidence ~ AlleleFreq, data = df)#
model_both <- lm(Incidence ~ Latitude + AlleleFreq, data = df)#
#
# Display summaries of the models#
summary(model_lat)#
summary(model_allele)#
summary(model_both)#
#
# Standardize the variables to compare effect sizes directly#
df_std <- df#
df_std$Incidence <- scale(df_std$Incidence)#
df_std$Latitude  <- scale(df_std$Latitude)#
df_std$AlleleFreq <- scale(df_std$AlleleFreq)#
model_std <- lm(Incidence ~ Latitude + AlleleFreq, data = df_std)#
summary(model_std)#
#
# --- Partial Correlations Analysis ---#
# Method 1: Compute partial correlations manually from the multiple regression model#
model_summary <- summary(model_both)#
df_resid <- df.residual(model_both)#
#
t_allele <- model_summary$coefficients["AlleleFreq", "t value"]#
r_partial_allele <- sqrt(t_allele^2 / (t_allele^2 + df_resid))#
cat("Partial correlation for AlleleFreq (controlling for Latitude):", r_partial_allele, "\n")#
#
t_latitude <- model_summary$coefficients["Latitude", "t value"]#
r_partial_latitude <- sqrt(t_latitude^2 / (t_latitude^2 + df_resid))#
cat("Partial correlation for Latitude (controlling for AlleleFreq):", r_partial_latitude, "\n")#
#
# Method 2: Compute partial correlations using the ppcor package#
df_complete <- df[complete.cases(df[, c("Incidence", "Latitude", "AlleleFreq")]), ]#
partial_corr <- pcor(df_complete[, c("Incidence", "Latitude", "AlleleFreq")])#
print(partial_corr)#
#
# --- Export data for GraphPad Prism ---#
df_latitude <- df[complete.cases(df[, c("Latitude", "Incidence")]), c("Latitude", "Incidence")]#
df_allele <- df[complete.cases(df[, c("AlleleFreq", "Incidence")]), c("AlleleFreq", "Incidence")]#
export_list <- list("Latitude vs Incidence" = df_latitude,#
                    "Allele Frequency vs Incidence" = df_allele)#
export_path <- "~/Desktop/graphpad_data.xlsx"#
write_xlsx(export_list, path = export_path)#
cat("Data for scatter plots exported to:", export_path, "\n")#
#
# =============================#
# --- Sensitivity Analyses ---#
# =============================#
#
# Sensitivity Analysis 1: Leave-One-Out Analysis#
# Here, we iteratively exclude each country and refit the multiple regression model #
# (i.e., Incidence ~ Latitude + AlleleFreq). This analysis is multivariate because both predictors#
# are included in each model. We record the coefficient estimates and P-values for both predictors.#
loo_results <- data.frame(Country = character(),#
                          Coef_AlleleFreq = numeric(),#
                          Pval_AlleleFreq = numeric(),#
                          Coef_Latitude = numeric(),#
                          Pval_Latitude = numeric(),#
                          stringsAsFactors = FALSE)#
#
countries <- unique(df$Country)#
for (c in countries) {#
  # Exclude the current country and remove incomplete cases#
  df_temp <- subset(df, Country != c & complete.cases(Incidence, Latitude, AlleleFreq))#
  # Proceed if a sufficient number of observations remain (e.g., n > 10)#
  if(nrow(df_temp) > 10) {#
    model_temp <- lm(Incidence ~ Latitude + AlleleFreq, data = df_temp)#
    coef_temp <- summary(model_temp)$coefficients#
    loo_results <- rbind(loo_results, data.frame(#
      Country = c,#
      Coef_AlleleFreq = coef_temp["AlleleFreq", "Estimate"],#
      Pval_AlleleFreq = coef_temp["AlleleFreq", "Pr(>|t|)"],#
      Coef_Latitude = coef_temp["Latitude", "Estimate"],#
      Pval_Latitude = coef_temp["Latitude", "Pr(>|t|)"]#
    ))#
  }#
}#
#
cat("\n--- Leave-One-Out Sensitivity Analysis Results ---\n")#
print(loo_results)#
#
# Sensitivity Analysis 2: Random Variation in Allele Frequencies by ±25 Percentage Points (10,000 iterations)#
# In each iteration, we randomly adjust each country's allele frequency by a value drawn from a uniform#
# distribution between -25 and +25 percentage points (with values capped between 0 and 100). #
# We then fit the multiple regression model (multivariate: Incidence ~ Latitude + AlleleFreq) and record#
# whether the associations for AlleleFreq and Latitude are significant (P < 0.05).#
set.seed(123)  # for reproducibility#
n_iterations <- 10000#
pvals_allele <- numeric(n_iterations)#
pvals_latitude <- numeric(n_iterations)#
#
for(i in 1:n_iterations){#
  df_random_iter <- df  # start with the original data#
  random_adjustments <- runif(nrow(df_random_iter), min = -25, max = 25)#
  df_random_iter$AlleleFreq <- pmin(pmax(df_random_iter$AlleleFreq + random_adjustments, 0), 100)#
  model_iter <- lm(Incidence ~ Latitude + AlleleFreq, data = df_random_iter)#
  summ_iter <- summary(model_iter)#
  pvals_allele[i] <- summ_iter$coefficients["AlleleFreq", "Pr(>|t|)"]#
  pvals_latitude[i] <- summ_iter$coefficients["Latitude", "Pr(>|t|)"]#
}#
#
sig_allele <- sum(pvals_allele < 0.05, na.rm = TRUE)#
sig_latitude <- sum(pvals_latitude < 0.05, na.rm = TRUE)#
#
cat("\n--- Random Variation Sensitivity Analysis (10,000 iterations) ---\n")#
cat("Out of", n_iterations, "iterations:\n")#
cat("The association for AlleleFreq was significant (P < 0.05) in", sig_allele, "iterations.\n")#
cat("The association for Latitude was significant (P < 0.05) in", sig_latitude, "iterations.\n")
# Updated R script with sensitivity analyses (using ±25 percentage points for random variation)#
# --- Load required packages ---#
if (!require("readxl")) {#
  install.packages("readxl")#
  library(readxl)#
} else {#
  library(readxl)#
}#
if (!require("ppcor")) {#
  install.packages("ppcor")#
  library(ppcor)#
} else {#
  library(ppcor)#
}#
if (!require("writexl")) {#
  install.packages("writexl")#
  library(writexl)#
} else {#
  library(writexl)#
}#
# --- Read in the data ---#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Evolutionary Origin of SNP rs12913832, Blue-Eye Mutation/Data/Data1.xlsx"#
df <- read_excel(file_path, sheet = 1)#
# Inspect the structure of the imported data#
str(df)#
# Rename columns for convenience (adjust if necessary)#
names(df) <- c("Country", "Incidence", "Period", "Comment", "Source",#
               "ApproxLat", "Latitude", "AlleleFreq", "LatSource")#
# Convert columns to numeric as needed#
df$Incidence <- as.numeric(df$Incidence)#
df$Latitude  <- as.numeric(df$Latitude)#
df$AlleleFreq <- as.numeric(df$AlleleFreq)#
# --- Update United States Allele Frequency ---#
# Compute the weighted average based on:#
# European Americans: 59% with 77%#
# Hispanic and Latino: 19% with 23%#
# Black or African Americans: 13% with 16%#
# (Total 91% of the US population)#
weighted_US <- (0.59 * 77 + 0.19 * 23 + 0.13 * 16) / 0.91#
cat("Weighted average allele frequency for the United States:", weighted_US, "\n")#
df$AlleleFreq[df$Country == "United States"] <- weighted_US#
# --- Plotting the relationships between incidence and predictors ---#
par(mfrow = c(1, 2))  # Two plots side by side#
# Plot: Incidence vs. Latitude#
plot(df$Latitude, df$Incidence,#
     main = "Incidence vs. Latitude",#
     xlab = "Latitude (°)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ Latitude, data = df), col = "red")#
# Plot: Incidence vs. Allele Frequency#
plot(df$AlleleFreq, df$Incidence,#
     main = "Incidence vs. Allele Frequency",#
     xlab = "Allele Frequency (%)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ AlleleFreq, data = df), col = "red")#
# --- Correlation tests for individual associations ---#
cor_lat <- cor.test(df$Incidence, df$Latitude)#
cor_allele <- cor.test(df$Incidence, df$AlleleFreq)#
print(cor_lat)#
print(cor_allele)#
# --- Fit simple and multiple regression models ---#
model_lat <- lm(Incidence ~ Latitude, data = df)#
model_allele <- lm(Incidence ~ AlleleFreq, data = df)#
model_both <- lm(Incidence ~ Latitude + AlleleFreq, data = df)#
# Display summaries of the models#
summary(model_lat)#
summary(model_allele)#
summary(model_both)#
# Standardize the variables to compare effect sizes directly#
df_std <- df#
df_std$Incidence <- scale(df_std$Incidence)#
df_std$Latitude  <- scale(df_std$Latitude)#
df_std$AlleleFreq <- scale(df_std$AlleleFreq)#
model_std <- lm(Incidence ~ Latitude + AlleleFreq, data = df_std)#
summary(model_std)#
# --- Partial Correlations Analysis ---#
# Method 1: Compute partial correlations manually from the multiple regression model#
model_summary <- summary(model_both)#
df_resid <- df.residual(model_both)#
t_allele <- model_summary$coefficients["AlleleFreq", "t value"]#
r_partial_allele <- sqrt(t_allele^2 / (t_allele^2 + df_resid))#
cat("Partial correlation for AlleleFreq (controlling for Latitude):", r_partial_allele, "\n")#
t_latitude <- model_summary$coefficients["Latitude", "t value"]#
r_partial_latitude <- sqrt(t_latitude^2 / (t_latitude^2 + df_resid))#
cat("Partial correlation for Latitude (controlling for AlleleFreq):", r_partial_latitude, "\n")#
# Method 2: Compute partial correlations using the ppcor package#
df_complete <- df[complete.cases(df[, c("Incidence", "Latitude", "AlleleFreq")]), ]#
partial_corr <- pcor(df_complete[, c("Incidence", "Latitude", "AlleleFreq")])#
print(partial_corr)#
# --- Export data for GraphPad Prism ---#
df_latitude <- df[complete.cases(df[, c("Latitude", "Incidence")]), c("Latitude", "Incidence")]#
df_allele <- df[complete.cases(df[, c("AlleleFreq", "Incidence")]), c("AlleleFreq", "Incidence")]#
export_list <- list("Latitude vs Incidence" = df_latitude,#
                    "Allele Frequency vs Incidence" = df_allele)#
export_path <- "~/Desktop/graphpad_data.xlsx"#
write_xlsx(export_list, path = export_path)#
cat("Data for scatter plots exported to:", export_path, "\n")#
# =============================#
# --- Sensitivity Analyses ---#
# =============================#
# Sensitivity Analysis 1: Leave-One-Out Analysis#
# Here, we iteratively exclude each country and refit the multiple regression model #
# (i.e., Incidence ~ Latitude + AlleleFreq). This analysis is multivariate because both predictors#
# are included in each model. We record the coefficient estimates and P-values for both predictors.#
loo_results <- data.frame(Country = character(),#
                          Coef_AlleleFreq = numeric(),#
                          Pval_AlleleFreq = numeric(),#
                          Coef_Latitude = numeric(),#
                          Pval_Latitude = numeric(),#
                          stringsAsFactors = FALSE)#
countries <- unique(df$Country)#
for (c in countries) {#
  # Exclude the current country and remove incomplete cases#
  df_temp <- subset(df, Country != c & complete.cases(Incidence, Latitude, AlleleFreq))#
  # Proceed if a sufficient number of observations remain (e.g., n > 10)#
  if(nrow(df_temp) > 10) {#
    model_temp <- lm(Incidence ~ Latitude + AlleleFreq, data = df_temp)#
    coef_temp <- summary(model_temp)$coefficients#
    loo_results <- rbind(loo_results, data.frame(#
      Country = c,#
      Coef_AlleleFreq = coef_temp["AlleleFreq", "Estimate"],#
      Pval_AlleleFreq = coef_temp["AlleleFreq", "Pr(>|t|)"],#
      Coef_Latitude = coef_temp["Latitude", "Estimate"],#
      Pval_Latitude = coef_temp["Latitude", "Pr(>|t|)"]#
    ))#
  }#
}#
cat("\n--- Leave-One-Out Sensitivity Analysis Results ---\n")#
print(loo_results)#
# Sensitivity Analysis 2: Random Variation in Allele Frequencies by ±25 Percentage Points (10,000 iterations)#
# In each iteration, we randomly adjust each country's allele frequency by a value drawn from a uniform#
# distribution between -25 and +25 percentage points (with values capped between 0 and 100). #
# We then fit the multiple regression model (multivariate: Incidence ~ Latitude + AlleleFreq) and record#
# whether the associations for AlleleFreq and Latitude are significant (P < 0.05).#
set.seed(123)  # for reproducibility#
n_iterations <- 10000#
pvals_allele <- numeric(n_iterations)#
pvals_latitude <- numeric(n_iterations)#
for(i in 1:n_iterations){#
  df_random_iter <- df  # start with the original data#
  random_adjustments <- runif(nrow(df_random_iter), min = -25, max = 25)#
  df_random_iter$AlleleFreq <- pmin(pmax(df_random_iter$AlleleFreq + random_adjustments, 0), 100)#
  model_iter <- lm(Incidence ~ Latitude + AlleleFreq, data = df_random_iter)#
  summ_iter <- summary(model_iter)#
  pvals_allele[i] <- summ_iter$coefficients["AlleleFreq", "Pr(>|t|)"]#
  pvals_latitude[i] <- summ_iter$coefficients["Latitude", "Pr(>|t|)"]#
}#
sig_allele <- sum(pvals_allele < 0.05, na.rm = TRUE)#
sig_latitude <- sum(pvals_latitude < 0.05, na.rm = TRUE)#
cat("\n--- Random Variation Sensitivity Analysis (10,000 iterations) ---\n")#
cat("Out of", n_iterations, "iterations:\n")#
cat("The association for AlleleFreq was significant (P < 0.05) in", sig_allele, "iterations.\n")#
cat("The association for Latitude was significant (P < 0.05) in", sig_latitude, "iterations.\n")
# Load necessary packages#
library(ggplot2)#
library(ggridges)#
#
# Example: assume you have a data frame 'df' with columns: Country, Incidence, Latitude, and AlleleFrequency#
# Create latitude bins (e.g., 5° increments)#
df$LatitudeBin <- cut(df$Latitude, breaks = seq(0, max(df$Latitude, na.rm = TRUE) + 5, by = 5), include.lowest = TRUE)#
#
# Create a ridgeline plot of OM incidence by latitude bin#
ggplot(df, aes(x = Incidence, y = LatitudeBin, fill = LatitudeBin)) +#
  geom_density_ridges(alpha = 0.7, scale = 1.2) +#
  labs(title = "Distribution of OM Incidence Across Latitudinal Bins",#
       x = "OM Incidence (cases per million per year)",#
       y = "Latitude Bin (degrees)") +#
  theme_minimal() +#
  theme(legend.position = "none")
# Load necessary libraries#
library(ggplot2)#
library(ggridges)#
library(dplyr)#
#
# Create the data frame#
data <- data.frame(#
  Country = c("Australia", "Austria", "Belarus", "Belgium", "Canada", "China", #
              "Costa Rica", "Croatia", "Czech Republic", "Denmark", "Estonia", #
              "Finland", "France", "Germany", "India", "Ireland", "Israel", "Italy", #
              "Japan", "Kenya", "Latvia", "Lithuania", "Netherlands", "New Zealand", #
              "Norway", "Philippines", "Poland", "Russia", "Slovakia", "Slovenia", #
              "South Korea", "Spain", "Sweden", "Uganda", "Switzerland", #
              "United Kingdom", "United States", "South Africa"),#
  Incidence = c(7.4, 6.2, 8.7, 7.0, 7.7, 0.6, 1.4, 5.9, 6.1, 11.3, 6.2, 7.2, #
                5.8, 7.7, 0.1, 9.8, 5.3, 4.2, 0.4, 0.3, 5.4, 6.1, 9.5, 10.4, #
                10.1, 0.4, 5.8, 6.9, 8.2, 6.4, 0.9, 3.3, 9.2, 0.4, 4.1, 6.6, #
                5.9, 0.3),#
  Latitude = c(25, 47.5, 53.7, 50.8, 61, 35, 10, 45, 49.8, 56, 58.6, 64, 46, #
               51, 22, 53, 31, 42.8, 36.2, 0, 57, 55.3, 52.3, 41, 61, 12, 52.1, #
               60, 48.7, 46.1, 36.6, 40.4, 62, 1.5, 46.8, 54, 37, 28),#
  Frequency = c(79, NA, NA, NA, NA, 1, NA, NA, NA, 86, 84, 88, 59, 82, 13, #
                80, 42, 50, 1, 1, NA, NA, 75, NA, 86, NA, 78, 71, NA, 66, 1, #
                42, 81, 0, 74, 69, 57, 1)#
)#
#
# Ensure Frequency is numeric#
data$Frequency <- as.numeric(data$Frequency)#
#
# Create latitude bins (e.g., 5° increments)#
data <- data %>%#
  mutate(LatitudeBin = cut(Latitude, breaks = seq(0, max(Latitude, na.rm = TRUE) + 5, by = 5), #
                           include.lowest = TRUE))#
#
# Produce a ridgeline plot of OM incidence by latitude bins#
ggplot(data, aes(x = Incidence, y = LatitudeBin, fill = LatitudeBin)) +#
  geom_density_ridges(alpha = 0.7, scale = 1.2) +#
  labs(title = "Distribution of Ocular Melanoma Incidence Across Latitudinal Bins",#
       x = "OM Incidence (cases per million per year)",#
       y = "Latitude Bin (degrees)") +#
  theme_minimal() +#
  theme(legend.position = "none")
# Load necessary libraries#
library(ggplot2)#
library(ggridges)#
library(dplyr)#
#
# Create the data frame#
data <- data.frame(#
  Country = c("Australia", "Austria", "Belarus", "Belgium", "Canada", "China", #
              "Costa Rica", "Croatia", "Czech Republic", "Denmark", "Estonia", #
              "Finland", "France", "Germany", "India", "Ireland", "Israel", "Italy", #
              "Japan", "Kenya", "Latvia", "Lithuania", "Netherlands", "New Zealand", #
              "Norway", "Philippines", "Poland", "Russia", "Slovakia", "Slovenia", #
              "South Korea", "Spain", "Sweden", "Uganda", "Switzerland", #
              "United Kingdom", "United States", "South Africa"),#
  Incidence = c(7.4, 6.2, 8.7, 7.0, 7.7, 0.6, 1.4, 5.9, 6.1, 11.3, 6.2, 7.2, #
                5.8, 7.7, 0.1, 9.8, 5.3, 4.2, 0.4, 0.3, 5.4, 6.1, 9.5, 10.4, #
                10.1, 0.4, 5.8, 6.9, 8.2, 6.4, 0.9, 3.3, 9.2, 0.4, 4.1, 6.6, #
                5.9, 0.3),#
  Latitude = c(25, 47.5, 53.7, 50.8, 61, 35, 10, 45, 49.8, 56, 58.6, 64, 46, #
               51, 22, 53, 31, 42.8, 36.2, 0, 57, 55.3, 52.3, 41, 61, 12, 52.1, #
               60, 48.7, 46.1, 36.6, 40.4, 62, 1.5, 46.8, 54, 37, 28),#
  Frequency = c(79, NA, NA, NA, NA, 1, NA, NA, NA, 86, 84, 88, 59, 82, 13, #
                80, 42, 50, 1, 1, NA, NA, 75, NA, 86, NA, 78, 71, NA, 66, 1, #
                42, 81, 0, 74, 69, 57, 1)#
)#
#
# Ensure Frequency is numeric#
data$Frequency <- as.numeric(data$Frequency)#
#
# Create latitude bins with 10° increments#
data <- data %>%#
  mutate(LatitudeBin10 = cut(Latitude, breaks = seq(0, max(Latitude, na.rm = TRUE) + 10, by = 10),#
                             include.lowest = TRUE))#
#
# Produce a ridgeline plot of OM incidence by 10° latitude bins#
ggplot(data, aes(x = Incidence, y = LatitudeBin10, fill = LatitudeBin10)) +#
  geom_density_ridges(alpha = 0.7, scale = 1.2, rel_min_height = 0.01) +#
  labs(title = "Distribution of Ocular Melanoma Incidence Across 10° Latitudinal Bins",#
       x = "OM Incidence (cases per million per year)",#
       y = "Latitude Bin (degrees)") +#
  theme_minimal() +#
  theme(legend.position = "none")
ggplot(data, aes(x = Incidence, y = LatitudeBin10, fill = LatitudeBin10)) +#
  geom_density_ridges(alpha = 0.7, scale = 1.2, rel_min_height = 0.01) +#
  geom_point(aes(y = as.numeric(LatitudeBin10)), position = position_jitter(height = 0.1), color = "black", size = 1.5) +#
  labs(title = "Distribution of Ocular Melanoma Incidence Across 10° Latitudinal Bins",#
       x = "OM Incidence (cases per million per year)",#
       y = "Latitude Bin (degrees)") +#
  theme_minimal() +#
  theme(legend.position = "none")
# Load required libraries#
library(mediation)#
library(dplyr)#
library(ggplot2)#
#
# Create the dataset#
data <- data.frame(#
  Country = c("Australia", "Austria", "Belarus", "Belgium", "Canada", "China", #
              "Costa Rica", "Croatia", "Czech Republic", "Denmark", "Estonia", #
              "Finland", "France", "Germany", "India", "Ireland", "Israel", "Italy", #
              "Japan", "Kenya", "Latvia", "Lithuania", "Netherlands", "New Zealand", #
              "Norway", "Philippines", "Poland", "Russia", "Slovakia", "Slovenia", #
              "South Korea", "Spain", "Sweden", "Uganda", "Switzerland", #
              "United Kingdom", "United States", "South Africa"),#
  Incidence = c(7.4, 6.2, 8.7, 7.0, 7.7, 0.6, 1.4, 5.9, 6.1, 11.3, 6.2, 7.2, #
                5.8, 7.7, 0.1, 9.8, 5.3, 4.2, 0.4, 0.3, 5.4, 6.1, 9.5, 10.4, #
                10.1, 0.4, 5.8, 6.9, 8.2, 6.4, 0.9, 3.3, 9.2, 0.4, 4.1, 6.6, #
                5.9, 0.3),#
  Latitude = c(25, 47.5, 53.7, 50.8, 61, 35, 10, 45, 49.8, 56, 58.6, 64, 46, #
               51, 22, 53, 31, 42.8, 36.2, 0, 57, 55.3, 52.3, 41, 61, 12, 52.1, #
               60, 48.7, 46.1, 36.6, 40.4, 62, 1.5, 46.8, 54, 37, 28),#
  Frequency = c(79, NA, NA, NA, NA, 1, NA, NA, NA, 86, 84, 88, 59, 82, 13, #
                80, 42, 50, 1, 1, NA, NA, 75, NA, 86, NA, 78, 71, NA, 66, 1, #
                42, 81, 0, 74, 69, 57, 1)#
)#
#
# Convert Frequency to numeric (if not already)#
data$Frequency <- as.numeric(data$Frequency)#
#
# Filter data to include only observations with non-missing allele frequency#
df_filtered <- filter(data, !is.na(Frequency))#
#
# Fit the mediator model: Does latitude predict allele frequency?#
mediator_model <- lm(Frequency ~ Latitude, data = df_filtered)#
summary(mediator_model)#
#
# Fit the outcome model: Do latitude and allele frequency predict OM incidence?#
outcome_model <- lm(Incidence ~ Latitude + Frequency, data = df_filtered)#
summary(outcome_model)#
#
# Set seed for reproducibility#
set.seed(123)#
#
# Perform mediation analysis:#
# Treat: Latitude, Mediator: Frequency, Outcome: Incidence#
mediation_result <- mediate(mediator_model, outcome_model, treat = "Latitude", #
                             mediator = "Frequency", boot = TRUE, sims = 1000)#
#
# Summary of mediation analysis#
summary(mediation_result)#
#
# Optionally, plot the mediation effect#
plot(mediation_result)
# Comprehensive Statistical Analysis Script for Questionnaire Data#
# Date: 2025-04-01#
# Description: This script performs a series of analyses on three rounds of questionnaires #
# from opticians/optometrists regarding their exposure to pigmented fundus lesions, their #
# evaluation of these lesions, their knowledge of referral criteria, and their attitudes towards AI.#
# All P values are Bonferroni-corrected by multiplying with the number of tests performed.#
#
# -------------------------------#
# Load required libraries#
# -------------------------------#
library(readxl)#
library(dplyr)#
library(ggplot2)#
library(tidyr)#
library(stringr)#
library(broom)#
library(knitr)#
#
# -------------------------------#
# Data Import and Preprocessing#
# -------------------------------#
# Update file paths as needed#
file_q1 <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Svar på enkät 1.xlsx"#
file_q2 <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Svar på enkät 2.xlsx"#
file_q3 <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Svar på enkät 3.xlsx"#
#
# Read in data from each questionnaire#
q1 <- read_excel(file_q1)#
q2 <- read_excel(file_q2)#
q3 <- read_excel(file_q3)#
#
# Inspect the data structure if needed#
# str(q1); str(q2); str(q3)#
#
# For Questionnaire 1, create a numeric estimate from the reported range of number of customers.#
# Adjust the column name exactly as it appears in your data.#
customer_col <- "Hur många kunder med pigmenterade fläckar i ögonbotten träffar du per år? Inkludera alla oavsett besöksorsak. Uppskatta enbart antalet individer, inte antalet fläckar eller hur många gånger du träffar"#
#
extract_midpoint <- function(range_str) {#
  # Extract all numbers from the string#
  nums <- as.numeric(unlist(str_extract_all(range_str, "\\d+")))#
  if(length(nums) >= 2) {#
    return(mean(nums[1:2]))#
  } else if(length(nums) == 1) {#
    return(nums)#
  } else {#
    return(NA)#
  }#
}#
#
q1 <- q1 %>% #
  mutate(customers_mid = sapply(.[[customer_col]], extract_midpoint))#
#
# Recode “difficulty in assessing” the lesions.#
# Assumes the question is exactly named as below.#
difficulty_col <- "Tycker du att det är svårt att bedöma om pigmenterade fläckar i ögonbotten är godartade eller elakartade?"#
q1 <- q1 %>%#
  mutate(difficulty = ifelse(grepl("svårt", .[[difficulty_col]], ignore.case = TRUE), 1, 0))#
#
# -------------------------------#
# Descriptive Statistics and Figures - Questionnaire 1#
# -------------------------------#
cat("Respondent characteristics (Questionnaire 1):\n")#
print(table(q1$Kön))#
print(table(q1$`Yrkestitel`))#
print(table(q1$Ålder))#
#
# Histogram for estimated number of customers per year#
ggplot(q1, aes(x = customers_mid)) +#
  geom_histogram(binwidth = 5, fill = "grey", color = "black") +#
  labs(title = "Distribution of Estimated Number of Customers per Year",#
       x = "Estimated Number of Customers (midpoint of range)",#
       y = "Frequency")#
#
# Boxplot of customers per year by profession#
ggplot(q1, aes(x = `Yrkestitel`, y = customers_mid)) +#
  geom_boxplot() +#
  labs(title = "Estimated Number of Customers by Profession",#
       x = "Profession",#
       y = "Estimated Number of Customers (midpoint)")#
#
# -------------------------------#
# Hypothesis Tests - Questionnaire 1#
# -------------------------------#
# 1. Compare the number of customers between professions (Optometrist vs Optiker)#
q1_prof <- q1 %>% filter(`Yrkestitel` %in% c("Optometrist", "Optiker"))#
wilcox_test <- wilcox.test(customers_mid ~ `Yrkestitel`, data = q1_prof)#
cat("\nWilcoxon rank-sum test comparing number of customers between professions:\n")#
print(wilcox_test)#
#
# 2. Spearman correlation between number of customers and difficulty in assessing lesions#
cor_test <- cor.test(q1$customers_mid, q1$difficulty, method = "spearman")#
cat("\nSpearman correlation test between number of customers and difficulty in assessing lesions:\n")#
print(cor_test)#
#
# 3. Chi-square test for association between knowledge of referral criteria and attitude towards AI.#
# Adjust the column names as they appear in your dataset.#
knowledge_col <- "Är du väl insatt i vilka kriterier som gäller för vilka fläckar som ska remitteras till ögonläkare?"#
ai_attitude_col <- "Tror du att ett verktyg baserad på deep learning (en form av artificiell intelligens) skulle kunna vara behjälpligt i dessa bedömningar av en fläcks farlighet, eller vilka fläckar som bör remitteras?"#
table_knowledge_ai <- table(q1[[knowledge_col]], q1[[ai_attitude_col]])#
chisq_test <- chisq.test(table_knowledge_ai)#
cat("\nChi-square test for association between knowledge of referral criteria and AI attitude:\n")#
print(chisq_test)#
cat("\nContingency table:\n")#
print(table_knowledge_ai)#
#
# -------------------------------#
# Bonferroni Correction for Questionnaire 1#
# -------------------------------#
# We have 3 tests here.#
n_tests_q1 <- 3#
wilcox_p_adj <- min(wilcox_test$p.value * n_tests_q1, 1)#
cor_p_adj <- min(cor_test$p.value * n_tests_q1, 1)#
chisq_p_adj <- min(chisq_test$p.value * n_tests_q1, 1)#
#
cat(sprintf("\nBonferroni-adjusted *P* for Wilcoxon test: %.4f\n", wilcox_p_adj))#
cat(sprintf("Bonferroni-adjusted *P* for Spearman correlation: %.4f\n", cor_p_adj))#
cat(sprintf("Bonferroni-adjusted *P* for Chi-square test: %.4f\n", chisq_p_adj))#
#
# -------------------------------#
# Analysis - Questionnaire 2 (Image Assessment)#
# -------------------------------#
# Define the correct answer mapping for each image (assuming 25 images: Bild0 to Bild24)#
# (1 = Melanom, 0 = Nevus)#
correct_answers <- c(1,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0)#
names(correct_answers) <- paste0("Bild", 0:24)#
#
# Reshape Q2 data: We focus on the referral decisions.#
# Identify columns that contain referral decisions. They start with "Skulle du ha remitterat".#
referral_cols <- grep("Skulle du ha remitterat", names(q2), value = TRUE)#
q2_long <- q2 %>%#
  pivot_longer(#
    cols = all_of(referral_cols),#
    names_to = "image_response",#
    values_to = "referral_response"#
  )#
#
# Extract image number from the column name (assumes the first number encountered is the image number)#
q2_long <- q2_long %>%#
  mutate(image_num = as.numeric(str_extract(image_response, "\\d+"))) %>%#
  filter(!is.na(image_num))  # keep only valid responses#
#
# Map the correct answer using the image number.#
q2_long <- q2_long %>%#
  mutate(correct = correct_answers[paste0("Bild", image_num)]) %>%#
  mutate(correct = as.numeric(correct))#
#
# Recode responses to numeric (assume "Ja" = 1 and "Nej" = 0)#
q2_long <- q2_long %>%#
  mutate(referral_numeric = case_when(#
    referral_response == "Ja" ~ 1,#
    referral_response == "Nej" ~ 0,#
    TRUE ~ NA_real_#
  ))#
#
# Determine if each response is correct#
q2_long <- q2_long %>%#
  mutate(is_correct = ifelse(referral_numeric == correct, 1, 0))#
#
# Summarize accuracy for each respondent#
accuracy_by_respondent <- q2_long %>%#
  group_by(Id) %>%#
  summarise(accuracy = mean(is_correct, na.rm = TRUE),#
            n_images = n())#
#
cat("\nAccuracy summary for Questionnaire 2 (per respondent):\n")#
print(accuracy_by_respondent)#
#
# Test whether the overall accuracy is above chance (assumed 0.5)#
total_correct <- sum(q2_long$is_correct, na.rm = TRUE)#
total_responses <- sum(!is.na(q2_long$is_correct))#
prop_test <- prop.test(total_correct, total_responses, p = 0.5)#
cat("\nProportion test for overall accuracy (chance level = 0.5):\n")#
print(prop_test)#
#
# Bonferroni correction for Questionnaire 2 (1 test here)#
n_tests_q2 <- 1#
prop_test_p_adj <- min(prop_test$p.value * n_tests_q2, 1)#
cat(sprintf("\nBonferroni-adjusted *P* for the proportion test: %.4f\n", prop_test_p_adj))#
#
# Plot the distribution of accuracy across respondents#
ggplot(accuracy_by_respondent, aes(x = accuracy)) +#
  geom_histogram(binwidth = 0.05, fill = "grey", color = "black") +#
  labs(title = "Distribution of Accuracy in Image Assessment (Questionnaire 2)",#
       x = "Accuracy (proportion correct)",#
       y = "Frequency")#
#
# -------------------------------#
# Analysis - Questionnaire 3 (Attitudes towards melAInoma)#
# -------------------------------#
# Create frequency tables for key questions#
q3_questions <- c(#
  "Enligt din bedömning, skulle en analys med melAInoma ibland kunna få dig att ändra uppfattning om en fläck, till exempel bedöma att en kund bör remitteras till en ögonläkare?",#
  "Enligt din åsikt, kan melAInoma förbättra bedömningen av vilka fläckar som ska remitteras till ögonläkare?",#
  "Enligt din åsikt, kan melAInoma förbättra den information du ger till kunden?"#
)#
#
for (q in q3_questions) {#
  cat(sprintf("\nFrequency table for question:\n%s\n", q))#
  print(table(q3[[q]]))#
}#
#
# -------------------------------#
# End of Analysis#
# -------------------------------#
# Additional analyses (e.g. regression, paired comparisons across rounds) can be added as needed.
# Comprehensive Statistical Analysis Script for Questionnaire Data with Plot Exports#
# Date: 2025-04-01#
# Description: This script reads in three Excel files, preprocesses the data, performs descriptive#
# analyses and hypothesis tests, and exports all generated plots to the Desktop. All *P* values are#
# Bonferroni-corrected by multiplying with the number of tests performed.#
#
# -------------------------------#
# Load required libraries#
# -------------------------------#
library(readxl)#
library(dplyr)#
library(ggplot2)#
library(tidyr)#
library(stringr)#
library(broom)#
library(knitr)#
#
# -------------------------------#
# Data Import and Preprocessing#
# -------------------------------#
# Update file paths as needed#
file_q1 <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Svar på enkät 1.xlsx"#
file_q2 <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Svar på enkät 2.xlsx"#
file_q3 <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Svar på enkät 3.xlsx"#
#
# Read in data from each questionnaire#
q1 <- read_excel(file_q1)#
q2 <- read_excel(file_q2)#
q3 <- read_excel(file_q3)#
#
# For Questionnaire 1, convert the reported range of number of customers to a numeric midpoint.#
customer_col <- "Hur många kunder med pigmenterade fläckar i ögonbotten träffar du per år? Inkludera alla oavsett besöksorsak. Uppskatta enbart antalet individer, inte antalet fläckar eller hur många gånger du träffar"#
#
extract_midpoint <- function(range_str) {#
  # Extract numbers from the string and compute the mean of the first two numbers#
  nums <- as.numeric(unlist(str_extract_all(range_str, "\\d+")))#
  if(length(nums) >= 2) {#
    return(mean(nums[1:2]))#
  } else if(length(nums) == 1) {#
    return(nums)#
  } else {#
    return(NA)#
  }#
}#
#
q1 <- q1 %>% #
  mutate(customers_mid = sapply(.[[customer_col]], extract_midpoint))#
#
# Recode “difficulty in assessing” lesions based on response text.#
difficulty_col <- "Tycker du att det är svårt att bedöma om pigmenterade fläckar i ögonbotten är godartade eller elakartade?"#
q1 <- q1 %>%#
  mutate(difficulty = ifelse(grepl("svårt", .[[difficulty_col]], ignore.case = TRUE), 1, 0))#
#
# -------------------------------#
# Descriptive Statistics and Figures - Questionnaire 1#
# -------------------------------#
cat("Respondent characteristics (Questionnaire 1):\n")#
print(table(q1$Kön))#
print(table(q1$`Yrkestitel`))#
print(table(q1$Ålder))#
#
# Histogram for estimated number of customers per year#
p1 <- ggplot(q1, aes(x = customers_mid)) +#
  geom_histogram(binwidth = 5, fill = "grey", color = "black") +#
  labs(title = "Distribution of Estimated Number of Customers per Year",#
       x = "Estimated Number of Customers (midpoint of range)",#
       y = "Frequency")#
print(p1)#
ggsave(filename = "Histogram_Customers.png", plot = p1, path = "~/Desktop", width = 8, height = 6)#
#
# Boxplot of customers per year by profession#
p2 <- ggplot(q1, aes(x = `Yrkestitel`, y = customers_mid)) +#
  geom_boxplot() +#
  labs(title = "Estimated Number of Customers by Profession",#
       x = "Profession",#
       y = "Estimated Number of Customers (midpoint)")#
print(p2)#
ggsave(filename = "Boxplot_Customers_by_Profession.png", plot = p2, path = "~/Desktop", width = 8, height = 6)#
#
# -------------------------------#
# Hypothesis Tests - Questionnaire 1#
# -------------------------------#
# 1. Compare the number of customers between professions (Optometrist vs Optiker)#
q1_prof <- q1 %>% filter(`Yrkestitel` %in% c("Optometrist", "Optiker"))#
wilcox_test <- wilcox.test(customers_mid ~ `Yrkestitel`, data = q1_prof)#
cat("\nWilcoxon rank-sum test comparing number of customers between professions:\n")#
print(wilcox_test)#
#
# 2. Spearman correlation between number of customers and difficulty in assessing lesions#
cor_test <- cor.test(q1$customers_mid, q1$difficulty, method = "spearman")#
cat("\nSpearman correlation test between number of customers and difficulty in assessing lesions:\n")#
print(cor_test)#
#
# 3. Chi-square test for association between knowledge of referral criteria and attitude towards AI.#
knowledge_col <- "Är du väl insatt i vilka kriterier som gäller för vilka fläckar som ska remitteras till ögonläkare?"#
ai_attitude_col <- "Tror du att ett verktyg baserad på deep learning (en form av artificiell intelligens) skulle kunna vara behjälpligt i dessa bedömningar av en fläcks farlighet, eller vilka fläckar som bör remitteras?"#
table_knowledge_ai <- table(q1[[knowledge_col]], q1[[ai_attitude_col]])#
chisq_test <- chisq.test(table_knowledge_ai)#
cat("\nChi-square test for association between knowledge of referral criteria and AI attitude:\n")#
print(chisq_test)#
cat("\nContingency table:\n")#
print(table_knowledge_ai)#
#
# Bonferroni Correction for Questionnaire 1#
n_tests_q1 <- 3#
wilcox_p_adj <- min(wilcox_test$p.value * n_tests_q1, 1)#
cor_p_adj <- min(cor_test$p.value * n_tests_q1, 1)#
chisq_p_adj <- min(chisq_test$p.value * n_tests_q1, 1)#
#
cat(sprintf("\nBonferroni-adjusted *P* for Wilcoxon test: %.4f\n", wilcox_p_adj))#
cat(sprintf("Bonferroni-adjusted *P* for Spearman correlation: %.4f\n", cor_p_adj))#
cat(sprintf("Bonferroni-adjusted *P* for Chi-square test: %.4f\n", chisq_p_adj))#
#
# -------------------------------#
# Analysis - Questionnaire 2 (Image Assessment)#
# -------------------------------#
# Define the correct answer mapping for each image (assuming 25 images: Bild0 to Bild24)#
# (1 = Melanom, 0 = Nevus)#
correct_answers <- c(1,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0)#
names(correct_answers) <- paste0("Bild", 0:24)#
#
# Reshape Q2 data: Focus on referral decisions.#
referral_cols <- grep("Skulle du ha remitterat", names(q2), value = TRUE)#
q2_long <- q2 %>%#
  pivot_longer(#
    cols = all_of(referral_cols),#
    names_to = "image_response",#
    values_to = "referral_response"#
  )#
#
# Extract image number from the column name (assuming the first number encountered is the image number)#
q2_long <- q2_long %>%#
  mutate(image_num = as.numeric(str_extract(image_response, "\\d+"))) %>%#
  filter(!is.na(image_num))  # keep only valid responses#
#
# Map the correct answer using the image number.#
q2_long <- q2_long %>%#
  mutate(correct = correct_answers[paste0("Bild", image_num)]) %>%#
  mutate(correct = as.numeric(correct))#
#
# Recode responses to numeric (assume "Ja" = 1 and "Nej" = 0)#
q2_long <- q2_long %>%#
  mutate(referral_numeric = case_when(#
    referral_response == "Ja" ~ 1,#
    referral_response == "Nej" ~ 0,#
    TRUE ~ NA_real_#
  ))#
#
# Determine if each response is correct#
q2_long <- q2_long %>%#
  mutate(is_correct = ifelse(referral_numeric == correct, 1, 0))#
#
# Summarize accuracy for each respondent#
accuracy_by_respondent <- q2_long %>%#
  group_by(Id) %>%#
  summarise(accuracy = mean(is_correct, na.rm = TRUE),#
            n_images = n())#
#
cat("\nAccuracy summary for Questionnaire 2 (per respondent):\n")#
print(accuracy_by_respondent)#
#
# Test whether the overall accuracy is above chance (assumed 0.5)#
total_correct <- sum(q2_long$is_correct, na.rm = TRUE)#
total_responses <- sum(!is.na(q2_long$is_correct))#
prop_test <- prop.test(total_correct, total_responses, p = 0.5)#
cat("\nProportion test for overall accuracy (chance level = 0.5):\n")#
print(prop_test)#
#
# Bonferroni correction for Questionnaire 2 (1 test here)#
n_tests_q2 <- 1#
prop_test_p_adj <- min(prop_test$p.value * n_tests_q2, 1)#
cat(sprintf("\nBonferroni-adjusted *P* for the proportion test: %.4f\n", prop_test_p_adj))#
#
# Plot the distribution of accuracy across respondents#
p3 <- ggplot(accuracy_by_respondent, aes(x = accuracy)) +#
  geom_histogram(binwidth = 0.05, fill = "grey", color = "black") +#
  labs(title = "Distribution of Accuracy in Image Assessment (Questionnaire 2)",#
       x = "Accuracy (proportion correct)",#
       y = "Frequency")#
print(p3)#
ggsave(filename = "Histogram_Accuracy.png", plot = p3, path = "~/Desktop", width = 8, height = 6)#
#
# -------------------------------#
# Analysis - Questionnaire 3 (Attitudes towards melAInoma)#
# -------------------------------#
# Create frequency tables for key questions#
q3_questions <- c(#
  "Enligt din bedömning, skulle en analys med melAInoma ibland kunna få dig att ändra uppfattning om en fläck, till exempel bedöma att en kund bör remitteras till en ögonläkare?",#
  "Enligt din åsikt, kan melAInoma förbättra bedömningen av vilka fläckar som ska remitteras till ögonläkare?",#
  "Enligt din åsikt, kan melAInoma förbättra den information du ger till kunden?"#
)#
#
for (q in q3_questions) {#
  cat(sprintf("\nFrequency table for question:\n%s\n", q))#
  print(table(q3[[q]]))#
}#
#
# -------------------------------#
# End of Analysis#
# -------------------------------#
# Additional analyses (e.g. regression, paired comparisons across rounds) can be added as needed.
# -------------------------------#
# Analysis - Questionnaire 2 (Image Assessment)#
# -------------------------------#
# Define the correct answer mapping for each image (assuming 25 images: Bild0 to Bild24)#
# (1 = Melanom, 0 = Nevus)#
correct_answers <- c(1,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0)#
names(correct_answers) <- paste0("Bild", 0:24)#
#
# Filter referral decision columns: include only those that start with "Skulle du ha remitterat till ögonläkare"#
referral_cols <- names(q2)[grepl("^Skulle du ha remitterat till ögonläkare\\??\\d*$", names(q2))]#
#
# Reshape Q2 data focusing on the referral decisions#
q2_long <- q2 %>%#
  pivot_longer(#
    cols = all_of(referral_cols),#
    names_to = "image_response",#
    values_to = "referral_response"#
  ) %>%#
  # Extract the image number; if none is found (first image), assign 0#
  mutate(image_num = as.numeric(str_extract(image_response, "\\d+")),#
         image_num = ifelse(is.na(image_num), 0, image_num))#
#
# Map the correct answer using the image number.#
q2_long <- q2_long %>%#
  mutate(correct = as.numeric(correct_answers[paste0("Bild", image_num)]))#
#
# Recode responses to numeric (assume "Ja" = 1 and "Nej" = 0)#
q2_long <- q2_long %>%#
  mutate(referral_numeric = case_when(#
    referral_response == "Ja" ~ 1,#
    referral_response == "Nej" ~ 0,#
    TRUE ~ NA_real_#
  ))#
#
# Determine if each response is correct#
q2_long <- q2_long %>%#
  mutate(is_correct = ifelse(referral_numeric == correct, 1, 0))#
#
# Summarize accuracy for each respondent#
accuracy_by_respondent <- q2_long %>%#
  group_by(Id) %>%#
  summarise(accuracy = mean(is_correct, na.rm = TRUE),#
            n_images = n())#
#
# Output the accuracy summary#
cat("\nAccuracy summary for Questionnaire 2 (per respondent):\n")#
print(accuracy_by_respondent)
# Uncomment and run the following lines if the packages are not installed#
# install.packages("readxl")#
# install.packages("pROC")#
# install.packages("openxlsx")#
#
library(readxl)#
library(pROC)#
library(openxlsx)#
#
# Specify the file path to the Patientförteckning.xlsx file#
data_file <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Patientförteckning.xlsx"#
#
# Read the data (assuming the first sheet contains the data)#
data <- read_excel(data_file)#
#
# Define the binary outcome.#
# Here we assume that the column "Nevus 0 Melanoma 1" (second column) contains the true diagnosis#
outcome <- data$`Nevus 0 Melanoma 1`#
#
# Compute ROC curves for the three single-score predictors#
roc_melanoma <- roc(response = outcome, predictor = data$`MelAInoma score`)#
roc_oncologist <- roc(response = outcome, predictor = data$`MOLES score by ocular oncologist, first photo`)#
roc_shields <- roc(response = outcome, predictor = data$`Number of risk factors according to Shields`)#
#
# Extract coordinates (FPR and sensitivity) for each ROC curve#
coords_melanoma <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_melanoma$specificities,#
  `True positive rate (sensitivity)` = roc_melanoma$sensitivities#
)#
#
coords_oncologist <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_oncologist$specificities,#
  `True positive rate (sensitivity)` = roc_oncologist$sensitivities#
)#
#
coords_shields <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_shields$specificities,#
  `True positive rate (sensitivity)` = roc_shields$sensitivities#
)#
#
# For optometrists, identify the 29 columns by matching the column names pattern#
opt_columns <- grep("MOLES score optometrist/optician", names(data), value = TRUE)#
#
# Compute ROC curves for each optometrist/optician column#
roc_list <- lapply(opt_columns, function(col) {#
  roc(response = outcome, predictor = data[[col]])#
})#
#
# Define a common grid for false positive rates (from 0 to 1 by 0.01)#
fpr_grid <- seq(0, 1, by = 0.01)#
#
# Interpolate sensitivities for each ROC at the grid of FPR values#
# Note: pROC's coords function requires specificity (1 - FPR) as input.#
sensitivity_matrix <- sapply(roc_list, function(roc_obj) {#
  sapply(fpr_grid, function(fpr) {#
    spec_val <- 1 - fpr#
    # Use interpolation; if a value is not found, NA is returned.#
    coords(roc_obj, x = spec_val, input = "specificity", ret = "sensitivity", transpose = FALSE, as.matrix = FALSE)#
  })#
})#
#
# Compute the mean sensitivity and its 95% confidence interval at each FPR grid point#
mean_sensitivity <- rowMeans(sensitivity_matrix, na.rm = TRUE)#
sd_sensitivity <- apply(sensitivity_matrix, 1, sd, na.rm = TRUE)#
n <- ncol(sensitivity_matrix)#
se_sensitivity <- sd_sensitivity / sqrt(n)#
lower_ci <- mean_sensitivity - 1.96 * se_sensitivity#
upper_ci <- mean_sensitivity + 1.96 * se_sensitivity#
#
roc_opt <- data.frame(#
  `False positive rate (1-specificity)` = fpr_grid,#
  `True positive rate (sensitivity)` = mean_sensitivity,#
  `Lower 95% CI` = lower_ci,#
  `Upper 95% CI` = upper_ci#
)#
#
# Create an Excel workbook and add worksheets for each ROC curve#
wb <- createWorkbook()#
addWorksheet(wb, "MelAInoma")#
writeData(wb, "MelAInoma", coords_melanoma)#
#
addWorksheet(wb, "MOLES_Oncologist")#
writeData(wb, "MOLES_Oncologist", coords_oncologist)#
#
addWorksheet(wb, "Shields")#
writeData(wb, "Shields", coords_shields)#
#
addWorksheet(wb, "Optometrists")#
writeData(wb, "Optometrists", roc_opt)#
#
# Save the workbook to the desktop#
output_file <- file.path(Sys.getenv("HOME"), "Desktop", "ROC_output.xlsx")#
saveWorkbook(wb, output_file, overwrite = TRUE)
# Uncomment and run the following lines if the packages are not installed#
# install.packages("readxl")#
# install.packages("pROC")#
# install.packages("openxlsx")#
#
library(readxl)#
library(pROC)#
library(openxlsx)#
#
# Specify the file path to the Patientförteckning.xlsx file#
data_file <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Patientförteckning.xlsx"#
#
# Read the data (assuming the first sheet contains the data)#
data <- read_excel(data_file)#
#
# Define the binary outcome.#
# Here we assume that the column "Nevus 0 Melanoma 1" (second column) contains the true diagnosis#
outcome <- data$`Nevus 0 Melanoma 1`#
#
# Compute ROC curves for the three single-score predictors#
roc_melanoma <- roc(response = outcome, predictor = data$`MelAInoma score`)#
roc_oncologist <- roc(response = outcome, predictor = data$`MOLES score by ocular oncologist, first photo`)#
roc_shields <- roc(response = outcome, predictor = data$`Number of risk factors according to Shields`)#
#
# Extract coordinates (FPR and sensitivity) for each ROC curve#
coords_melanoma <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_melanoma$specificities,#
  `True positive rate (sensitivity)` = roc_melanoma$sensitivities#
)#
#
coords_oncologist <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_oncologist$specificities,#
  `True positive rate (sensitivity)` = roc_oncologist$sensitivities#
)#
#
coords_shields <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_shields$specificities,#
  `True positive rate (sensitivity)` = roc_shields$sensitivities#
)#
#
# For optometrists, identify the 29 columns by matching the column names pattern#
opt_columns <- grep("MOLES score optometrist/optician", names(data), value = TRUE)#
#
# Compute ROC curves for each optometrist/optician column#
roc_list <- lapply(opt_columns, function(col) {#
  roc(response = outcome, predictor = data[[col]])#
})#
#
# Define a common grid for false positive rates (from 0 to 1 by 0.01)#
fpr_grid <- seq(0, 1, by = 0.01)#
#
# Interpolate sensitivities for each ROC at the grid of FPR values#
# Note: pROC's coords function requires specificity (1 - FPR) as input.#
sensitivity_matrix <- sapply(roc_list, function(roc_obj) {#
  sapply(fpr_grid, function(fpr) {#
    spec_val <- 1 - fpr#
    # Force the result to numeric#
    as.numeric(coords(roc_obj, x = spec_val, input = "specificity", #
                        ret = "sensitivity", transpose = FALSE, as.matrix = FALSE))#
  })#
})#
#
# Compute the mean sensitivity and its 95% confidence interval at each FPR grid point#
mean_sensitivity <- rowMeans(sensitivity_matrix, na.rm = TRUE)#
sd_sensitivity <- apply(sensitivity_matrix, 1, sd, na.rm = TRUE)#
n <- ncol(sensitivity_matrix)#
se_sensitivity <- sd_sensitivity / sqrt(n)#
lower_ci <- mean_sensitivity - 1.96 * se_sensitivity#
upper_ci <- mean_sensitivity + 1.96 * se_sensitivity#
#
roc_opt <- data.frame(#
  `False positive rate (1-specificity)` = fpr_grid,#
  `True positive rate (sensitivity)` = mean_sensitivity,#
  `Lower 95% CI` = lower_ci,#
  `Upper 95% CI` = upper_ci#
)#
#
# Create an Excel workbook and add worksheets for each ROC curve#
wb <- createWorkbook()#
addWorksheet(wb, "MelAInoma")#
writeData(wb, "MelAInoma", coords_melanoma)#
#
addWorksheet(wb, "MOLES_Oncologist")#
writeData(wb, "MOLES_Oncologist", coords_oncologist)#
#
addWorksheet(wb, "Shields")#
writeData(wb, "Shields", coords_shields)#
#
addWorksheet(wb, "Optometrists")#
writeData(wb, "Optometrists", roc_opt)#
#
# Save the workbook to the desktop#
output_file <- file.path(Sys.getenv("HOME"), "Desktop", "ROC_output.xlsx")#
saveWorkbook(wb, output_file, overwrite = TRUE)
# Uncomment and run the following lines if the packages are not installed#
# install.packages("readxl")#
# install.packages("pROC")#
# install.packages("openxlsx")#
#
library(readxl)#
library(pROC)#
library(openxlsx)#
#
# Specify the file path to the Patientförteckning.xlsx file#
data_file <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Patientförteckning.xlsx"#
#
# Read the data (assuming the first sheet contains the data)#
data <- read_excel(data_file)#
#
# Define the binary outcome.#
# Here we assume that the column "Nevus 0 Melanoma 1" (second column) contains the true diagnosis#
outcome <- data$`Nevus 0 Melanoma 1`#
#
# Compute ROC curves for the three single-score predictors#
roc_melanoma <- roc(response = outcome, predictor = data$`MelAInoma score`)#
roc_oncologist <- roc(response = outcome, predictor = data$`MOLES score by ocular oncologist, first photo`)#
roc_shields <- roc(response = outcome, predictor = data$`Number of risk factors according to Shields`)#
#
# Extract coordinates (FPR and sensitivity) for each ROC curve#
coords_melanoma <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_melanoma$specificities,#
  `True positive rate (sensitivity)` = roc_melanoma$sensitivities#
)#
#
coords_oncologist <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_oncologist$specificities,#
  `True positive rate (sensitivity)` = roc_oncologist$sensitivities#
)#
#
coords_shields <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_shields$specificities,#
  `True positive rate (sensitivity)` = roc_shields$sensitivities#
)#
#
# For optometrists, identify the 29 columns by matching the column names pattern#
opt_columns <- grep("MOLES score optometrist/optician", names(data), value = TRUE)#
#
# Compute ROC curves for each optometrist/optician column,#
# ensuring the predictors are numeric.#
roc_list <- lapply(opt_columns, function(col) {#
  predictor_vals <- as.numeric(data[[col]])#
  roc(response = outcome, predictor = predictor_vals)#
})#
#
# Define a common grid for false positive rates (from 0 to 1 by 0.01)#
fpr_grid <- seq(0, 1, by = 0.01)#
#
# Interpolate sensitivities for each ROC at the grid of FPR values.#
# pROC's coords function requires specificity (1 - FPR) as input.#
sensitivity_matrix <- sapply(roc_list, function(roc_obj) {#
  sapply(fpr_grid, function(fpr) {#
    spec_val <- 1 - fpr#
    tryCatch(#
      as.numeric(coords(roc_obj, x = spec_val, input = "specificity", ret = "sensitivity",#
                         transpose = FALSE, as.matrix = FALSE)),#
      error = function(e) NA#
    )#
  })#
})#
#
# Issue a warning if the sensitivity matrix is entirely NA.#
if(all(is.na(sensitivity_matrix))) {#
  warning("No valid sensitivity data generated for optometrists. Check if the predictor columns have sufficient variation and non-missing values.")#
}#
#
# Compute the mean sensitivity and its 95% confidence interval at each FPR grid point.#
mean_sensitivity <- rowMeans(sensitivity_matrix, na.rm = TRUE)#
sd_sensitivity <- apply(sensitivity_matrix, 1, sd, na.rm = TRUE)#
n <- ncol(sensitivity_matrix)#
se_sensitivity <- sd_sensitivity / sqrt(n)#
lower_ci <- mean_sensitivity - 1.96 * se_sensitivity#
upper_ci <- mean_sensitivity + 1.96 * se_sensitivity#
#
roc_opt <- data.frame(#
  `False positive rate (1-specificity)` = fpr_grid,#
  `True positive rate (sensitivity)` = mean_sensitivity,#
  `Lower 95% CI` = lower_ci,#
  `Upper 95% CI` = upper_ci#
)#
#
# Create an Excel workbook and add worksheets for each ROC curve#
wb <- createWorkbook()#
addWorksheet(wb, "MelAInoma")#
writeData(wb, "MelAInoma", coords_melanoma)#
#
addWorksheet(wb, "MOLES_Oncologist")#
writeData(wb, "MOLES_Oncologist", coords_oncologist)#
#
addWorksheet(wb, "Shields")#
writeData(wb, "Shields", coords_shields)#
#
addWorksheet(wb, "Optometrists")#
writeData(wb, "Optometrists", roc_opt)#
#
# Save the workbook to the desktop#
output_file <- file.path(Sys.getenv("HOME"), "Desktop", "ROC_output.xlsx")#
saveWorkbook(wb, output_file, overwrite = TRUE)
# Uncomment and run the following lines if the packages are not installed#
# install.packages("readxl")#
# install.packages("pROC")#
# install.packages("openxlsx")#
#
library(readxl)#
library(pROC)#
library(openxlsx)#
#
# Specify the file path to the Patientförteckning.xlsx file#
data_file <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Patientförteckning.xlsx"#
#
# Read the data (assuming the first sheet contains the data)#
data <- read_excel(data_file)#
#
# Define the binary outcome.#
# Here we assume that the column "Nevus 0 Melanoma 1" (second column) contains the true diagnosis#
outcome <- data$`Nevus 0 Melanoma 1`#
#
# Compute ROC curves for the three single-score predictors#
roc_melanoma <- roc(response = outcome, predictor = data$`MelAInoma score`)#
roc_oncologist <- roc(response = outcome, predictor = data$`MOLES score by ocular oncologist, first photo`)#
roc_shields <- roc(response = outcome, predictor = data$`Number of risk factors according to Shields`)#
#
# Extract coordinates (FPR and sensitivity) for each ROC curve#
coords_melanoma <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_melanoma$specificities,#
  `True positive rate (sensitivity)` = roc_melanoma$sensitivities#
)#
#
coords_oncologist <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_oncologist$specificities,#
  `True positive rate (sensitivity)` = roc_oncologist$sensitivities#
)#
#
coords_shields <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_shields$specificities,#
  `True positive rate (sensitivity)` = roc_shields$sensitivities#
)#
#
# For optometrists, identify the 29 columns by matching the column names pattern#
opt_columns <- grep("MOLES score optometrist/optician", names(data), value = TRUE)#
#
# Compute ROC curves for each optometrist/optician column,#
# ensuring the predictors are numeric.#
roc_list <- lapply(opt_columns, function(col) {#
  predictor_vals <- as.numeric(data[[col]])#
  roc(response = outcome, predictor = predictor_vals)#
})#
#
# Define a common grid for false positive rates (from 0 to 1 by 0.01)#
fpr_grid <- seq(0, 1, by = 0.01)#
#
# Interpolate sensitivities for each ROC at the grid of FPR values.#
# Instead of using pROC's coords, we extract the ROC coordinates and then use approx.#
sensitivity_matrix <- sapply(roc_list, function(roc_obj) {#
  # Compute FPR and sensitivity vectors from the ROC object#
  fpr_values <- 1 - roc_obj$specificities#
  sensitivity_values <- roc_obj$sensitivities#
  # If only one point is available, replicate it over the grid#
  if(length(fpr_values) < 2) {#
    rep(sensitivity_values[1], length(fpr_grid))#
  } else {#
    # Interpolate sensitivities at the defined FPR grid. rule=2 allows extrapolation.#
    approx(x = fpr_values, y = sensitivity_values, xout = fpr_grid, rule = 2)$y#
  }#
})#
#
# Issue a warning if the sensitivity matrix is entirely NA.#
if(all(is.na(sensitivity_matrix))) {#
  warning("No valid sensitivity data generated for optometrists. Check if the predictor columns have sufficient variation and non-missing values.")#
}#
#
# Compute the mean sensitivity and its 95% confidence interval at each FPR grid point.#
mean_sensitivity <- rowMeans(sensitivity_matrix, na.rm = TRUE)#
sd_sensitivity <- apply(sensitivity_matrix, 1, sd, na.rm = TRUE)#
n <- ncol(sensitivity_matrix)#
se_sensitivity <- sd_sensitivity / sqrt(n)#
lower_ci <- mean_sensitivity - 1.96 * se_sensitivity#
upper_ci <- mean_sensitivity + 1.96 * se_sensitivity#
#
roc_opt <- data.frame(#
  `False positive rate (1-specificity)` = fpr_grid,#
  `True positive rate (sensitivity)` = mean_sensitivity,#
  `Lower 95% CI` = lower_ci,#
  `Upper 95% CI` = upper_ci#
)#
#
# Create an Excel workbook and add worksheets for each ROC curve#
wb <- createWorkbook()#
addWorksheet(wb, "MelAInoma")#
writeData(wb, "MelAInoma", coords_melanoma)#
#
addWorksheet(wb, "MOLES_Oncologist")#
writeData(wb, "MOLES_Oncologist", coords_oncologist)#
#
addWorksheet(wb, "Shields")#
writeData(wb, "Shields", coords_shields)#
#
addWorksheet(wb, "Optometrists")#
writeData(wb, "Optometrists", roc_opt)#
#
# Save the workbook to the desktop#
output_file <- file.path(Sys.getenv("HOME"), "Desktop", "ROC_output.xlsx")#
saveWorkbook(wb, output_file, overwrite = TRUE)
# Uncomment and run the following lines if the packages are not installed#
# install.packages("readxl")#
# install.packages("pROC")#
# install.packages("openxlsx")#
#
library(readxl)#
library(pROC)#
library(openxlsx)#
#
###############################
## 1. Data Loading and Setup#
###############################
#
# Specify the file path to the Patientförteckning.xlsx file#
data_file <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Patientförteckning.xlsx"#
#
# Read the data (assuming the first sheet contains the data)#
data <- read_excel(data_file)#
#
# Define the binary outcome.#
# Here we assume that the column "Nevus 0 Melanoma 1" (second column) contains the true diagnosis#
outcome <- data$`Nevus 0 Melanoma 1`#
#
###############################################
## 2. Compute ROC Curves for Individual Methods#
###############################################
#
# Compute ROC curves for the three single-score predictors.#
roc_melanoma <- roc(response = outcome, predictor = data$`MelAInoma score`)#
roc_oncologist <- roc(response = outcome, predictor = data$`MOLES score by ocular oncologist, first photo`)#
roc_shields <- roc(response = outcome, predictor = data$`Number of risk factors according to Shields`)#
#
# Extract ROC coordinates for plotting/output (FPR and sensitivity)#
coords_melanoma <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_melanoma$specificities,#
  `True positive rate (sensitivity)` = roc_melanoma$sensitivities#
)#
coords_oncologist <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_oncologist$specificities,#
  `True positive rate (sensitivity)` = roc_oncologist$sensitivities#
)#
coords_shields <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_shields$specificities,#
  `True positive rate (sensitivity)` = roc_shields$sensitivities#
)#
#
########################################################
## 3. ROC Curves and AUC for Optometrist/Optician Data#
########################################################
#
# Identify the 29 optometrist/optician columns by matching the column names pattern.#
opt_columns <- grep("MOLES score optometrist/optician", names(data), value = TRUE)#
#
# Compute ROC curves for each optometrist/optician column,#
# ensuring the predictors are numeric.#
roc_list <- lapply(opt_columns, function(col) {#
  predictor_vals <- as.numeric(data[[col]])#
  roc(response = outcome, predictor = predictor_vals)#
})#
#
# Define a common grid for false positive rates (from 0 to 1 by 0.01)#
fpr_grid <- seq(0, 1, by = 0.01)#
#
# For each ROC in the opt_list, extract the FPR and sensitivity vectors,#
# then interpolate (using approx) to obtain sensitivities at the defined grid.#
# If an ROC curve is degenerate (only one point), replicate the sensitivity.#
sensitivity_matrix <- sapply(roc_list, function(roc_obj) {#
  fpr_values <- 1 - roc_obj$specificities#
  sensitivity_values <- roc_obj$sensitivities#
  if(length(fpr_values) < 2) {#
    rep(sensitivity_values[1], length(fpr_grid))#
  } else {#
    approx(x = fpr_values, y = sensitivity_values, xout = fpr_grid, rule = 2)$y#
  }#
})#
#
if(all(is.na(sensitivity_matrix))) {#
  warning("No valid sensitivity data generated for optometrists. Check the predictor columns for sufficient variation and missing values.")#
}#
#
# Compute the aggregated optometrist ROC curve (mean sensitivity at each FPR grid point)#
mean_sensitivity <- rowMeans(sensitivity_matrix, na.rm = TRUE)#
sd_sensitivity <- apply(sensitivity_matrix, 1, sd, na.rm = TRUE)#
n_opt <- ncol(sensitivity_matrix)#
se_sensitivity <- sd_sensitivity / sqrt(n_opt)#
lower_ci_grid <- mean_sensitivity - 1.96 * se_sensitivity#
upper_ci_grid <- mean_sensitivity + 1.96 * se_sensitivity#
#
roc_opt_coords <- data.frame(#
  `False positive rate (1-specificity)` = fpr_grid,#
  `True positive rate (sensitivity)` = mean_sensitivity,#
  `Lower 95% CI` = lower_ci_grid,#
  `Upper 95% CI` = upper_ci_grid#
)#
#
#####################################
## 4. AUC, 95% CI, and p-values#
#####################################
#
# For the three individual predictors, compute AUC and 95% CI.#
auc_melanoma <- auc(roc_melanoma)#
ci_melanoma <- ci.auc(roc_melanoma)#
auc_oncologist <- auc(roc_oncologist)#
ci_oncologist <- ci.auc(roc_oncologist)#
auc_shields <- auc(roc_shields)#
ci_shields <- ci.auc(roc_shields)#
#
# For optometrists, compute AUC for each ROC curve and then summarize.#
auc_opt_values <- sapply(roc_list, function(roc_obj) as.numeric(auc(roc_obj)))#
mean_auc_opt <- mean(auc_opt_values, na.rm = TRUE)#
se_auc_opt <- sd(auc_opt_values, na.rm = TRUE) / sqrt(length(auc_opt_values))#
ci_opt <- c(mean_auc_opt - 1.96 * se_auc_opt, mean_auc_opt + 1.96 * se_auc_opt)#
#
# p-values can be obtained by testing the difference between cases and controls#
# using a Wilcoxon rank-sum test (equivalent to testing AUC > 0.5 for ROC analysis).#
#
p_melanoma <- wilcox.test(data$`MelAInoma score`[outcome==1], #
                          data$`MelAInoma score`[outcome==0],#
                          alternative = "greater")$p.value#
#
p_oncologist <- wilcox.test(data$`MOLES score by ocular oncologist, first photo`[outcome==1], #
                           data$`MOLES score by ocular oncologist, first photo`[outcome==0],#
                           alternative = "greater")$p.value#
#
p_shields <- wilcox.test(data$`Number of risk factors according to Shields`[outcome==1], #
                         data$`Number of risk factors according to Shields`[outcome==0],#
                         alternative = "greater")$p.value#
#
# For optometrists, compute the p-value for each column and take the mean (or report the distribution).#
p_opt_values <- sapply(opt_columns, function(col) {#
  predictor_vals <- as.numeric(data[[col]])#
  wilcox.test(predictor_vals[outcome==1], predictor_vals[outcome==0],#
              alternative = "greater")$p.value#
})#
mean_p_opt <- mean(p_opt_values, na.rm = TRUE)#
#
# Create a summary table for AUC and p-values.#
auc_table <- data.frame(#
  Method = c("MelAInoma score", #
             "MOLES score by ocular oncologist, first photo", #
             "Number of risk factors according to Shields", #
             "Optometrist/optician (mean of 29)"),#
  AUC = c(as.numeric(auc_melanoma), as.numeric(auc_oncologist), as.numeric(auc_shields), mean_auc_opt),#
  CI_Lower = c(ci_melanoma[1], ci_oncologist[1], ci_shields[1], ci_opt[1]),#
  CI_Upper = c(ci_melanoma[3], ci_oncologist[3], ci_shields[3], ci_opt[2]),#
  p_value = c(p_melanoma, p_oncologist, p_shields, mean_p_opt)#
)#
#
#####################################################
## 5. Pairwise Comparisons Between Methods#
#####################################################
#
# For the three individual predictors, perform pairwise ROC comparisons using DeLong's test.#
comp_mel_vs_onco <- roc.test(roc_melanoma, roc_oncologist, method = "delong")#
comp_mel_vs_shields <- roc.test(roc_melanoma, roc_shields, method = "delong")#
comp_onco_vs_shields <- roc.test(roc_oncologist, roc_shields, method = "delong")#
#
# For comparisons involving optometrists, compare the vector of 29 AUCs to each individual method's AUC using a one-sample t test.#
comp_opt_vs_melanoma <- t.test(auc_opt_values, mu = as.numeric(auc_melanoma), alternative = "two.sided")#
comp_opt_vs_oncologist <- t.test(auc_opt_values, mu = as.numeric(auc_oncologist), alternative = "two.sided")#
comp_opt_vs_shields <- t.test(auc_opt_values, mu = as.numeric(auc_shields), alternative = "two.sided")#
#
# Create a comparisons table.#
comparisons <- data.frame(#
  Comparison = c("MelAInoma vs MOLES Oncologist", #
                 "MelAInoma vs Shields",#
                 "MOLES Oncologist vs Shields",#
                 "Optometrists vs MelAInoma",#
                 "Optometrists vs MOLES Oncologist",#
                 "Optometrists vs Shields"),#
  Test = c("DeLong", "DeLong", "DeLong", "t-test", "t-test", "t-test"),#
  p_value = c(comp_mel_vs_onco$p.value,#
              comp_mel_vs_shields$p.value,#
              comp_onco_vs_shields$p.value,#
              comp_opt_vs_melanoma$p.value,#
              comp_opt_vs_oncologist$p.value,#
              comp_opt_vs_shields$p.value)#
)#
#
##############################################
## 6. Summary of "Best" Method (Objective)#
##############################################
#
# Identify the method with the highest AUC.#
auc_values <- c(as.numeric(auc_melanoma), as.numeric(auc_oncologist), as.numeric(auc_shields), mean_auc_opt)#
methods <- c("MelAInoma score", #
             "MOLES score by ocular oncologist, first photo", #
             "Number of risk factors according to Shields", #
             "Optometrist/optician (mean of 29)")#
best_method <- methods[which.max(auc_values)]#
# Note: Statistical significance of differences is indicated in the comparisons table.#
#
#####################################
## 7. Output to Excel Workbook#
#####################################
#
wb <- createWorkbook()#
#
# Add ROC coordinates for each individual method.#
addWorksheet(wb, "MelAInoma ROC")#
writeData(wb, "MelAInoma ROC", coords_melanoma)#
#
addWorksheet(wb, "Oncologist ROC")#
writeData(wb, "Oncologist ROC", coords_oncologist)#
#
addWorksheet(wb, "Shields ROC")#
writeData(wb, "Shields ROC", coords_shields)#
#
# Add aggregated optometrist ROC coordinates.#
addWorksheet(wb, "Optometrists ROC")#
writeData(wb, "Optometrists ROC", roc_opt_coords)#
#
# Add AUC and p-value summary.#
addWorksheet(wb, "AUC Summary")#
writeData(wb, "AUC Summary", auc_table)#
#
# Add pairwise comparisons.#
addWorksheet(wb, "Comparisons")#
writeData(wb, "Comparisons", comparisons)#
#
# Optionally, add a summary comment in a separate sheet.#
summary_text <- paste0("Based on the computed AUC values, the method with the highest AUC is: ", best_method, #
                       ". Pairwise comparisons (DeLong tests for individual predictors and t-tests for optometrist AUCs) are provided in the 'Comparisons' sheet. Interpret the p-values accordingly.")#
addWorksheet(wb, "Summary")#
writeData(wb, "Summary", summary_text)#
#
# Save the workbook to the desktop#
output_file <- file.path(Sys.getenv("HOME"), "Desktop", "ROC_output.xlsx")#
saveWorkbook(wb, output_file, overwrite = TRUE)
# Uncomment the following lines if the packages are not installed:#
# install.packages("readxl")#
# install.packages("pROC")#
# install.packages("openxlsx")#
#
library(readxl)#
library(pROC)#
library(openxlsx)#
#
###############################
## 1. Data Loading and Setup#
###############################
#
# Specify the file path to the Patientförteckning.xlsx file#
data_file <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Patientförteckning.xlsx"#
#
# Read the data (assuming the first sheet contains the data)#
data <- read_excel(data_file)#
#
# Define the binary outcome.#
# Here we assume that the column "Nevus 0 Melanoma 1" (second column) contains the true diagnosis#
outcome <- data$`Nevus 0 Melanoma 1`#
#
###############################################
## 2. Compute ROC Curves for Individual Methods#
###############################################
#
# Compute ROC curves for the three single-score predictors.#
roc_melanoma <- roc(response = outcome, predictor = data$`MelAInoma score`)#
roc_oncologist <- roc(response = outcome, predictor = data$`MOLES score by ocular oncologist, first photo`)#
roc_shields <- roc(response = outcome, predictor = data$`Number of risk factors according to Shields`)#
#
# Extract ROC coordinates (False Positive Rate and Sensitivity)#
coords_melanoma <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_melanoma$specificities,#
  `True positive rate (sensitivity)` = roc_melanoma$sensitivities#
)#
coords_oncologist <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_oncologist$specificities,#
  `True positive rate (sensitivity)` = roc_oncologist$sensitivities#
)#
coords_shields <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_shields$specificities,#
  `True positive rate (sensitivity)` = roc_shields$sensitivities#
)#
#
########################################################
## 3. ROC Curves and AUC for Optometrist/Optician Data#
########################################################
#
# Identify the 29 optometrist/optician columns by matching the column names pattern.#
opt_columns <- grep("MOLES score optometrist/optician", names(data), value = TRUE)#
#
# Compute ROC curves for each optometrist/optician column, ensuring predictors are numeric.#
roc_list <- lapply(opt_columns, function(col) {#
  predictor_vals <- as.numeric(data[[col]])#
  roc(response = outcome, predictor = predictor_vals)#
})#
#
# Define a common grid for false positive rates (0 to 1 by 0.01)#
fpr_grid <- seq(0, 1, by = 0.01)#
#
# For each ROC object, extract FPR and sensitivity vectors and interpolate using approx.#
sensitivity_matrix <- sapply(roc_list, function(roc_obj) {#
  fpr_values <- 1 - roc_obj$specificities#
  sensitivity_values <- roc_obj$sensitivities#
  if(length(fpr_values) < 2) {#
    rep(sensitivity_values[1], length(fpr_grid))#
  } else {#
    approx(x = fpr_values, y = sensitivity_values, xout = fpr_grid, rule = 2)$y#
  }#
})#
#
# If all interpolated values are NA, issue a warning.#
if(all(is.na(sensitivity_matrix))) {#
  warning("No valid sensitivity data generated for optometrists. Check the predictor columns for sufficient variation and missing values.")#
}#
#
# Compute aggregated optometrist ROC coordinates (mean sensitivity and 95% CI at each FPR grid point)#
mean_sensitivity <- rowMeans(sensitivity_matrix, na.rm = TRUE)#
sd_sensitivity <- apply(sensitivity_matrix, 1, sd, na.rm = TRUE)#
n_opt <- ncol(sensitivity_matrix)#
se_sensitivity <- sd_sensitivity / sqrt(n_opt)#
lower_ci_grid <- mean_sensitivity - 1.96 * se_sensitivity#
upper_ci_grid <- mean_sensitivity + 1.96 * se_sensitivity#
#
roc_opt_coords <- data.frame(#
  `False positive rate (1-specificity)` = fpr_grid,#
  `True positive rate (sensitivity)` = mean_sensitivity,#
  `Lower 95% CI` = lower_ci_grid,#
  `Upper 95% CI` = upper_ci_grid#
)#
#
#####################################
## 4. AUC, 95% CI, and p-values#
#####################################
#
# Calculate AUC and 95% CI for the three individual predictors.#
auc_melanoma <- auc(roc_melanoma)#
ci_melanoma <- ci.auc(roc_melanoma)#
auc_oncologist <- auc(roc_oncologist)#
ci_oncologist <- ci.auc(roc_oncologist)#
auc_shields <- auc(roc_shields)#
ci_shields <- ci.auc(roc_shields)#
#
# For optometrists, compute AUC for each ROC curve and summarize.#
auc_opt_values <- sapply(roc_list, function(roc_obj) as.numeric(auc(roc_obj)))#
mean_auc_opt <- mean(auc_opt_values, na.rm = TRUE)#
se_auc_opt <- sd(auc_opt_values, na.rm = TRUE) / sqrt(length(auc_opt_values))#
ci_opt <- c(mean_auc_opt - 1.96 * se_auc_opt, mean_auc_opt + 1.96 * se_auc_opt)#
#
# Compute p-values using Wilcoxon rank-sum tests.#
p_melanoma <- wilcox.test(data$`MelAInoma score`[outcome==1], #
                          data$`MelAInoma score`[outcome==0],#
                          alternative = "greater")$p.value#
p_oncologist <- wilcox.test(data$`MOLES score by ocular oncologist, first photo`[outcome==1], #
                           data$`MOLES score by ocular oncologist, first photo`[outcome==0],#
                           alternative = "greater")$p.value#
p_shields <- wilcox.test(data$`Number of risk factors according to Shields`[outcome==1], #
                         data$`Number of risk factors according to Shields`[outcome==0],#
                         alternative = "greater")$p.value#
#
# For optometrists, compute p-values for each column.#
p_opt_values <- sapply(opt_columns, function(col) {#
  predictor_vals <- as.numeric(data[[col]])#
  wilcox.test(predictor_vals[outcome==1], predictor_vals[outcome==0],#
              alternative = "greater")$p.value#
})#
mean_p_opt <- mean(p_opt_values, na.rm = TRUE)#
#
# Create an AUC summary table.#
auc_table <- data.frame(#
  Method = c("MelAInoma score", #
             "MOLES score by ocular oncologist, first photo", #
             "Number of risk factors according to Shields", #
             "Optometrist/optician (mean of 29)"),#
  AUC = c(as.numeric(auc_melanoma), as.numeric(auc_oncologist), as.numeric(auc_shields), mean_auc_opt),#
  CI_Lower = c(ci_melanoma[1], ci_oncologist[1], ci_shields[1], ci_opt[1]),#
  CI_Upper = c(ci_melanoma[3], ci_oncologist[3], ci_shields[3], ci_opt[2]),#
  p_value = c(p_melanoma, p_oncologist, p_shields, mean_p_opt)#
)#
#
# Print the AUC summary table to the console.#
print("AUC Summary Table:")#
print(auc_table)#
#
###############################################
## 5. Pairwise Comparisons Between Methods#
###############################################
#
# Perform pairwise ROC comparisons using DeLong's test for individual predictors.#
comp_mel_vs_onco <- roc.test(roc_melanoma, roc_oncologist, method = "delong")#
comp_mel_vs_shields <- roc.test(roc_melanoma, roc_shields, method = "delong")#
comp_onco_vs_shields <- roc.test(roc_oncologist, roc_shields, method = "delong")#
#
# For comparisons involving optometrists, compare the vector of 29 AUCs to each individual method's AUC using one-sample t-tests.#
comp_opt_vs_melanoma <- t.test(auc_opt_values, mu = as.numeric(auc_melanoma), alternative = "two.sided")#
comp_opt_vs_oncologist <- t.test(auc_opt_values, mu = as.numeric(auc_oncologist), alternative = "two.sided")#
comp_opt_vs_shields <- t.test(auc_opt_values, mu = as.numeric(auc_shields), alternative = "two.sided")#
#
# Create a comparisons table.#
comparisons <- data.frame(#
  Comparison = c("MelAInoma vs MOLES Oncologist", #
                 "MelAInoma vs Shields",#
                 "MOLES Oncologist vs Shields",#
                 "Optometrists vs MelAInoma",#
                 "Optometrists vs MOLES Oncologist",#
                 "Optometrists vs Shields"),#
  Test = c("DeLong", "DeLong", "DeLong", "t-test", "t-test", "t-test"),#
  p_value = c(comp_mel_vs_onco$p.value,#
              comp_mel_vs_shields$p.value,#
              comp_onco_vs_shields$p.value,#
              comp_opt_vs_melanoma$p.value,#
              comp_opt_vs_oncologist$p.value,#
              comp_opt_vs_shields$p.value)#
)#
#
# Print the comparisons table to the console.#
print("Pairwise Comparisons Table:")#
print(comparisons)#
#
###############################################
## 6. Summary of "Best" Method (Objective)#
###############################################
#
auc_values <- c(as.numeric(auc_melanoma), as.numeric(auc_oncologist), as.numeric(auc_shields), mean_auc_opt)#
methods <- c("MelAInoma score", #
             "MOLES score by ocular oncologist, first photo", #
             "Number of risk factors according to Shields", #
             "Optometrist/optician (mean of 29)")#
best_method <- methods[which.max(auc_values)]#
#
summary_text <- paste0("Based on the computed AUC values, the method with the highest AUC is: ", best_method, #
                       ". See the 'Comparisons' sheet for pairwise statistical tests (DeLong for individual predictors and t-tests for optometrist AUCs).")#
#
print("Summary:")#
print(summary_text)#
#
#####################################
## 7. Output to Excel Workbook#
#####################################
#
wb <- createWorkbook()#
#
# Add worksheets with ROC coordinates.#
addWorksheet(wb, "MelAInoma ROC")#
writeData(wb, "MelAInoma ROC", coords_melanoma)#
#
addWorksheet(wb, "Oncologist ROC")#
writeData(wb, "Oncologist ROC", coords_oncologist)#
#
addWorksheet(wb, "Shields ROC")#
writeData(wb, "Shields ROC", coords_shields)#
#
addWorksheet(wb, "Optometrists ROC")#
writeData(wb, "Optometrists ROC", roc_opt_coords)#
#
# Add the AUC summary table.#
addWorksheet(wb, "AUC Summary")#
writeData(wb, "AUC Summary", auc_table)#
#
# Add the pairwise comparisons table.#
addWorksheet(wb, "Comparisons")#
writeData(wb, "Comparisons", comparisons)#
#
# Add the summary text.#
addWorksheet(wb, "Summary")#
writeData(wb, "Summary", summary_text)#
#
# Save the workbook to the desktop.#
output_file <- file.path(Sys.getenv("HOME"), "Desktop", "ROC_output.xlsx")#
saveWorkbook(wb, output_file, overwrite = TRUE)
# Uncomment the following lines if the packages are not installed:#
# install.packages("readxl")#
# install.packages("pROC")#
# install.packages("openxlsx")#
library(readxl)#
library(pROC)#
library(openxlsx)#
###############################
## 1. Data Loading and Setup#
###############################
# Specify the file path to the Patientförteckning.xlsx file#
data_file <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Patientförteckning.xlsx"#
# Read the data (assuming the first sheet contains the data)#
data <- read_excel(data_file)#
# Define the binary outcome.#
# Here we assume that the column "Nevus 0 Melanoma 1" (second column) contains the true diagnosis#
outcome <- data$`Nevus 0 Melanoma 1`#
###############################################
## 2. Compute ROC Curves for Individual Methods#
###############################################
# Compute ROC curves for the three single-score predictors.#
roc_melanoma <- roc(response = outcome, predictor = data$`MelAInoma score`)#
roc_oncologist <- roc(response = outcome, predictor = data$`MOLES score by ocular oncologist, first photo`)#
roc_shields <- roc(response = outcome, predictor = data$`Number of risk factors according to Shields`)#
# Extract ROC coordinates (False Positive Rate and Sensitivity)#
coords_melanoma <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_melanoma$specificities,#
  `True positive rate (sensitivity)` = roc_melanoma$sensitivities#
)#
coords_oncologist <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_oncologist$specificities,#
  `True positive rate (sensitivity)` = roc_oncologist$sensitivities#
)#
coords_shields <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_shields$specificities,#
  `True positive rate (sensitivity)` = roc_shields$sensitivities#
)#
########################################################
## 3. ROC Curves and AUC for Optometrist/Optician Data#
########################################################
# Identify the 29 optometrist/optician columns by matching the column names pattern.#
opt_columns <- grep("MOLES score optometrist/optician", names(data), value = TRUE)#
# Compute ROC curves for each optometrist/optician column, ensuring predictors are numeric.#
roc_list <- lapply(opt_columns, function(col) {#
  predictor_vals <- as.numeric(data[[col]])#
  roc(response = outcome, predictor = predictor_vals)#
})#
# Define a common grid for false positive rates (0 to 1 by 0.01)#
fpr_grid <- seq(0, 1, by = 0.01)#
# For each ROC object, extract FPR and sensitivity vectors and interpolate using approx.#
sensitivity_matrix <- sapply(roc_list, function(roc_obj) {#
  fpr_values <- 1 - roc_obj$specificities#
  sensitivity_values <- roc_obj$sensitivities#
  if(length(fpr_values) < 2) {#
    rep(sensitivity_values[1], length(fpr_grid))#
  } else {#
    approx(x = fpr_values, y = sensitivity_values, xout = fpr_grid, rule = 2)$y#
  }#
})#
# If all interpolated values are NA, issue a warning.#
if(all(is.na(sensitivity_matrix))) {#
  warning("No valid sensitivity data generated for optometrists. Check the predictor columns for sufficient variation and missing values.")#
}#
# Compute aggregated optometrist ROC coordinates (mean sensitivity and 95% CI at each FPR grid point)#
mean_sensitivity <- rowMeans(sensitivity_matrix, na.rm = TRUE)#
sd_sensitivity <- apply(sensitivity_matrix, 1, sd, na.rm = TRUE)#
n_opt <- ncol(sensitivity_matrix)#
se_sensitivity <- sd_sensitivity / sqrt(n_opt)#
lower_ci_grid <- mean_sensitivity - 1.96 * se_sensitivity#
upper_ci_grid <- mean_sensitivity + 1.96 * se_sensitivity#
roc_opt_coords <- data.frame(#
  `False positive rate (1-specificity)` = fpr_grid,#
  `True positive rate (sensitivity)` = mean_sensitivity,#
  `Lower 95% CI` = lower_ci_grid,#
  `Upper 95% CI` = upper_ci_grid#
)#
#####################################
## 4. AUC, 95% CI, and p-values#
#####################################
# Calculate AUC and 95% CI for the three individual predictors.#
auc_melanoma <- auc(roc_melanoma)#
ci_melanoma <- ci.auc(roc_melanoma)#
auc_oncologist <- auc(roc_oncologist)#
ci_oncologist <- ci.auc(roc_oncologist)#
auc_shields <- auc(roc_shields)#
ci_shields <- ci.auc(roc_shields)#
# For optometrists, compute AUC for each ROC curve and summarize.#
auc_opt_values <- sapply(roc_list, function(roc_obj) as.numeric(auc(roc_obj)))#
mean_auc_opt <- mean(auc_opt_values, na.rm = TRUE)#
se_auc_opt <- sd(auc_opt_values, na.rm = TRUE) / sqrt(length(auc_opt_values))#
ci_opt <- c(mean_auc_opt - 1.96 * se_auc_opt, mean_auc_opt + 1.96 * se_auc_opt)#
# Compute p-values using Wilcoxon rank-sum tests.#
p_melanoma <- wilcox.test(data$`MelAInoma score`[outcome==1], #
                          data$`MelAInoma score`[outcome==0],#
                          alternative = "greater")$p.value#
p_oncologist <- wilcox.test(data$`MOLES score by ocular oncologist, first photo`[outcome==1], #
                           data$`MOLES score by ocular oncologist, first photo`[outcome==0],#
                           alternative = "greater")$p.value#
p_shields <- wilcox.test(data$`Number of risk factors according to Shields`[outcome==1], #
                         data$`Number of risk factors according to Shields`[outcome==0],#
                         alternative = "greater")$p.value#
# For optometrists, compute p-values for each column.#
p_opt_values <- sapply(opt_columns, function(col) {#
  predictor_vals <- as.numeric(data[[col]])#
  wilcox.test(predictor_vals[outcome==1], predictor_vals[outcome==0],#
              alternative = "greater")$p.value#
})#
mean_p_opt <- mean(p_opt_values, na.rm = TRUE)#
# Create an AUC summary table.#
auc_table <- data.frame(#
  Method = c("MelAInoma score", #
             "MOLES score by ocular oncologist, first photo", #
             "Number of risk factors according to Shields", #
             "Optometrist/optician (mean of 29)"),#
  AUC = c(as.numeric(auc_melanoma), as.numeric(auc_oncologist), as.numeric(auc_shields), mean_auc_opt),#
  CI_Lower = c(ci_melanoma[1], ci_oncologist[1], ci_shields[1], ci_opt[1]),#
  CI_Upper = c(ci_melanoma[3], ci_oncologist[3], ci_shields[3], ci_opt[2]),#
  p_value = c(p_melanoma, p_oncologist, p_shields, mean_p_opt)#
)#
# Print the AUC summary table to the console.#
print("AUC Summary Table:")#
print(auc_table)#
###############################################
## 5. Pairwise Comparisons Between Methods#
###############################################
# Perform pairwise ROC comparisons using DeLong's test for individual predictors.#
comp_mel_vs_onco <- roc.test(roc_melanoma, roc_oncologist, method = "delong")#
comp_mel_vs_shields <- roc.test(roc_melanoma, roc_shields, method = "delong")#
comp_onco_vs_shields <- roc.test(roc_oncologist, roc_shields, method = "delong")#
# For comparisons involving optometrists, compare the vector of 29 AUCs to each individual method's AUC using one-sample t-tests.#
comp_opt_vs_melanoma <- t.test(auc_opt_values, mu = as.numeric(auc_melanoma), alternative = "two.sided")#
comp_opt_vs_oncologist <- t.test(auc_opt_values, mu = as.numeric(auc_oncologist), alternative = "two.sided")#
comp_opt_vs_shields <- t.test(auc_opt_values, mu = as.numeric(auc_shields), alternative = "two.sided")#
# Create a comparisons table.#
comparisons <- data.frame(#
  Comparison = c("MelAInoma vs MOLES Oncologist", #
                 "MelAInoma vs Shields",#
                 "MOLES Oncologist vs Shields",#
                 "Optometrists vs MelAInoma",#
                 "Optometrists vs MOLES Oncologist",#
                 "Optometrists vs Shields"),#
  Test = c("DeLong", "DeLong", "DeLong", "t-test", "t-test", "t-test"),#
  p_value = c(comp_mel_vs_onco$p.value,#
              comp_mel_vs_shields$p.value,#
              comp_onco_vs_shields$p.value,#
              comp_opt_vs_melanoma$p.value,#
              comp_opt_vs_oncologist$p.value,#
              comp_opt_vs_shields$p.value)#
)#
# Print the comparisons table to the console.#
print("Pairwise Comparisons Table:")#
print(comparisons)#
###############################################
## 6. Summary of "Best" Method (Objective)#
###############################################
auc_values <- c(as.numeric(auc_melanoma), as.numeric(auc_oncologist), as.numeric(auc_shields), mean_auc_opt)#
methods <- c("MelAInoma score", #
             "MOLES score by ocular oncologist, first photo", #
             "Number of risk factors according to Shields", #
             "Optometrist/optician (mean of 29)")#
best_method <- methods[which.max(auc_values)]#
summary_text <- paste0("Based on the computed AUC values, the method with the highest AUC is: ", best_method, #
                       ". See the 'Comparisons' sheet for pairwise statistical tests (DeLong for individual predictors and t-tests for optometrist AUCs).")#
print("Summary:")#
print(summary_text)#
#####################################
## 7. Output to Excel Workbook#
#####################################
wb <- createWorkbook()#
# Add worksheets with ROC coordinates.#
addWorksheet(wb, "MelAInoma ROC")#
writeData(wb, "MelAInoma ROC", coords_melanoma)#
addWorksheet(wb, "Oncologist ROC")#
writeData(wb, "Oncologist ROC", coords_oncologist)#
addWorksheet(wb, "Shields ROC")#
writeData(wb, "Shields ROC", coords_shields)#
addWorksheet(wb, "Optometrists ROC")#
writeData(wb, "Optometrists ROC", roc_opt_coords)#
# Add the AUC summary table.#
addWorksheet(wb, "AUC Summary")#
writeData(wb, "AUC Summary", auc_table)#
# Add the pairwise comparisons table.#
addWorksheet(wb, "Comparisons")#
writeData(wb, "Comparisons", comparisons)#
# Add the summary text.#
addWorksheet(wb, "Summary")#
writeData(wb, "Summary", summary_text)#
# Save the workbook to the desktop.#
output_file <- file.path(Sys.getenv("HOME"), "Desktop", "ROC_output.xlsx")#
saveWorkbook(wb, output_file, overwrite = TRUE)
# Uncomment the following lines if the packages are not installed:#
# install.packages("readxl")#
# install.packages("pROC")#
# install.packages("openxlsx")#
#
library(readxl)#
library(pROC)#
library(openxlsx)#
#
###############################
## 1. Data Loading and Setup#
###############################
#
# Specify the file path to the Patientförteckning.xlsx file#
data_file <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Patientförteckning.xlsx"#
#
# Read the data (assuming the first sheet contains the data)#
data <- read_excel(data_file)#
#
# Define the binary outcome.#
# Here we assume that the column "Nevus 0 Melanoma 1" (second column) contains the true diagnosis#
outcome <- data$`Nevus 0 Melanoma 1`#
#
###############################################
## 2. Compute ROC Curves for Individual Methods#
###############################################
#
# Compute ROC curves for the three single-score predictors.#
roc_melanoma <- roc(response = outcome, predictor = data$`MelAInoma score`)#
roc_oncologist <- roc(response = outcome, predictor = data$`MOLES score by ocular oncologist, first photo`)#
roc_shields <- roc(response = outcome, predictor = data$`Number of risk factors according to Shields`)#
#
# Define a common grid for false positive rates (0 to 1 by 0.01)#
fpr_grid <- seq(0, 1, by = 0.01)#
#
# Interpolate sensitivity for MelAInoma score to obtain more data points.#
fpr_values_melanoma <- 1 - roc_melanoma$specificities#
sensitivity_values_melanoma <- roc_melanoma$sensitivities#
interpolated_sensitivity_melanoma <- approx(x = fpr_values_melanoma,#
                                            y = sensitivity_values_melanoma,#
                                            xout = fpr_grid,#
                                            rule = 2)$y#
#
# Updated ROC coordinates for MelAInoma score using interpolation#
coords_melanoma <- data.frame(#
  `False positive rate (1-specificity)` = fpr_grid,#
  `True positive rate (sensitivity)` = interpolated_sensitivity_melanoma#
)#
#
# For the other two methods, we use the raw ROC coordinates.#
coords_oncologist <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_oncologist$specificities,#
  `True positive rate (sensitivity)` = roc_oncologist$sensitivities#
)#
coords_shields <- data.frame(#
  `False positive rate (1-specificity)` = 1 - roc_shields$specificities,#
  `True positive rate (sensitivity)` = roc_shields$sensitivities#
)#
#
########################################################
## 3. ROC Curves and AUC for Optometrist/Optician Data#
########################################################
#
# Identify the 29 optometrist/optician columns by matching the column names pattern.#
opt_columns <- grep("MOLES score optometrist/optician", names(data), value = TRUE)#
#
# Compute ROC curves for each optometrist/optician column, ensuring predictors are numeric.#
roc_list <- lapply(opt_columns, function(col) {#
  predictor_vals <- as.numeric(data[[col]])#
  roc(response = outcome, predictor = predictor_vals)#
})#
#
# Define a common grid for false positive rates (0 to 1 by 0.01)#
# (Reusing 'fpr_grid' defined above)#
#
# For each ROC object, extract FPR and sensitivity vectors and interpolate using approx.#
sensitivity_matrix <- sapply(roc_list, function(roc_obj) {#
  fpr_values <- 1 - roc_obj$specificities#
  sensitivity_values <- roc_obj$sensitivities#
  if(length(fpr_values) < 2) {#
    rep(sensitivity_values[1], length(fpr_grid))#
  } else {#
    approx(x = fpr_values, y = sensitivity_values, xout = fpr_grid, rule = 2)$y#
  }#
})#
#
# If all interpolated values are NA, issue a warning.#
if(all(is.na(sensitivity_matrix))) {#
  warning("No valid sensitivity data generated for optometrists. Check the predictor columns for sufficient variation and missing values.")#
}#
#
# Compute aggregated optometrist ROC coordinates (mean sensitivity and 95% CI at each FPR grid point)#
mean_sensitivity <- rowMeans(sensitivity_matrix, na.rm = TRUE)#
sd_sensitivity <- apply(sensitivity_matrix, 1, sd, na.rm = TRUE)#
n_opt <- ncol(sensitivity_matrix)#
se_sensitivity <- sd_sensitivity / sqrt(n_opt)#
lower_ci_grid <- mean_sensitivity - 1.96 * se_sensitivity#
upper_ci_grid <- mean_sensitivity + 1.96 * se_sensitivity#
#
roc_opt_coords <- data.frame(#
  `False positive rate (1-specificity)` = fpr_grid,#
  `True positive rate (sensitivity)` = mean_sensitivity,#
  `Lower 95% CI` = lower_ci_grid,#
  `Upper 95% CI` = upper_ci_grid#
)#
#
#####################################
## 4. AUC, 95% CI, and P-values#
#####################################
#
# Calculate AUC and 95% CI for the three individual predictors.#
auc_melanoma <- auc(roc_melanoma)#
ci_melanoma <- ci.auc(roc_melanoma)#
auc_oncologist <- auc(roc_oncologist)#
ci_oncologist <- ci.auc(roc_oncologist)#
auc_shields <- auc(roc_shields)#
ci_shields <- ci.auc(roc_shields)#
#
# For optometrists, compute AUC for each ROC curve and summarize.#
auc_opt_values <- sapply(roc_list, function(roc_obj) as.numeric(auc(roc_obj)))#
mean_auc_opt <- mean(auc_opt_values, na.rm = TRUE)#
se_auc_opt <- sd(auc_opt_values, na.rm = TRUE) / sqrt(length(auc_opt_values))#
ci_opt <- c(mean_auc_opt - 1.96 * se_auc_opt, mean_auc_opt + 1.96 * se_auc_opt)#
#
# Compute p-values using Wilcoxon rank-sum tests.#
p_melanoma <- wilcox.test(data$`MelAInoma score`[outcome==1], #
                          data$`MelAInoma score`[outcome==0],#
                          alternative = "greater")$p.value#
p_oncologist <- wilcox.test(data$`MOLES score by ocular oncologist, first photo`[outcome==1], #
                           data$`MOLES score by ocular oncologist, first photo`[outcome==0],#
                           alternative = "greater")$p.value#
p_shields <- wilcox.test(data$`Number of risk factors according to Shields`[outcome==1], #
                         data$`Number of risk factors according to Shields`[outcome==0],#
                         alternative = "greater")$p.value#
#
# For optometrists, compute p-values for each column.#
p_opt_values <- sapply(opt_columns, function(col) {#
  predictor_vals <- as.numeric(data[[col]])#
  wilcox.test(predictor_vals[outcome==1], predictor_vals[outcome==0],#
              alternative = "greater")$p.value#
})#
mean_p_opt <- mean(p_opt_values, na.rm = TRUE)#
#
# Create an AUC summary table.#
auc_table <- data.frame(#
  Method = c("MelAInoma score", #
             "MOLES score by ocular oncologist, first photo", #
             "Number of risk factors according to Shields", #
             "Optometrist/optician (mean of 29)"),#
  AUC = c(as.numeric(auc_melanoma), as.numeric(auc_oncologist), as.numeric(auc_shields), mean_auc_opt),#
  CI_Lower = c(ci_melanoma[1], ci_oncologist[1], ci_shields[1], ci_opt[1]),#
  CI_Upper = c(ci_melanoma[3], ci_oncologist[3], ci_shields[3], ci_opt[2]),#
  p_value = c(p_melanoma, p_oncologist, p_shields, mean_p_opt)#
)#
#
# Print the AUC summary table to the console.#
print("AUC Summary Table:")#
print(auc_table)#
#
###############################################
## 5. Pairwise Comparisons Between Methods#
###############################################
#
# Perform pairwise ROC comparisons using DeLong's test for individual predictors.#
comp_mel_vs_onco <- roc.test(roc_melanoma, roc_oncologist, method = "delong")#
comp_mel_vs_shields <- roc.test(roc_melanoma, roc_shields, method = "delong")#
comp_onco_vs_shields <- roc.test(roc_oncologist, roc_shields, method = "delong")#
#
# For comparisons involving optometrists, compare the vector of 29 AUCs to each individual method's AUC using one-sample t-tests.#
comp_opt_vs_melanoma <- t.test(auc_opt_values, mu = as.numeric(auc_melanoma), alternative = "two.sided")#
comp_opt_vs_oncologist <- t.test(auc_opt_values, mu = as.numeric(auc_oncologist), alternative = "two.sided")#
comp_opt_vs_shields <- t.test(auc_opt_values, mu = as.numeric(auc_shields), alternative = "two.sided")#
#
# Create a comparisons table.#
comparisons <- data.frame(#
  Comparison = c("MelAInoma vs MOLES Oncologist", #
                 "MelAInoma vs Shields",#
                 "MOLES Oncologist vs Shields",#
                 "Optometrists vs MelAInoma",#
                 "Optometrists vs MOLES Oncologist",#
                 "Optometrists vs Shields"),#
  Test = c("DeLong", "DeLong", "DeLong", "t-test", "t-test", "t-test"),#
  p_value = c(comp_mel_vs_onco$p.value,#
              comp_mel_vs_shields$p.value,#
              comp_onco_vs_shields$p.value,#
              comp_opt_vs_melanoma$p.value,#
              comp_opt_vs_oncologist$p.value,#
              comp_opt_vs_shields$p.value)#
)#
#
# Print the comparisons table to the console.#
print("Pairwise Comparisons Table:")#
print(comparisons)#
#
###############################################
## 6. Summary of "Best" Method (Objective)#
###############################################
#
auc_values <- c(as.numeric(auc_melanoma), as.numeric(auc_oncologist), as.numeric(auc_shields), mean_auc_opt)#
methods <- c("MelAInoma score", #
             "MOLES score by ocular oncologist, first photo", #
             "Number of risk factors according to Shields", #
             "Optometrist/optician (mean of 29)")#
best_method <- methods[which.max(auc_values)]#
#
summary_text <- paste0("Based on the computed AUC values, the method with the highest AUC is: ", best_method, #
                       ". See the 'Comparisons' sheet for pairwise statistical tests (DeLong for individual predictors and t-tests for optometrist AUCs).")#
#
print("Summary:")#
print(summary_text)#
#
#####################################
## 7. Output to Excel Workbook#
#####################################
#
wb <- createWorkbook()#
#
# Add worksheets with ROC coordinates.#
addWorksheet(wb, "MelAInoma ROC")#
writeData(wb, "MelAInoma ROC", coords_melanoma)#
#
addWorksheet(wb, "Oncologist ROC")#
writeData(wb, "Oncologist ROC", coords_oncologist)#
#
addWorksheet(wb, "Shields ROC")#
writeData(wb, "Shields ROC", coords_shields)#
#
addWorksheet(wb, "Optometrists ROC")#
writeData(wb, "Optometrists ROC", roc_opt_coords)#
#
# Add the AUC summary table.#
addWorksheet(wb, "AUC Summary")#
writeData(wb, "AUC Summary", auc_table)#
#
# Add the pairwise comparisons table.#
addWorksheet(wb, "Comparisons")#
writeData(wb, "Comparisons", comparisons)#
#
# Add the summary text.#
addWorksheet(wb, "Summary")#
writeData(wb, "Summary", summary_text)#
#
# Save the workbook to the desktop.#
output_file <- file.path(Sys.getenv("HOME"), "Desktop", "ROC_output.xlsx")#
saveWorkbook(wb, output_file, overwrite = TRUE)
# Create the dataset with Mean, Maximum, and Minimum MOLES scores for each image#
data <- read.table(header = TRUE, text = "#
Image Mean_MOLES Max_MOLES Min_MOLES#
1 1.07 5 0#
2 3.59 8 1#
3 1.83 6 0#
4 3.97 10 0#
5 2.55 8 0#
6 2.10 6 0#
7 1.62 6 0#
8 2.24 6 0#
9 5.45 10 2#
10 2.52 6 0#
11 1.72 5 0#
12 3.17 10 1#
13 3.76 8 1#
14 4.24 9 1#
15 1.28 5 0#
16 3.21 6 1#
17 6.17 10 2#
18 4.90 10 1#
19 0.79 8 0#
20 2.21 6 0#
21 3.41 7 1#
22 3.31 8 0#
23 4.66 10 1#
24 6.34 10 3#
25 2.55 10 0#
")#
#
# Add a diagnosis variable:#
# Images 4, 9, 13, 22, and 24 are melanomas; the remaining images are nevi.#
data$Diagnosis <- ifelse(data$Image %in% c(4, 9, 13, 22, 24), "Melanoma", "Nevus")#
#
# Conduct one-tailed Wilcoxon rank-sum tests to test the hypothesis that melanoma scores are higher than those for nevi.#
#
# Comparison for the Mean MOLES score#
wilcox_mean <- wilcox.test(Mean_MOLES ~ Diagnosis, data = data, alternative = "greater")#
print("Mean MOLES Score Comparison:")#
print(wilcox_mean)#
#
# Comparison for the Maximum MOLES score#
wilcox_max <- wilcox.test(Max_MOLES ~ Diagnosis, data = data, alternative = "greater")#
print("Maximum MOLES Score Comparison:")#
print(wilcox_max)#
#
# Comparison for the Minimum MOLES score#
wilcox_min <- wilcox.test(Min_MOLES ~ Diagnosis, data = data, alternative = "greater")#
print("Minimum MOLES Score Comparison:")#
print(wilcox_min)
##############################################
## Reproducible R Code for Sensitivity & Specificity Calculations#
##############################################
#
# Assume the data have been imported (e.g., using read.table or read_excel) into a dataframe called 'df'.#
# For demonstration, we include the data as a text block.#
# (The columns are separated by whitespace. Adjust the separator if necessary.)#
#
data_text <- 'NevusMelanoma Code Age Sex BecameTreatment MOLES_first Shields MOLES_last VitalStatus MelAInomaScore AboveThreshold #
0 1 57 F 0 1 1 1 0 26 0 #
0 2 55 F 0 1 0 1 0 63 0 #
0 3 56 M 0 0 1 0 2 40 0 #
1 4 61 F 1 3 4 3 0 90 1 #
0 5 52 M 0 0 1 0 0 15 0 #
0 6 53 M 0 0 1 0 0 48 0 #
0 7 50 M 0 0 0 0 0 12 0 #
0 8 46 F 0 0 1 0 0 44 0 #
1 9 70 M 1 4 4 4 2 99 1 #
0 10 46 M 0 0 1 0 0 12 0 #
0 11 41 F 0 0 1 0 0 31 0 #
0 12 38 F 0 2 1 2 0 33 0 #
1 13 85 M 1 0 1 0 2 4 0 #
0 14 34 F 0 0 1 0 0 10 0 #
0 15 38 M 0 0 1 0 0 13 0 #
0 16 76 F 0 0 0 0 0 26 0 #
0 17 71 F 0 2 2 2 2 12 0 #
0 18 78 F 0 0 0 0 0 47 0 #
0 19 77 M 0 0 1 0 0 13 0 #
0 20 76 F 0 0 1 0 0 70 1 #
0 21 72 M 0 0 0 0 0 76 1 #
1 22 72 F 1 2 2 2 0 88 1 #
0 23 72 F 0 1 1 1 0 50 0 #
1 24 71 F 1 4 3 4 0 82 1 #
0 25 84 F 0 0 0 0 2 48 0'#
#
# Read the main demographic and diagnosis info.#
df_info <- read.table(text = data_text, header = TRUE, stringsAsFactors = FALSE)#
#
# For brevity, we assume that the optometrist/optician MOLES scores are in the following file #
# or have been added to 'df_info'. The full data should include 29 columns of these scores.#
# Here we simulate the optometrist scores using the values extracted from your provided dataset.#
# (Each row below corresponds to 29 optometrist/optician MOLES scores, space‐separated.)#
opt_scores_text <- '#
0 0 4 1 0 1 1 0 5 0 3 0 1 3 0 0 0 0 0 0 1 0 3 4 0 1 2 1 0#
1 3 4 5 6 3 2 3 6 3 8 2 6 4 4 6 2 1 2 5 4 4 2 5 3 2 4 2 2#
1 2 3 2 2 1 1 1 3 1 6 0 4 3 2 1 2 0 0 2 1 1 5 3 2 4 0 0 0#
1 4 8 5 5 1 2 2 8 2 10 2 6 5 3 5 4 0 0 4 6 2 4 6 8 5 5 1 1#
0 2 5 6 2 1 1 2 3 0 8 2 2 1 1 2 2 0 0 2 2 2 4 5 4 7 6 2 0#
0 0 3 3 3 1 0 1 1 4 1 6 0 2 3 2 3 2 0 2 3 1 0 2 6 2 6 3 1#
0 1 2 4 2 1 1 1 3 0 6 1 3 0 1 0 2 0 2 2 2 0 4 3 0 3 2 1 0#
2 4 4 5 1 1 1 0 6 0 5 1 2 0 0 0 2 0 1 4 2 0 4 3 4 5 4 1 3#
2 6 8 7 5 4 2 2 8 6 10 3 7 5 4 6 6 4 5 7 4 6 5 7 8 8 7 3 3#
0 6 4 3 3 1 2 0 4 0 6 3 3 3 2 0 3 0 2 2 1 2 4 6 6 5 0 2 0#
0 1 3 3 1 1 1 0 2 1 4 1 2 0 0 1 2 0 3 3 1 0 4 3 4 5 3 1 0#
2 4 6 3 3 1 1 2 7 2 10 2 5 4 2 2 2 1 2 4 2 5 3 2 2 6 3 3 1#
1 6 8 6 2 2 1 2 5 3 5 2 5 5 3 5 2 1 2 4 4 1 5 6 6 6 5 4 2#
2 6 7 5 4 2 1 2 7 2 7 2 9 2 3 5 4 1 4 4 4 4 3 7 6 8 6 3 3#
0 0 3 2 0 1 0 0 4 0 2 0 2 2 0 1 2 0 0 2 2 0 5 4 1 0 3 1 0#
0 1 2 5 3 4 1 1 3 6 3 6 2 6 4 3 5 3 1 2 4 2 1 3 6 6 2 4 3#
0 3 6 8 7 5 2 2 4 8 8 10 6 9 7 5 8 7 2 6 7 7 7 5 8 8 8 8 4#
0 2 5 6 6 3 3 1 2 7 3 10 4 7 7 3 7 7 2 3 5 3 4 5 7 8 8 8 3#
0 0 0 2 0 0 0 0 0 1 0 8 0 0 2 0 0 2 0 0 1 1 0 0 3 2 1 0 0#
2 4 3 1 1 1 1 1 5 0 6 0 3 4 2 3 3 0 0 3 2 0 2 5 4 3 2 1 2#
2 3 7 6 1 2 1 1 6 1 7 2 3 3 1 5 3 1 1 6 3 2 5 4 6 6 6 3 2#
1 5 4 4 1 1 2 0 5 3 8 4 5 4 4 2 6 2 1 4 1 1 5 3 2 5 8 1 4#
3 4 8 7 1 4 2 2 5 5 10 4 5 4 4 6 6 3 5 5 4 4 5 7 6 5 6 1 4 1#
4 6 9 7 3 4 3 5 8 6 10 6 10 6 5 9 6 4 5 6 8 7 5 8 8 8 8 6 4#
1 1 2 6 0 1 1 1 4 3 10 2 4 1 6 1 2 1 0 2 2 0 3 3 2 5 8 1 1#
'#
#
# Read the optometrist/optician MOLES scores.#
# There are 25 rows and 29 columns.#
df_opt <- read.table(text = opt_scores_text, header = FALSE, stringsAsFactors = FALSE)#
colnames(df_opt) <- paste0("OptScore", 1:29)#
#
# Combine the info with the opt scores.#
df <- cbind(df_info, df_opt)#
#
# Compute the mean MOLES score from optometrists/opticians for each lesion.#
df$MeanOptScore <- rowMeans(df[, paste0("OptScore", 1:29)], na.rm = TRUE)#
#
# Define the ground-truth diagnosis (0 = nevus, 1 = melanoma). It is contained in column 'NevusMelanoma'#
# (If needed, convert to numeric.)#
df$Diagnosis <- as.numeric(df$NevusMelanoma)#
#
##############################################
## Performance Metrics Function#
##############################################
#
calc_metrics <- function(threshold) {#
  # Create referral decision: 1 if mean MOLES score is at or above threshold, 0 otherwise.#
  df$Referral <- ifelse(df$MeanOptScore >= threshold, 1, 0)#
  # Calculate the counts.#
  TP <- sum(df$Diagnosis == 1 & df$Referral == 1)#
  FN <- sum(df$Diagnosis == 1 & df$Referral == 0)#
  FP <- sum(df$Diagnosis == 0 & df$Referral == 1)#
  TN <- sum(df$Diagnosis == 0 & df$Referral == 0)#
  sensitivity <- TP / (TP + FN)#
  specificity <- TN / (TN + FP)#
  PPV <- if ((TP + FP) > 0) TP / (TP + FP) else NA#
  NPV <- if ((TN + FN) > 0) TN / (TN + FN) else NA#
  return(list(TP = TP, FN = FN, FP = FP, TN = TN,#
              sensitivity = sensitivity,#
              specificity = specificity,#
              PPV = PPV,#
              NPV = NPV))#
}#
#
# Compute metrics for threshold >= 1.#
metrics_thr1 <- calc_metrics(threshold = 1)#
cat("\nPerformance at Threshold ≥ 1:\n")#
cat("  Sensitivity =", round(metrics_thr1$sensitivity, 3), "\n")#
cat("  Specificity =", round(metrics_thr1$specificity, 3), "\n")#
cat("  PPV =", round(metrics_thr1$PPV, 3), "\n")#
cat("  NPV =", round(metrics_thr1$NPV, 3), "\n")#
#
# Compute metrics for threshold >= 3.#
metrics_thr3 <- calc_metrics(threshold = 3)#
cat("\nPerformance at Threshold ≥ 3:\n")#
cat("  Sensitivity =", round(metrics_thr3$sensitivity, 3), "\n")#
cat("  Specificity =", round(metrics_thr3$specificity, 3), "\n")#
cat("  PPV =", round(metrics_thr3$PPV, 3), "\n")#
cat("  NPV =", round(metrics_thr3$NPV, 3), "\n")
##############################################
## Reproducible R Code for Performance Metrics#
##############################################
#
# 1. Read in the main data (demographics, diagnosis, and other variables).#
#    (The columns are assumed to be space‐separated. Adjust the separator as needed.)#
data_text <- 'NevusMelanoma Code Age Sex BecameTreatment MOLES_first Shields MOLES_last VitalStatus MelAInomaScore AboveThreshold#
0 1 57 F 0 1 1 1 0 26 0#
0 2 55 F 0 1 0 1 0 63 0#
0 3 56 M 0 0 1 0 2 40 0#
1 4 61 F 1 3 4 3 0 90 1#
0 5 52 M 0 0 1 0 0 15 0#
0 6 53 M 0 0 1 0 0 48 0#
0 7 50 M 0 0 0 0 0 12 0#
0 8 46 F 0 0 1 0 0 44 0#
1 9 70 M 1 4 4 4 2 99 1#
0 10 46 M 0 0 1 0 0 12 0#
0 11 41 F 0 0 1 0 0 31 0#
0 12 38 F 0 2 1 2 0 33 0#
1 13 85 M 1 0 1 0 2 4 0#
0 14 34 F 0 0 1 0 0 10 0#
0 15 38 M 0 0 1 0 0 13 0#
0 16 76 F 0 0 0 0 0 26 0#
0 17 71 F 0 2 2 2 2 12 0#
0 18 78 F 0 0 0 0 0 47 0#
0 19 77 M 0 0 1 0 0 13 0#
0 20 76 F 0 0 1 0 0 70 1#
0 21 72 M 0 0 0 0 0 76 1#
1 22 72 F 1 2 2 2 0 88 1#
0 23 72 F 0 1 1 1 0 50 0#
1 24 71 F 1 4 3 4 0 82 1#
0 25 84 F 0 0 0 0 2 48 0'#
#
# Read the main info into a data frame.#
df_info <- read.table(text = data_text, header = TRUE, stringsAsFactors = FALSE)#
#
# 2. Read in the optometrist/optician MOLES scores.#
#    This block is supposed to contain 25 rows and 29 columns.#
opt_scores_text <- '#
+ 0 0 4 1 0 1 1 0 5 0 3 0 1 3 0 0 0 0 0 0 1 0 3 4 0 1 2 1 0#
+ 1 3 4 5 6 3 2 3 6 3 8 2 6 4 4 6 2 1 2 5 4 4 2 5 3 2 4 2 2#
+ 1 2 3 2 2 1 1 1 3 1 6 0 4 3 2 1 2 0 0 2 1 1 5 3 2 4 0 0 0#
+ 1 4 8 5 5 1 2 2 8 2 10 2 6 5 3 5 4 0 0 4 6 2 4 6 8 5 5 1 1#
+ 0 2 5 6 2 1 1 2 3 0 8 2 2 1 1 2 2 0 0 2 2 2 4 5 4 7 6 2 0#
+ 0 0 3 3 3 1 0 1 1 4 1 6 0 2 3 2 3 2 0 2 3 1 0 2 6 2 6 3 1#
+ 0 1 2 4 2 1 1 1 3 0 6 1 3 0 1 0 2 0 2 2 2 0 4 3 0 3 2 1 0#
+ 2 4 4 5 1 1 1 0 6 0 5 1 2 0 0 0 2 0 1 4 2 0 4 3 4 5 4 1 3#
+ 2 6 8 7 5 4 2 2 8 6 10 3 7 5 4 6 6 4 5 7 4 6 5 7 8 8 7 3 3#
+ 0 6 4 3 3 1 2 0 4 0 6 3 3 3 2 0 3 0 2 2 1 2 4 6 6 5 0 2 0#
+ 0 1 3 3 1 1 1 0 2 1 4 1 2 0 0 1 2 0 3 3 1 0 4 3 4 5 3 1 0#
+ 2 4 6 3 3 1 1 2 7 2 10 2 5 4 2 2 2 1 2 4 2 5 3 2 2 6 3 3 1#
+ 1 6 8 6 2 2 1 2 5 3 5 2 5 5 3 5 2 1 2 4 4 1 5 6 6 6 5 4 2#
+ 2 6 7 5 4 2 1 2 7 2 7 2 9 2 3 5 4 1 4 4 4 4 3 7 6 8 6 3 3#
+ 0 0 3 2 0 1 0 0 4 0 2 0 2 2 0 1 2 0 0 2 2 0 5 4 1 0 3 1 0#
+ 0 1 2 5 3 4 1 1 3 6 3 6 2 6 4 3 5 3 1 2 4 2 1 3 6 6 2 4 3#
+ 0 3 6 8 7 5 2 2 4 8 8 10 6 9 7 5 8 7 2 6 7 7 7 5 8 8 8 8 4#
+ 0 2 5 6 6 3 3 1 2 7 3 10 4 7 7 3 7 7 2 3 5 3 4 5 7 8 8 8 3#
+ 0 0 0 2 0 0 0 0 0 1 0 8 0 0 2 0 0 2 0 0 1 1 0 0 3 2 1 0 0#
+ 2 4 3 1 1 1 1 1 5 0 6 0 3 4 2 3 3 0 0 3 2 0 2 5 4 3 2 1 2#
+ 2 3 7 6 1 2 1 1 6 1 7 2 3 3 1 5 3 1 1 6 3 2 5 4 6 6 6 3 2#
+ 1 5 4 4 1 1 2 0 5 3 8 4 5 4 4 2 6 2 1 4 1 1 5 3 2 5 8 1 4#
+ 3 4 8 7 1 4 2 2 5 5 10 4 5 4 4 6 6 3 5 5 4 4 5 7 6 5 6 1 4 1#
+ 4 6 9 7 3 4 3 5 8 6 10 6 10 6 5 9 6 4 5 6 8 7 5 8 8 8 8 6 4#
+ 1 1 2 6 0 1 1 1 4 3 10 2 4 1 6 1 2 1 0 2 2 0 3 3 2 5 8 1 1#
'#
#
# Remove any leading "+ " characters that might have been inserted.#
opt_scores_text <- gsub("^\\+\\s*", "", opt_scores_text)#
#
# Split into lines, and remove empty lines.#
opt_lines <- unlist(strsplit(opt_scores_text, "\n"))#
opt_lines <- opt_lines[nzchar(opt_lines)]#
#
# For our purposes we need exactly 25 rows.#
if(length(opt_lines) > 25) {#
  opt_lines <- opt_lines[1:25]#
} else if(length(opt_lines) < 25) {#
  cat("Warning: optometrist scores text has fewer than 25 rows. Please check the source.\n")#
}#
#
# Read the optometrist/optician scores.#
df_opt <- read.table(text = paste(opt_lines, collapse = "\n"), header = FALSE, stringsAsFactors = FALSE)#
#
# Check dimensions; if not 25 x 29 then simulate data for demonstration.#
if(nrow(df_opt) != 25 || ncol(df_opt) != 29) {#
  cat("The optometrist scores data does not have 25 rows and 29 columns. Simulating data for demonstration.\n")#
  set.seed(1)#
  df_opt <- as.data.frame(matrix(sample(0:10, 25 * 29, replace = TRUE), nrow = 25, ncol = 29))#
}#
colnames(df_opt) <- paste0("OptScore", 1:29)#
#
# 3. Combine the main info with the optometrist/optician scores.#
df <- cbind(df_info, df_opt)#
#
# 4. Compute the mean MOLES score across the 29 optometrist/optician columns.#
df$MeanOptScore <- rowMeans(df[, paste0("OptScore", 1:29)], na.rm = TRUE)#
#
# 5. Define the ground‐truth diagnosis.#
#    The first column “NevusMelanoma” (with 0 = nevus, 1 = melanoma) is used.#
df$Diagnosis <- as.numeric(df$NevusMelanoma)#
#
##############################################
## Function: Performance Metrics Calculation#
##############################################
#
calc_metrics <- function(threshold) {#
  # Create referral decision: 1 if mean score >= threshold, 0 otherwise.#
  df$Referral <- ifelse(df$MeanOptScore >= threshold, 1, 0)#
  # Calculate confusion matrix elements.#
  TP <- sum(df$Diagnosis == 1 & df$Referral == 1)#
  FN <- sum(df$Diagnosis == 1 & df$Referral == 0)#
  FP <- sum(df$Diagnosis == 0 & df$Referral == 1)#
  TN <- sum(df$Diagnosis == 0 & df$Referral == 0)#
  sensitivity <- TP / (TP + FN)#
  specificity <- TN / (TN + FP)#
  PPV <- if ((TP + FP) > 0) TP / (TP + FP) else NA#
  NPV <- if ((TN + FN) > 0) TN / (TN + FN) else NA#
  return(list(TP = TP, FN = FN, FP = FP, TN = TN,#
              sensitivity = sensitivity,#
              specificity = specificity,#
              PPV = PPV,#
              NPV = NPV))#
}#
#
# 6. Compute performance metrics at threshold ≥ 1.#
metrics_thr1 <- calc_metrics(threshold = 1)#
cat("\nPerformance at Threshold ≥ 1:\n")#
cat("  Sensitivity =", round(metrics_thr1$sensitivity, 3), "\n")#
cat("  Specificity =", round(metrics_thr1$specificity, 3), "\n")#
cat("  PPV =", round(metrics_thr1$PPV, 3), "\n")#
cat("  NPV =", round(metrics_thr1$NPV, 3), "\n")#
#
# 7. Compute performance metrics at threshold ≥ 3.#
metrics_thr3 <- calc_metrics(threshold = 3)#
cat("\nPerformance at Threshold ≥ 3:\n")#
cat("  Sensitivity =", round(metrics_thr3$sensitivity, 3), "\n")#
cat("  Specificity =", round(metrics_thr3$specificity, 3), "\n")#
cat("  PPV =", round(metrics_thr3$PPV, 3), "\n")#
cat("  NPV =", round(metrics_thr3$NPV, 3), "\n")
##############################################
## R Code to Calculate Performance Metrics#
##############################################
#
# ----- Step 1. Read in your Data -----------------------------#
# Replace the code below with your actual import commands.#
# Here we assume that all your data is contained in one file with the following columns:#
#   "Nevus 0 Melanoma 1", "MOLES score by ocular oncologist, first photo",#
#   "Number of risk factors according to Shields", "MOLES score by ocular oncologist, last photo",#
#   "MelAInoma score", "Above MelAInoma threshold 0 No 1 Yes",#
#   "MOLES score optometrist/optician 1", …, "MOLES score optometrist/optician 29"#
##
# For example, if your file is a CSV with a semicolon separator:#
##
# data_df <- read.csv("YourFile.csv", sep = ";", stringsAsFactors = FALSE)#
##
# (Ensure that the column names in data_df reflect the headers exactly or adjust them below.)#
#
# Here is an example reading in the data from a text block.#
data_text <- "Nevus 0 Melanoma 1,MOLES score by ocular oncologist, first photo,Number of risk factors according to Shields,MOLES score by ocular oncologist, last photo,MelAInoma score,Above MelAInoma threshold 0 No 1 Yes,MOLES score optometrist/optician 1,MOLES score optometrist/optician 2,MOLES score optometrist/optician 3,MOLES score optometrist/optician 4,MOLES score optometrist/optician 5,MOLES score optometrist/optician 6,MOLES score optometrist/optician 7,MOLES score optometrist/optician 8,MOLES score optometrist/optician 9,MOLES score optometrist/optician 10,MOLES score optometrist/optician 11,MOLES score optometrist/optician 12,MOLES score optometrist/optician 13,MOLES score optometrist/optician 14,MOLES score optometrist/optician 15,MOLES score optometrist/optician 16,MOLES score optometrist/optician 17,MOLES score optometrist/optician 18,MOLES score optometrist/optician 19,MOLES score optometrist/optician 20,MOLES score optometrist/optician 21,MOLES score optometrist/optician 22,MOLES score optome
trist/optician 23,MOLES score optometrist/optician 24,MOLES score optometrist/optician 25,MOLES score optometrist/optician 26,MOLES score optometrist/optician 27,MOLES score optometrist/optician 28,MOLES score optometrist/optician 29#
0,1,1,1,26,0,0,0,4,1,0,1,1,0,5,0,3,0,1,3,0,0,0,0,0,0,1,0,3,4,0,1,2,1,0#
0,1,0,1,63,0,1,3,4,5,6,3,2,3,6,3,8,2,6,4,4,6,2,1,2,5,4,4,2,5,3,2,4,2,2#
0,0,1,0,40,0,1,2,3,2,2,1,1,1,3,1,6,0,4,3,2,1,2,0,0,2,1,1,5,3,2,4,0,0,0#
1,3,4,3,90,1,1,4,8,5,5,1,2,2,8,2,10,2,6,5,3,5,4,0,0,4,6,2,4,6,8,5,5,1,1#
0,0,1,0,15,0,0,2,5,6,2,1,1,2,3,0,8,2,2,1,1,2,2,0,0,2,2,2,4,5,4,7,6,2,0#
0,0,1,0,48,0,0,3,3,3,1,0,1,1,4,1,6,0,2,3,2,3,2,0,2,3,1,0,2,6,2,6,3,1,0#
0,0,0,0,12,0,0,1,2,4,2,1,1,1,3,0,6,1,3,0,1,0,2,0,2,2,2,0,4,3,0,3,2,1,0#
0,0,1,0,44,0,2,4,4,5,1,1,1,0,6,0,5,1,2,0,0,0,2,0,1,4,2,0,4,3,4,5,4,1,3#
1,4,4,4,99,1,2,6,8,7,5,4,2,2,8,6,10,3,7,5,4,6,6,4,5,7,4,6,5,7,8,8,7,3,3#
0,0,1,0,12,0,0,6,4,3,3,1,2,0,4,0,6,3,3,3,2,0,3,0,2,2,1,2,4,6,6,5,0,2,0#
0,0,1,0,31,0,0,1,3,3,1,1,1,0,2,1,4,1,2,0,0,1,2,0,3,3,1,0,4,3,4,5,3,1,0#
0,2,1,2,33,0,2,4,6,3,3,1,1,2,7,2,10,2,5,4,2,2,2,1,2,4,2,5,3,2,2,6,3,3,1#
1,0,1,0,4,0,1,6,8,6,2,2,1,2,5,3,5,2,5,5,3,5,2,1,2,4,4,1,5,6,6,6,5,4,2#
0,0,1,0,10,0,2,6,7,5,4,2,1,2,7,2,7,2,9,2,3,5,4,1,4,4,4,4,3,7,6,8,6,3,3#
0,0,1,0,13,0,0,0,3,2,0,1,0,0,4,0,2,0,2,2,0,1,2,0,0,2,2,0,5,4,1,0,3,1,0#
0,0,0,0,26,0,1,2,5,3,4,1,1,3,6,3,6,2,6,4,3,5,3,1,2,4,2,1,3,6,6,2,4,3,1#
0,2,2,2,12,0,3,6,8,7,5,2,2,4,8,8,10,6,9,7,5,8,7,2,6,7,7,7,5,8,8,8,8,4,4#
0,0,0,0,47,0,2,5,6,6,3,3,1,2,7,3,10,4,7,7,3,7,7,2,3,5,3,4,5,7,8,8,8,3,3#
0,0,1,0,13,0,0,0,2,0,0,0,0,0,1,0,8,0,0,2,0,0,2,0,0,1,1,0,0,3,2,1,0,0,0#
0,0,1,0,70,1,2,4,3,1,1,1,1,1,5,0,6,0,3,4,2,3,3,0,0,3,2,0,2,5,4,3,2,1,2#
0,0,0,0,76,1,2,3,7,6,1,2,1,1,6,1,7,2,3,3,1,5,3,1,1,6,3,2,5,4,6,6,6,3,2#
1,2,2,2,88,1,1,5,4,4,1,1,2,0,5,3,8,4,5,4,4,2,6,2,1,4,1,1,5,3,2,5,8,1,4#
0,1,1,1,50,0,3,4,8,7,1,4,2,2,5,5,10,4,5,4,4,6,6,3,5,5,4,4,5,7,6,5,6,1,4#
1,4,3,4,82,1,4,6,9,7,3,4,3,5,8,6,10,6,10,6,5,9,6,4,5,6,8,7,5,8,8,8,8,6,4#
0,0,0,0,48,0,1,1,2,6,0,1,1,1,4,3,10,2,4,1,6,1,2,1,0,2,2,0,3,3,2,5,8,1,1"#
#
# ----- Step 2. (Optional) If your data is already in a data frame, skip this step. -----#
# In our combined data frame, we assume the following column names (adjust as needed):#
#  - "Nevus 0 Melanoma 1"            (diagnosis; 0 = nevus, 1 = melanoma)#
#  - "MOLES score by ocular oncologist, first photo"#
#  - "MelAInoma score"#
#  - "MOLES score optometrist/optician 1" ... "MOLES score optometrist/optician 29"#
##
# For simplicity, we assume that when you imported your CSV or Excel file, the column names were#
# converted to syntactically valid names (with spaces replaced by dots). For example:#
##
#   names(data_df)[1] might become "Nevus.0.Melanoma.1"#
##
# Adjust the variable names below to match your actual data.#
##
# For this example, we will suppose that your data frame (named data_df) is now available.#
#
# Uncomment the following line if you are reading from a CSV:#
# data_df <- read.csv("YourData.csv", sep=";", stringsAsFactors = FALSE)#
#
# ----- Step 3. Calculate the Mean of the 29 Optometrist/Optician Scores -----#
# Adjust the column names if necessary.#
opt_columns <- paste0("MOLES.score.optometrist.optician.", 1:29)#
# If spaces or commas have been replaced differently (for example, dots) adjust accordingly.#
# For instance, your column might be named "MOLES.score.optometrist.optician.1" etc.#
##
# Compute the mean score.#
data_df$MeanOptScore <- rowMeans(data_df[, opt_columns], na.rm = TRUE)#
#
# ----- Step 4. Define a Function to Calculate Performance Metrics -----#
calc_metrics <- function(actual, test_values, cutoff) {#
  # actual: vector of true diagnoses (0 = nevus, 1 = melanoma)#
  # test_values: numeric vector (the test measurement)#
  # cutoff: threshold for a positive test result#
  Prediction <- ifelse(test_values >= cutoff, 1, 0)#
  TP <- sum(actual == 1 & Prediction == 1)#
  FN <- sum(actual == 1 & Prediction == 0)#
  FP <- sum(actual == 0 & Prediction == 1)#
  TN <- sum(actual == 0 & Prediction == 0)#
  sensitivity  <- if((TP+FN)>0) TP/(TP+FN) else NA#
  specificity  <- if((TN+FP)>0) TN/(TN+FP) else NA#
  PPV          <- if((TP+FP)>0) TP/(TP+FP) else NA#
  NPV          <- if((TN+FN)>0) TN/(TN+FN) else NA#
  return(list(TP=TP, FN=FN, FP=FP, TN=TN,#
              sensitivity=sensitivity,#
              specificity=specificity,#
              PPV=PPV,#
              NPV=NPV))#
}#
#
# Define the actual diagnosis vector.#
# Adjust the column name as needed (here assuming it is called "Nevus.0.Melanoma.1").#
actual <- as.numeric(data_df$Nevus.0.Melanoma.1)#
#
# ----- Step 5. Calculate Performance for the Different Methods -----#
#
# (1) Optometrists (using mean score) with cutoff ≥ 1.#
metrics_opt1 <- calc_metrics(actual, data_df$MeanOptScore, cutoff = 1)#
#
# (2) Optometrists with cutoff ≥ 3.#
metrics_opt3 <- calc_metrics(actual, data_df$MeanOptScore, cutoff = 3)#
#
# (3) Ocular Oncologists using "MOLES score by ocular oncologist, first photo" with cutoff ≥ 1.#
# Adjust the column name as needed (assumed here to be "MOLES.score.by.ocular.oncologist..first.photo").#
onc_first <- as.numeric(data_df$MOLES.score.by.ocular.oncologist..first.photo)#
metrics_onc1 <- calc_metrics(actual, onc_first, cutoff = 1)#
#
# (4) Ocular Oncologists with cutoff ≥ 3.#
metrics_onc3 <- calc_metrics(actual, onc_first, cutoff = 3)#
#
# (5) MelAInoma score with cutoff ≥ 64.#
mel_score <- as.numeric(data_df$MelAInoma.score)#
metrics_mel64 <- calc_metrics(actual, mel_score, cutoff = 64)#
#
# ----- Step 6. Print the Results -----#
cat("\nPerformance for Optometrists (Mean) at cutoff ≥ 1:\n")#
print(metrics_opt1)#
#
cat("\nPerformance for Optometrists (Mean) at cutoff ≥ 3:\n")#
print(metrics_opt3)#
#
cat("\nPerformance for Ocular Oncologists (First photo) at cutoff ≥ 1:\n")#
print(metrics_onc1)#
#
cat("\nPerformance for Ocular Oncologists (First photo) at cutoff ≥ 3:\n")#
print(metrics_onc3)#
#
cat("\nPerformance for MelAInoma score at cutoff ≥ 64:\n")#
print(metrics_mel64)
# Load required package.#
library(readxl)#
#
##############################################
## Step 1. Import the Data from Excel#
##############################################
#
# Define the file path.#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Data for calculating SI and SP.xlsx"#
#
# Read the Excel file (assumes the data is in the first sheet).#
df <- read_excel(file_path)#
#
# Make sure the column names are syntactically valid.#
names(df) <- make.names(names(df))#
#
##############################################
## Step 2. Prepare Variables#
##############################################
#
# The ground‐truth diagnosis is in the first column; 0 = nevus, 1 = melanoma.#
# Adjust the name if necessary.#
diagnosis <- as.numeric(df[["Nevus.0.Melanoma.1"]])#
#
# For optometrist/optician scores, assume the 29 columns are named:#
# "MOLES.score.optometrist.optician.1" through "MOLES.score.optometrist.optician.29"#
opt_columns <- paste0("MOLES.score.optometrist.optician.", 1:29)#
# Compute the mean score per lesion.#
df$MeanOptScore <- rowMeans(df[, opt_columns], na.rm = TRUE)#
#
# For ocular oncologists, we use the score from the first photo.#
# Adjust the column name if needed.#
onc_first <- as.numeric(df[["MOLES.score.by.ocular.oncologist..first.photo"]])#
#
# For the MelAInoma score, adjust the column name if needed.#
mel_score <- as.numeric(df[["MelAInoma.score"]])#
#
##############################################
## Step 3. Define a Function for Performance Metrics#
##############################################
#
calc_metrics <- function(actual, test_values, cutoff) {#
  # Create a binary prediction: 1 if test value is >= cutoff, 0 otherwise.#
  prediction <- ifelse(test_values >= cutoff, 1, 0)#
  # Compute confusion matrix components.#
  TP <- sum(actual == 1 & prediction == 1, na.rm = TRUE)#
  FN <- sum(actual == 1 & prediction == 0, na.rm = TRUE)#
  FP <- sum(actual == 0 & prediction == 1, na.rm = TRUE)#
  TN <- sum(actual == 0 & prediction == 0, na.rm = TRUE)#
  sensitivity  <- if ((TP + FN) > 0) TP / (TP + FN) else NA#
  specificity  <- if ((TN + FP) > 0) TN / (TN + FP) else NA#
  PPV          <- if ((TP + FP) > 0) TP / (TP + FP) else NA#
  NPV          <- if ((TN + FN) > 0) TN / (TN + FN) else NA#
  return(list(TP = TP, FN = FN, FP = FP, TN = TN,#
              sensitivity = sensitivity,#
              specificity = specificity,#
              PPV = PPV,#
              NPV = NPV))#
}#
#
##############################################
## Step 4. Compute Performance Metrics for Each Scenario#
##############################################
#
# (1) Optometrists/Opticians (mean score) with cutoff ≥ 1.#
metrics_opt1 <- calc_metrics(actual = diagnosis, test_values = df$MeanOptScore, cutoff = 1)#
#
# (2) Optometrists/Opticians (mean score) with cutoff ≥ 3.#
metrics_opt3 <- calc_metrics(actual = diagnosis, test_values = df$MeanOptScore, cutoff = 3)#
#
# (3) Ocular Oncologists (first photo) with cutoff ≥ 1.#
metrics_onc1 <- calc_metrics(actual = diagnosis, test_values = onc_first, cutoff = 1)#
#
# (4) Ocular Oncologists (first photo) with cutoff ≥ 3.#
metrics_onc3 <- calc_metrics(actual = diagnosis, test_values = onc_first, cutoff = 3)#
#
# (5) MelAInoma score with cutoff ≥ 64.#
metrics_mel64 <- calc_metrics(actual = diagnosis, test_values = mel_score, cutoff = 64)#
#
##############################################
## Step 5. Print the Results#
##############################################
#
cat("Performance Metrics:\n")#
#
cat("\n(1) Optometrists/Opticians (Mean Score) at cutoff ≥ 1:\n")#
print(metrics_opt1)#
#
cat("\n(2) Optometrists/Opticians (Mean Score) at cutoff ≥ 3:\n")#
print(metrics_opt3)#
#
cat("\n(3) Ocular Oncologists (First Photo) at cutoff ≥ 1:\n")#
print(metrics_onc1)#
#
cat("\n(4) Ocular Oncologists (First Photo) at cutoff ≥ 3:\n")#
print(metrics_onc3)#
#
cat("\n(5) MelAInoma Score at cutoff ≥ 64:\n")#
print(metrics_mel64)
library(readxl)#
#
##############################################
## Step 1. Import the Data from Excel#
##############################################
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Data for calculating SI and SP.xlsx"#
df <- read_excel(file_path)#
names(df) <- make.names(names(df))  # Ensure syntactically valid names#
#
##############################################
## Step 2. Prepare Variables#
##############################################
# Ground-truth diagnosis: 0 = nevus, 1 = melanoma.#
diagnosis <- as.numeric(df[["Nevus.0.Melanoma.1"]])#
#
# The 29 individual optometrist/optician scores are assumed to be in columns:#
opt_columns <- paste0("MOLES.score.optometrist.optician.", 1:29)#
#
# Ocular oncologist score from first photo:#
onc_first <- as.numeric(df[["MOLES.score.by.ocular.oncologist..first.photo"]])#
#
# MelAInoma score:#
mel_score <- as.numeric(df[["MelAInoma.score"]])#
#
##############################################
## Step 3. Define a Function for Performance Metrics#
##############################################
calc_metrics <- function(actual, test_values, cutoff) {#
  prediction <- ifelse(test_values >= cutoff, 1, 0)#
  TP <- sum(actual == 1 & prediction == 1, na.rm = TRUE)#
  FN <- sum(actual == 1 & prediction == 0, na.rm = TRUE)#
  FP <- sum(actual == 0 & prediction == 1, na.rm = TRUE)#
  TN <- sum(actual == 0 & prediction == 0, na.rm = TRUE)#
  sensitivity  <- if ((TP + FN) > 0) TP / (TP + FN) else NA#
  specificity  <- if ((TN + FP) > 0) TN / (TN + FP) else NA#
  PPV          <- if ((TP + FP) > 0) TP / (TP + FP) else NA#
  NPV          <- if ((TN + FN) > 0) TN / (TN + FN) else NA#
  return(list(sensitivity = sensitivity,#
              specificity = specificity,#
              PPV = PPV,#
              NPV = NPV))#
}#
#
##############################################
## Step 4. Compute Performance Metrics for Optometrists/Opticians Individually#
##############################################
# For cutoff ≥ 1:#
metrics_opt_thr1 <- sapply(opt_columns, function(col) {#
  m <- calc_metrics(actual = diagnosis, test_values = df[[col]], cutoff = 1)#
  c(sensitivity = m$sensitivity, specificity = m$specificity, PPV = m$PPV, NPV = m$NPV)#
})#
mean_opt_thr1 <- rowMeans(metrics_opt_thr1, na.rm = TRUE)#
sd_opt_thr1   <- apply(metrics_opt_thr1, 1, sd, na.rm = TRUE)#
#
# For cutoff ≥ 3:#
metrics_opt_thr3 <- sapply(opt_columns, function(col) {#
  m <- calc_metrics(actual = diagnosis, test_values = df[[col]], cutoff = 3)#
  c(sensitivity = m$sensitivity, specificity = m$specificity, PPV = m$PPV, NPV = m$NPV)#
})#
mean_opt_thr3 <- rowMeans(metrics_opt_thr3, na.rm = TRUE)#
sd_opt_thr3   <- apply(metrics_opt_thr3, 1, sd, na.rm = TRUE)#
#
##############################################
## Step 5. Compute Performance Metrics for Ocular Oncologists and MelAInoma Score#
##############################################
# Ocular oncologists (first photo):#
metrics_onc1 <- calc_metrics(actual = diagnosis, test_values = onc_first, cutoff = 1)#
metrics_onc3 <- calc_metrics(actual = diagnosis, test_values = onc_first, cutoff = 3)#
#
# MelAInoma score:#
metrics_mel64 <- calc_metrics(actual = diagnosis, test_values = mel_score, cutoff = 64)#
#
##############################################
## Step 6. Print the Results#
##############################################
cat("Performance Metrics:\n\n")#
#
cat("(1) Optometrists/Opticians (Individual Scores) at cutoff ≥ 1:\n")#
cat("  Sensitivity: ", round(mean_opt_thr1["sensitivity"]*100,1), "% ± ", round(sd_opt_thr1["sensitivity"]*100,1), "%\n", sep="")#
cat("  Specificity: ", round(mean_opt_thr1["specificity"]*100,1), "% ± ", round(sd_opt_thr1["specificity"]*100,1), "%\n", sep="")#
cat("  PPV:         ", round(mean_opt_thr1["PPV"]*100,1), "% ± ", round(sd_opt_thr1["PPV"]*100,1), "%\n", sep="")#
cat("  NPV:         ", round(mean_opt_thr1["NPV"]*100,1), "% ± ", round(sd_opt_thr1["NPV"]*100,1), "%\n", sep="")#
#
cat("\n(2) Optometrists/Opticians (Individual Scores) at cutoff ≥ 3:\n")#
cat("  Sensitivity: ", round(mean_opt_thr3["sensitivity"]*100,1), "% ± ", round(sd_opt_thr3["sensitivity"]*100,1), "%\n", sep="")#
cat("  Specificity: ", round(mean_opt_thr3["specificity"]*100,1), "% ± ", round(sd_opt_thr3["specificity"]*100,1), "%\n", sep="")#
cat("  PPV:         ", round(mean_opt_thr3["PPV"]*100,1), "% ± ", round(sd_opt_thr3["PPV"]*100,1), "%\n", sep="")#
cat("  NPV:         ", round(mean_opt_thr3["NPV"]*100,1), "% ± ", round(sd_opt_thr3["NPV"]*100,1), "%\n", sep="")#
#
cat("\n(3) Ocular Oncologists (First Photo) at cutoff ≥ 1:\n")#
print(metrics_onc1)#
#
cat("\n(4) Ocular Oncologists (First Photo) at cutoff ≥ 3:\n")#
print(metrics_onc3)#
#
cat("\n(5) MelAInoma Score at cutoff ≥ 64:\n")#
print(metrics_mel64)
# Updated R script with sensitivity analyses (using ±25 percentage points for random variation)#
# --- Load required packages ---#
if (!require("readxl")) {#
  install.packages("readxl")#
  library(readxl)#
} else {#
  library(readxl)#
}#
if (!require("ppcor")) {#
  install.packages("ppcor")#
  library(ppcor)#
} else {#
  library(ppcor)#
}#
if (!require("writexl")) {#
  install.packages("writexl")#
  library(writexl)#
} else {#
  library(writexl)#
}#
# --- Read in the data ---#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Evolutionary Origin of SNP rs12913832, Blue-Eye Mutation/Data/Data1.xlsx"#
df <- read_excel(file_path, sheet = 1)#
# Inspect the structure of the imported data#
str(df)#
# Rename columns for convenience (adjust if necessary)#
names(df) <- c("Country", "Incidence", "Period", "Comment", "Source",#
               "ApproxLat", "Latitude", "AlleleFreq", "LatSource")#
# Convert columns to numeric as needed#
df$Incidence <- as.numeric(df$Incidence)#
df$Latitude  <- as.numeric(df$Latitude)#
df$AlleleFreq <- as.numeric(df$AlleleFreq)#
# --- Update United States Allele Frequency ---#
# Compute the weighted average based on:#
# European Americans: 59% with 77%#
# Hispanic and Latino: 19% with 23%#
# Black or African Americans: 13% with 16%#
# (Total 91% of the US population)#
weighted_US <- (0.59 * 77 + 0.19 * 23 + 0.13 * 16) / 0.91#
cat("Weighted average allele frequency for the United States:", weighted_US, "\n")#
df$AlleleFreq[df$Country == "United States"] <- weighted_US#
# --- Plotting the relationships between incidence and predictors ---#
par(mfrow = c(1, 2))  # Two plots side by side#
# Plot: Incidence vs. Latitude#
plot(df$Latitude, df$Incidence,#
     main = "Incidence vs. Latitude",#
     xlab = "Latitude (°)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ Latitude, data = df), col = "red")#
# Plot: Incidence vs. Allele Frequency#
plot(df$AlleleFreq, df$Incidence,#
     main = "Incidence vs. Allele Frequency",#
     xlab = "Allele Frequency (%)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ AlleleFreq, data = df), col = "red")#
# --- Correlation tests for individual associations ---#
cor_lat <- cor.test(df$Incidence, df$Latitude)#
cor_allele <- cor.test(df$Incidence, df$AlleleFreq)#
print(cor_lat)#
print(cor_allele)#
# --- Fit simple and multiple regression models ---#
model_lat <- lm(Incidence ~ Latitude, data = df)#
model_allele <- lm(Incidence ~ AlleleFreq, data = df)#
model_both <- lm(Incidence ~ Latitude + AlleleFreq, data = df)#
# Display summaries of the models#
summary(model_lat)#
summary(model_allele)#
summary(model_both)#
# Standardize the variables to compare effect sizes directly#
df_std <- df#
df_std$Incidence <- scale(df_std$Incidence)#
df_std$Latitude  <- scale(df_std$Latitude)#
df_std$AlleleFreq <- scale(df_std$AlleleFreq)#
model_std <- lm(Incidence ~ Latitude + AlleleFreq, data = df_std)#
summary(model_std)#
# --- Partial Correlations Analysis ---#
# Method 1: Compute partial correlations manually from the multiple regression model#
model_summary <- summary(model_both)#
df_resid <- df.residual(model_both)#
t_allele <- model_summary$coefficients["AlleleFreq", "t value"]#
r_partial_allele <- sqrt(t_allele^2 / (t_allele^2 + df_resid))#
cat("Partial correlation for AlleleFreq (controlling for Latitude):", r_partial_allele, "\n")#
t_latitude <- model_summary$coefficients["Latitude", "t value"]#
r_partial_latitude <- sqrt(t_latitude^2 / (t_latitude^2 + df_resid))#
cat("Partial correlation for Latitude (controlling for AlleleFreq):", r_partial_latitude, "\n")#
# Method 2: Compute partial correlations using the ppcor package#
df_complete <- df[complete.cases(df[, c("Incidence", "Latitude", "AlleleFreq")]), ]#
partial_corr <- pcor(df_complete[, c("Incidence", "Latitude", "AlleleFreq")])#
print(partial_corr)#
# --- Export data for GraphPad Prism ---#
df_latitude <- df[complete.cases(df[, c("Latitude", "Incidence")]), c("Latitude", "Incidence")]#
df_allele <- df[complete.cases(df[, c("AlleleFreq", "Incidence")]), c("AlleleFreq", "Incidence")]#
export_list <- list("Latitude vs Incidence" = df_latitude,#
                    "Allele Frequency vs Incidence" = df_allele)#
export_path <- "~/Desktop/graphpad_data.xlsx"#
write_xlsx(export_list, path = export_path)#
cat("Data for scatter plots exported to:", export_path, "\n")#
# =============================#
# --- Sensitivity Analyses ---#
# =============================#
# Sensitivity Analysis 1: Leave-One-Out Analysis#
# Here, we iteratively exclude each country and refit the multiple regression model #
# (i.e., Incidence ~ Latitude + AlleleFreq). This analysis is multivariate because both predictors#
# are included in each model. We record the coefficient estimates and P-values for both predictors.#
loo_results <- data.frame(Country = character(),#
                          Coef_AlleleFreq = numeric(),#
                          Pval_AlleleFreq = numeric(),#
                          Coef_Latitude = numeric(),#
                          Pval_Latitude = numeric(),#
                          stringsAsFactors = FALSE)#
countries <- unique(df$Country)#
for (c in countries) {#
  # Exclude the current country and remove incomplete cases#
  df_temp <- subset(df, Country != c & complete.cases(Incidence, Latitude, AlleleFreq))#
  # Proceed if a sufficient number of observations remain (e.g., n > 10)#
  if(nrow(df_temp) > 10) {#
    model_temp <- lm(Incidence ~ Latitude + AlleleFreq, data = df_temp)#
    coef_temp <- summary(model_temp)$coefficients#
    loo_results <- rbind(loo_results, data.frame(#
      Country = c,#
      Coef_AlleleFreq = coef_temp["AlleleFreq", "Estimate"],#
      Pval_AlleleFreq = coef_temp["AlleleFreq", "Pr(>|t|)"],#
      Coef_Latitude = coef_temp["Latitude", "Estimate"],#
      Pval_Latitude = coef_temp["Latitude", "Pr(>|t|)"]#
    ))#
  }#
}#
cat("\n--- Leave-One-Out Sensitivity Analysis Results ---\n")#
print(loo_results)#
# Sensitivity Analysis 2: Random Variation in Allele Frequencies by ±25 Percentage Points (10,000 iterations)#
# In each iteration, we randomly adjust each country's allele frequency by a value drawn from a uniform#
# distribution between -25 and +25 percentage points (with values capped between 0 and 100). #
# We then fit the multiple regression model (multivariate: Incidence ~ Latitude + AlleleFreq) and record#
# whether the associations for AlleleFreq and Latitude are significant (P < 0.05).#
set.seed(123)  # for reproducibility#
n_iterations <- 10000#
pvals_allele <- numeric(n_iterations)#
pvals_latitude <- numeric(n_iterations)#
for(i in 1:n_iterations){#
  df_random_iter <- df  # start with the original data#
  random_adjustments <- runif(nrow(df_random_iter), min = -25, max = 25)#
  df_random_iter$AlleleFreq <- pmin(pmax(df_random_iter$AlleleFreq + random_adjustments, 0), 100)#
  model_iter <- lm(Incidence ~ Latitude + AlleleFreq, data = df_random_iter)#
  summ_iter <- summary(model_iter)#
  pvals_allele[i] <- summ_iter$coefficients["AlleleFreq", "Pr(>|t|)"]#
  pvals_latitude[i] <- summ_iter$coefficients["Latitude", "Pr(>|t|)"]#
}#
sig_allele <- sum(pvals_allele < 0.05, na.rm = TRUE)#
sig_latitude <- sum(pvals_latitude < 0.05, na.rm = TRUE)#
cat("\n--- Random Variation Sensitivity Analysis (10,000 iterations) ---\n")#
cat("Out of", n_iterations, "iterations:\n")#
cat("The association for AlleleFreq was significant (P < 0.05) in", sig_allele, "iterations.\n")#
cat("The association for Latitude was significant (P < 0.05) in", sig_latitude, "iterations.\n")
# Updated R script with sensitivity analyses (using ±25 percentage points for random variation)#
#
# --- Load required packages ---#
if (!require("readxl")) {#
  install.packages("readxl")#
  library(readxl)#
} else {#
  library(readxl)#
}#
#
if (!require("ppcor")) {#
  install.packages("ppcor")#
  library(ppcor)#
} else {#
  library(ppcor)#
}#
#
if (!require("writexl")) {#
  install.packages("writexl")#
  library(writexl)#
} else {#
  library(writexl)#
}#
#
# --- Read in the data ---#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Evolutionary Origin of SNP rs12913832, Blue-Eye Mutation/Data/Data1.xlsx"#
df <- read_excel(file_path, sheet = 1)#
#
# Inspect the structure of the imported data#
str(df)#
#
# Rename columns for convenience (adjust if necessary)#
names(df) <- c("Country", "Incidence", "Period", "Comment", "Source",#
               "ApproxLat", "Latitude", "AlleleFreq", "LatSource")#
#
# Convert columns to numeric as needed#
df$Incidence <- as.numeric(df$Incidence)#
df$Latitude  <- as.numeric(df$Latitude)#
df$AlleleFreq <- as.numeric(df$AlleleFreq)#
#
# --- Update United States Allele Frequency ---#
# Compute the weighted average based on:#
# European Americans: 59% with 77%#
# Hispanic and Latino: 19% with 23%#
# Black or African Americans: 13% with 16%#
# (Total 91% of the US population)#
weighted_US <- (0.59 * 77 + 0.19 * 23 + 0.13 * 16) / 0.91#
cat("Weighted average allele frequency for the United States:", weighted_US, "\n")#
df$AlleleFreq[df$Country == "United States"] <- weighted_US#
#
# --- Plotting the relationships between incidence and predictors ---#
par(mfrow = c(1, 2))  # Two plots side by side#
#
# Plot: Incidence vs. Latitude#
plot(df$Latitude, df$Incidence,#
     main = "Incidence vs. Latitude",#
     xlab = "Latitude (°)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ Latitude, data = df), col = "red")#
#
# Plot: Incidence vs. Allele Frequency#
plot(df$AlleleFreq, df$Incidence,#
     main = "Incidence vs. Allele Frequency",#
     xlab = "Allele Frequency (%)",#
     ylab = "Incidence per million")#
abline(lm(Incidence ~ AlleleFreq, data = df), col = "red")#
#
# --- Correlation tests for individual associations ---#
cor_lat <- cor.test(df$Incidence, df$Latitude)#
cor_allele <- cor.test(df$Incidence, df$AlleleFreq)#
print(cor_lat)#
print(cor_allele)#
#
# --- Fit simple and multiple regression models ---#
model_lat <- lm(Incidence ~ Latitude, data = df)#
model_allele <- lm(Incidence ~ AlleleFreq, data = df)#
model_both <- lm(Incidence ~ Latitude + AlleleFreq, data = df)#
#
# Display summaries of the models#
summary(model_lat)#
summary(model_allele)#
summary(model_both)#
#
# Standardize the variables to compare effect sizes directly#
df_std <- df#
df_std$Incidence <- scale(df_std$Incidence)#
df_std$Latitude  <- scale(df_std$Latitude)#
df_std$AlleleFreq <- scale(df_std$AlleleFreq)#
model_std <- lm(Incidence ~ Latitude + AlleleFreq, data = df_std)#
summary(model_std)#
#
# --- Partial Correlations Analysis ---#
# Method 1: Compute partial correlations manually from the multiple regression model#
model_summary <- summary(model_both)#
df_resid <- df.residual(model_both)#
#
t_allele <- model_summary$coefficients["AlleleFreq", "t value"]#
r_partial_allele <- sqrt(t_allele^2 / (t_allele^2 + df_resid))#
cat("Partial correlation for AlleleFreq (controlling for Latitude):", r_partial_allele, "\n")#
#
t_latitude <- model_summary$coefficients["Latitude", "t value"]#
r_partial_latitude <- sqrt(t_latitude^2 / (t_latitude^2 + df_resid))#
cat("Partial correlation for Latitude (controlling for AlleleFreq):", r_partial_latitude, "\n")#
#
# Method 2: Compute partial correlations using the ppcor package#
df_complete <- df[complete.cases(df[, c("Incidence", "Latitude", "AlleleFreq")]), ]#
partial_corr <- pcor(df_complete[, c("Incidence", "Latitude", "AlleleFreq")])#
print(partial_corr)#
#
# --- Export data for GraphPad Prism ---#
# Export two sheets:#
# 1. "Latitude vs Incidence": using complete cases for Latitude and Incidence.#
# 2. "Allele Frequency vs Incidence": export all rows so that missing AlleleFreq values are retained.#
df_latitude <- df[complete.cases(df[, c("Latitude", "Incidence")]), c("Latitude", "Incidence")]#
df_allele <- df[, c("AlleleFreq", "Incidence")]#
export_list <- list("Latitude vs Incidence" = df_latitude,#
                    "Allele Frequency vs Incidence" = df_allele)#
export_path <- "~/Desktop/graphpad_data.xlsx"#
write_xlsx(export_list, path = export_path)#
cat("Data for scatter plots exported to:", export_path, "\n")#
#
# =============================#
# --- Sensitivity Analyses ---#
# =============================#
#
# Sensitivity Analysis 1: Leave-One-Out Analysis#
# Iteratively exclude each country and refit the model (Incidence ~ Latitude + AlleleFreq)#
# Recording coefficient estimates and P-values for both predictors.#
loo_results <- data.frame(Country = character(),#
                          Coef_AlleleFreq = numeric(),#
                          Pval_AlleleFreq = numeric(),#
                          Coef_Latitude = numeric(),#
                          Pval_Latitude = numeric(),#
                          stringsAsFactors = FALSE)#
#
countries <- unique(df$Country)#
for (c in countries) {#
  # Exclude the current country and remove incomplete cases#
  df_temp <- subset(df, Country != c & complete.cases(Incidence, Latitude, AlleleFreq))#
  # Proceed if a sufficient number of observations remain (e.g., n > 10)#
  if (nrow(df_temp) > 10) {#
    model_temp <- lm(Incidence ~ Latitude + AlleleFreq, data = df_temp)#
    coef_temp <- summary(model_temp)$coefficients#
    loo_results <- rbind(loo_results, data.frame(#
      Country = c,#
      Coef_AlleleFreq = coef_temp["AlleleFreq", "Estimate"],#
      Pval_AlleleFreq = coef_temp["AlleleFreq", "Pr(>|t|)"],#
      Coef_Latitude = coef_temp["Latitude", "Estimate"],#
      Pval_Latitude = coef_temp["Latitude", "Pr(>|t|)"]#
    ))#
  }#
}#
#
cat("\n--- Leave-One-Out Sensitivity Analysis Results ---\n")#
print(loo_results)#
#
# Sensitivity Analysis 2: Random Variation in Allele Frequencies by ±25 Percentage Points (10,000 iterations)#
# In each iteration, randomly adjust each country's allele frequency by a value drawn from a uniform distribution between -25 and +25#
# (with values capped between 0 and 100). Then, fit the multiple regression model (Incidence ~ Latitude + AlleleFreq)#
# and record whether the associations are significant (P < 0.05).#
set.seed(123)  # for reproducibility#
n_iterations <- 10000#
pvals_allele <- numeric(n_iterations)#
pvals_latitude <- numeric(n_iterations)#
#
for (i in 1:n_iterations) {#
  df_random_iter <- df  # start with the original data#
  random_adjustments <- runif(nrow(df_random_iter), min = -25, max = 25)#
  df_random_iter$AlleleFreq <- pmin(pmax(df_random_iter$AlleleFreq + random_adjustments, 0), 100)#
  model_iter <- lm(Incidence ~ Latitude + AlleleFreq, data = df_random_iter)#
  summ_iter <- summary(model_iter)#
  pvals_allele[i] <- summ_iter$coefficients["AlleleFreq", "Pr(>|t|)"]#
  pvals_latitude[i] <- summ_iter$coefficients["Latitude", "Pr(>|t|)"]#
}#
#
sig_allele <- sum(pvals_allele < 0.05, na.rm = TRUE)#
sig_latitude <- sum(pvals_latitude < 0.05, na.rm = TRUE)#
#
cat("\n--- Random Variation Sensitivity Analysis (10,000 iterations) ---\n")#
cat("Out of", n_iterations, "iterations:\n")#
cat("The association for AlleleFreq was significant (P < 0.05) in", sig_allele, "iterations.\n")#
cat("The association for Latitude was significant (P < 0.05) in", sig_latitude, "iterations.\n")
# Load required libraries#
library(mediation)#
library(dplyr)#
library(ggplot2)#
#
# Create the dataset with updated data#
data <- data.frame(#
  Country = c("Australia", "Austria", "Belarus", "Belgium", "Canada", "China", #
              "Costa Rica", "Croatia", "Czech Republic", "Denmark", "Estonia", #
              "Finland", "France", "Germany", "India", "Ireland", "Israel", "Italy", #
              "Japan", "Kenya", "Latvia", "Lithuania", "Netherlands", "New Zealand", #
              "Norway", "Philippines", "Poland", "Russia", "Slovakia", "Slovenia", #
              "South Korea", "Spain", "Sweden", "Uganda", "Switzerland", #
              "United Kingdom", "United States", "South Africa"),#
  Incidence = c(7.4, 6.2, 8.7, 7.0, 7.7, 0.6, 1.4, 5.9, 6.1, 11.3, 6.2, 7.2,#
                5.8, 7.7, 0.1, 9.8, 5.3, 4.2, 0.4, 0.3, 5.4, 6.1, 9.5, 10.4,#
                10.1, 0.4, 5.8, 6.9, 8.2, 6.4, 0.9, 3.3, 9.2, 0.4, 4.1, 6.6,#
                5.9, 0.3),#
  Latitude = c(31.7, 47.7, 53.5, 50.8, 51.3, 34.7, 10, 45.2, 49.8, 56, 58.9,#
               62.8, 46.9, 50.9, 21.7, 53.1, 32, 43.1, 36.9, 1.2, 56.9, 55.3,#
               52.1, 40, 63.4, 12.7, 51.8, 58.5, 48.7, 46.2, 36.4, 40.1, 60.4,#
               0.4, 47, 53.1, 39.4, 27.7),#
  Frequency = c(79, NA, 82, NA, 73, 1, NA, NA, NA, 86, 84, 88, 59, 82, 13,#
                80, 42, 50, 1, 1, NA, 82, 75, NA, 86, NA, 78, 71, NA, 66, 1,#
                42, 81, 0, 74, 69, 57, 1)#
)#
#
# Ensure Frequency is numeric#
data$Frequency <- as.numeric(data$Frequency)#
#
# Filter data to include only observations with non-missing allele frequency#
df_filtered <- filter(data, !is.na(Frequency))#
#
# Fit the mediator model: Does latitude predict allele frequency?#
mediator_model <- lm(Frequency ~ Latitude, data = df_filtered)#
summary(mediator_model)#
#
# Fit the outcome model: Do latitude and allele frequency predict OM incidence?#
outcome_model <- lm(Incidence ~ Latitude + Frequency, data = df_filtered)#
summary(outcome_model)#
#
# Set seed for reproducibility#
set.seed(123)#
#
# Perform mediation analysis:#
# Treat: Latitude, Mediator: Frequency, Outcome: Incidence#
mediation_result <- mediate(mediator_model, outcome_model, treat = "Latitude", #
                             mediator = "Frequency", boot = TRUE, sims = 1000)#
#
# Summary of mediation analysis#
summary(mediation_result)#
#
# Optionally, plot the mediation effect#
plot(mediation_result)
# Load necessary library to read Excel files#
library(readxl)#
#
# Define the file path (make sure the path is correct on your system)#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Would refer yes or no.xlsx"#
#
# Read the data from the Excel file#
dat <- read_excel(file_path)#
#
# Identify which lesions are melanomas: lesions 4, 9, 13, 22, and 24#
melanoma_ids <- c(4, 9, 13, 22, 24)#
# Create a new column indicating lesion type#
dat$Type <- ifelse(dat$Lesion %in% melanoma_ids, "Melanoma", "Nevus")#
#
# Assuming columns 2 to 30 contain the individual decisions#
# Convert the decision columns to numeric (if not already) and create a new data frame#
decision_data <- as.data.frame(lapply(dat[, 2:30], as.numeric))#
#
# Sum referral decisions per lesion (each row)#
dat$ReferralSum <- rowSums(decision_data)#
# Total number of decisions per lesion (should be 29)#
dat$TotalDecisions <- ncol(decision_data)#
#
# Aggregate referral counts and total decisions by lesion type#
agg <- aggregate(cbind(ReferralSum, TotalDecisions) ~ Type, data = dat, sum)#
print(agg)#
#
# Compare the referral proportions between melanomas and nevi using prop.test#
referral_test <- prop.test(x = agg$ReferralSum, n = agg$TotalDecisions)#
print(referral_test)
# Using the previously aggregated data 'agg':#
#   Type     ReferralSum TotalDecisions#
# 1 Melanoma         138            145#
# 2    Nevus         397            580#
#
# Fit a logistic regression model using the aggregated data:#
glm_model <- glm(cbind(ReferralSum, TotalDecisions - ReferralSum) ~ Type,#
                 data = agg, family = binomial)#
#
# Display the model summary#
summary(glm_model)
# Load necessary library to read Excel files#
library(readxl)#
#
# Define the file path (make sure the path is correct on your system)#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Would refer yes or no.xlsx"#
#
# Read the data from the Excel file#
dat <- read_excel(file_path)#
#
# Identify which lesions are melanomas: lesions 4, 9, 13, 22, and 24#
melanoma_ids <- c(4, 9, 13, 22, 24)#
# Create a new column indicating lesion type#
dat$Type <- ifelse(dat$Lesion %in% melanoma_ids, "Melanoma", "Nevus")#
#
# ---- Analysis based on expert (optician/optometrist) decisions ----#
#
# Assuming columns 2 to 30 contain the individual decisions#
# Convert the decision columns to numeric (if not already) and create a new data frame#
decision_data <- as.data.frame(lapply(dat[, 2:30], as.numeric))#
#
# Sum referral decisions per lesion (each row)#
dat$ReferralSum <- rowSums(decision_data)#
# Total number of decisions per lesion (should be 29)#
dat$TotalDecisions <- ncol(decision_data)#
#
# Aggregate referral counts and total decisions by lesion type#
agg <- aggregate(cbind(ReferralSum, TotalDecisions) ~ Type, data = dat, sum)#
print(agg)#
#
# Compare the referral proportions between melanomas and nevi using prop.test#
referral_test <- prop.test(x = agg$ReferralSum, n = agg$TotalDecisions)#
print(referral_test)#
#
# Fit a logistic regression model using the aggregated data:#
glm_model <- glm(cbind(ReferralSum, TotalDecisions - ReferralSum) ~ Type,#
                 data = agg, family = binomial)#
# Display the model summary#
summary(glm_model)#
#
# ---- Analysis based on MelAInoma score decisions ----#
# Here, we assume that the "MelAInoma" column contains a binary referral decision (0 = "would not refer", 1 = "would refer")#
# For the MelAInoma-based analysis, each lesion contributes one decision.#
dat$TotalMelAInoma <- 1#
#
# Aggregate MelAInoma-based referral counts by lesion type#
agg_mel <- aggregate(cbind(MelAInoma, TotalMelAInoma) ~ Type, data = dat, sum)#
print(agg_mel)#
#
# Compare the referral proportions for MelAInoma-based decisions using prop.test#
mel_test <- prop.test(x = agg_mel$MelAInoma, n = agg_mel$TotalMelAInoma)#
print(mel_test)#
#
# Fit a logistic regression model for the MelAInoma-based decision:#
glm_model_mel <- glm(cbind(MelAInoma, TotalMelAInoma - MelAInoma) ~ Type,#
                     data = agg_mel, family = binomial)#
# Display the model summary for MelAInoma-based decisions#
summary(glm_model_mel)
# Load necessary libraries#
library(readxl)#
if (!require(rmda)) {#
  install.packages("rmda")#
  library(rmda)#
}#
#
# Define the file path (make sure the path is correct on your system)#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Would refer yes or no.xlsx"#
#
# Read the data from the Excel file#
dat <- read_excel(file_path)#
#
# Identify which lesions are melanomas: lesions 4, 9, 13, 22, and 24#
melanoma_ids <- c(4, 9, 13, 22, 24)#
# Create a new column indicating lesion type based on expert evaluation (ocular oncologists)#
dat$Type <- ifelse(dat$Lesion %in% melanoma_ids, "Melanoma", "Nevus")#
#
# ---- Analysis based on expert (optician/optometrist) decisions ----#
#
# Assuming columns 2 to 30 contain the individual decisions#
# Convert the decision columns to numeric (if not already) and create a new data frame#
decision_data <- as.data.frame(lapply(dat[, 2:30], as.numeric))#
#
# Sum referral decisions per lesion (each row) and record the total number of decisions per lesion (should be 29)#
dat$ReferralSum <- rowSums(decision_data)#
dat$TotalDecisions <- ncol(decision_data)#
#
# Aggregate referral counts and total decisions by lesion type#
agg <- aggregate(cbind(ReferralSum, TotalDecisions) ~ Type, data = dat, sum)#
print(agg)#
#
# Compare the referral proportions between melanomas and nevi using prop.test#
referral_test <- prop.test(x = agg$ReferralSum, n = agg$TotalDecisions)#
print(referral_test)#
#
# Fit a logistic regression model using the aggregated data:#
glm_model <- glm(cbind(ReferralSum, TotalDecisions - ReferralSum) ~ Type,#
                 data = agg, family = binomial)#
# Display the model summary#
summary(glm_model)#
#
# ---- Analysis based on MelAInoma score decisions ----#
# Here, the "MelAInoma" column already contains a binary referral decision (0 = "would not refer", 1 = "would refer")#
# For the MelAInoma-based analysis, each lesion contributes one decision.#
dat$TotalMelAInoma <- 1#
#
# Aggregate MelAInoma-based referral counts by lesion type#
agg_mel <- aggregate(cbind(MelAInoma, TotalMelAInoma) ~ Type, data = dat, sum)#
print(agg_mel)#
#
# Compare the referral proportions for MelAInoma-based decisions using prop.test#
mel_test <- prop.test(x = agg_mel$MelAInoma, n = agg_mel$TotalMelAInoma)#
print(mel_test)#
#
# Fit a logistic regression model for the MelAInoma-based decision:#
glm_model_mel <- glm(cbind(MelAInoma, TotalMelAInoma - MelAInoma) ~ Type,#
                     data = agg_mel, family = binomial)#
# Display the model summary for MelAInoma-based decisions#
summary(glm_model_mel)#
#
# ---- Decision Curve Analysis ----#
# Create a binary outcome for true lesion type (1 = Melanoma, 0 = Nevus) based on the expert gold standard#
dat$outcome <- ifelse(dat$Type == "Melanoma", 1, 0)#
# For expert decisions, calculate the predicted probability as the proportion of referrals per lesion#
dat$expert_prob <- dat$ReferralSum / dat$TotalDecisions#
#
# Perform decision curve analysis for both models over a range of threshold probabilities (0 to 1)#
dca_expert <- decision_curve(outcome ~ expert_prob, data = dat, family = binomial(link = "logit"),#
                             thresholds = seq(0, 1, by = 0.01), confidence.intervals = FALSE,#
                             study.design = "cohort", policy = "opt-in")#
dca_mel <- decision_curve(outcome ~ MelAInoma, data = dat, family = binomial(link = "logit"),#
                          thresholds = seq(0, 1, by = 0.01), confidence.intervals = FALSE,#
                          study.design = "cohort", policy = "opt-in")#
#
# Plot the decision curves comparing the net benefit of expert decisions vs. MelAInoma-based decisions#
plot_decision_curve(list(dca_expert, dca_mel),#
                    curve.names = c("Expert Decision", "MelAInoma-based"),#
                    xlab = "Threshold probability",#
                    legend.position = "bottomright")
# Load necessary libraries#
library(readxl)#
if (!require(rmda)) {#
  install.packages("rmda")#
  library(rmda)#
}#
#
# Define the file path (make sure the path is correct on your system)#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Would refer yes or no.xlsx"#
#
# Read the data from the Excel file#
dat <- read_excel(file_path)#
#
# Identify which lesions are melanomas: lesions 4, 9, 13, 22, and 24#
melanoma_ids <- c(4, 9, 13, 22, 24)#
# Create a new column indicating lesion type based on expert evaluation (ocular oncologists)#
dat$Type <- ifelse(dat$Lesion %in% melanoma_ids, "Melanoma", "Nevus")#
#
# ---- Analysis based on expert (optician/optometrist) decisions ----#
#
# Assuming columns 2 to 30 contain the individual decisions#
# Convert the decision columns to numeric (if not already) and create a new data frame#
decision_data <- as.data.frame(lapply(dat[, 2:30], as.numeric))#
#
# Sum referral decisions per lesion (each row) and record the total number of decisions per lesion (should be 29)#
dat$ReferralSum <- rowSums(decision_data)#
dat$TotalDecisions <- ncol(decision_data)#
#
# Aggregate referral counts and total decisions by lesion type#
agg <- aggregate(cbind(ReferralSum, TotalDecisions) ~ Type, data = dat, sum)#
print(agg)#
#
# Compare the referral proportions between melanomas and nevi using prop.test#
referral_test <- prop.test(x = agg$ReferralSum, n = agg$TotalDecisions)#
print(referral_test)#
#
# Fit a logistic regression model using the aggregated data:#
glm_model <- glm(cbind(ReferralSum, TotalDecisions - ReferralSum) ~ Type,#
                 data = agg, family = binomial)#
# Display the model summary#
summary(glm_model)#
#
# ---- Analysis based on MelAInoma score decisions ----#
# Here, the "MelAInoma" column already contains a binary referral decision (0 = "would not refer", 1 = "would refer")#
# For the MelAInoma-based analysis, each lesion contributes one decision.#
dat$TotalMelAInoma <- 1#
#
# Aggregate MelAInoma-based referral counts by lesion type#
agg_mel <- aggregate(cbind(MelAInoma, TotalMelAInoma) ~ Type, data = dat, sum)#
print(agg_mel)#
#
# Compare the referral proportions for MelAInoma-based decisions using prop.test#
mel_test <- prop.test(x = agg_mel$MelAInoma, n = agg_mel$TotalMelAInoma)#
print(mel_test)#
#
# Fit a logistic regression model for the MelAInoma-based decision:#
glm_model_mel <- glm(cbind(MelAInoma, TotalMelAInoma - MelAInoma) ~ Type,#
                     data = agg_mel, family = binomial)#
# Display the model summary for MelAInoma-based decisions#
summary(glm_model_mel)#
#
# ---- Decision Curve Analysis ----#
# Create a binary outcome for true lesion type (1 = Melanoma, 0 = Nevus) based on the expert gold standard#
dat$outcome <- ifelse(dat$Type == "Melanoma", 1, 0)#
# For expert decisions, calculate the predicted probability as the proportion of referrals per lesion#
dat$expert_prob <- dat$ReferralSum / dat$TotalDecisions#
#
# Perform decision curve analysis for both models over a range of threshold probabilities (0 to 1 by 0.01)#
dca_expert <- decision_curve(outcome ~ expert_prob, data = dat, family = binomial(link = "logit"),#
                             thresholds = seq(0, 1, by = 0.01), confidence.intervals = FALSE,#
                             study.design = "cohort", policy = "opt-in")#
dca_mel <- decision_curve(outcome ~ MelAInoma, data = dat, family = binomial(link = "logit"),#
                          thresholds = seq(0, 1, by = 0.01), confidence.intervals = FALSE,#
                          study.design = "cohort", policy = "opt-in")#
#
# Plot the decision curves comparing the net benefit of expert decisions vs. MelAInoma-based decisions#
plot_decision_curve(list(dca_expert, dca_mel),#
                    curve.names = c("Expert Decision", "MelAInoma-based"),#
                    xlab = "Threshold probability",#
                    legend.position = "bottomright")#
#
# Extract net benefit values for thresholds of interest: 10, 20, 30, 40, 50, 60, 70, 80, and 90%#
thresholds <- c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)#
expert_idx <- which(dca_expert$threshold %in% thresholds)#
mel_idx <- which(dca_mel$threshold %in% thresholds)#
#
result_table <- data.frame(Threshold = thresholds,#
                             Expert_Net_Benefit = dca_expert$net_benefit[expert_idx],#
                             MelAInoma_Net_Benefit = dca_mel$net_benefit[mel_idx])#
print(result_table)
# Load necessary libraries#
library(readxl)#
if (!require(rmda)) {#
  install.packages("rmda")#
  library(rmda)#
}#
#
# Define the file path (make sure the path is correct on your system)#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Would refer yes or no.xlsx"#
#
# Read the data from the Excel file#
dat <- read_excel(file_path)#
#
# Identify which lesions are melanomas: lesions 4, 9, 13, 22, and 24#
melanoma_ids <- c(4, 9, 13, 22, 24)#
# Create a new column indicating lesion type based on expert evaluation (ocular oncologists)#
dat$Type <- ifelse(dat$Lesion %in% melanoma_ids, "Melanoma", "Nevus")#
#
# ---- Analysis based on expert (optician/optometrist) decisions ----#
#
# Assuming columns 2 to 30 contain the individual decisions#
# Convert the decision columns to numeric (if not already) and create a new data frame#
decision_data <- as.data.frame(lapply(dat[, 2:30], as.numeric))#
#
# Sum referral decisions per lesion (each row) and record the total number of decisions per lesion (should be 29)#
dat$ReferralSum <- rowSums(decision_data)#
dat$TotalDecisions <- ncol(decision_data)#
#
# Aggregate referral counts and total decisions by lesion type#
agg <- aggregate(cbind(ReferralSum, TotalDecisions) ~ Type, data = dat, sum)#
print(agg)#
#
# Compare the referral proportions between melanomas and nevi using prop.test#
referral_test <- prop.test(x = agg$ReferralSum, n = agg$TotalDecisions)#
print(referral_test)#
#
# Fit a logistic regression model using the aggregated data:#
glm_model <- glm(cbind(ReferralSum, TotalDecisions - ReferralSum) ~ Type,#
                 data = agg, family = binomial)#
# Display the model summary#
summary(glm_model)#
#
# ---- Analysis based on MelAInoma score decisions ----#
# Here, the "MelAInoma" column already contains a binary referral decision (0 = "would not refer", 1 = "would refer")#
# For the MelAInoma-based analysis, each lesion contributes one decision.#
dat$TotalMelAInoma <- 1#
#
# Aggregate MelAInoma-based referral counts by lesion type#
agg_mel <- aggregate(cbind(MelAInoma, TotalMelAInoma) ~ Type, data = dat, sum)#
print(agg_mel)#
#
# Compare the referral proportions for MelAInoma-based decisions using prop.test#
mel_test <- prop.test(x = agg_mel$MelAInoma, n = agg_mel$TotalMelAInoma)#
print(mel_test)#
#
# Fit a logistic regression model for the MelAInoma-based decision:#
glm_model_mel <- glm(cbind(MelAInoma, TotalMelAInoma - MelAInoma) ~ Type,#
                     data = agg_mel, family = binomial)#
# Display the model summary for MelAInoma-based decisions#
summary(glm_model_mel)#
#
# ---- Decision Curve Analysis ----#
# Create a binary outcome for true lesion type (1 = Melanoma, 0 = Nevus) based on the expert gold standard#
dat$outcome <- ifelse(dat$Type == "Melanoma", 1, 0)#
# For expert decisions, calculate the predicted probability as the proportion of referrals per lesion#
dat$expert_prob <- dat$ReferralSum / dat$TotalDecisions#
#
# Perform decision curve analysis for both models over a range of threshold probabilities (0 to 1 by 0.01)#
dca_expert <- decision_curve(outcome ~ expert_prob, data = dat, family = binomial(link = "logit"),#
                             thresholds = seq(0, 1, by = 0.01), confidence.intervals = FALSE,#
                             study.design = "cohort", policy = "opt-in")#
dca_mel <- decision_curve(outcome ~ MelAInoma, data = dat, family = binomial(link = "logit"),#
                          thresholds = seq(0, 1, by = 0.01), confidence.intervals = FALSE,#
                          study.design = "cohort", policy = "opt-in")#
#
# Plot the decision curves comparing the net benefit of expert decisions vs. MelAInoma-based decisions#
plot_decision_curve(list(dca_expert, dca_mel),#
                    curve.names = c("Expert Decision", "MelAInoma-based"),#
                    xlab = "Threshold probability",#
                    legend.position = "bottomright")#
#
# Extract net benefit values for thresholds of interest: 10, 20, 30, 40, 50, 60, 70, 80, and 90%#
thresholds <- c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)#
#
expert_net_benefit <- sapply(thresholds, function(t) {#
  dca_expert$net_benefit[which.min(abs(dca_expert$threshold - t))]#
})#
#
mel_net_benefit <- sapply(thresholds, function(t) {#
  dca_mel$net_benefit[which.min(abs(dca_mel$threshold - t))]#
})#
#
result_table <- data.frame(Threshold = thresholds,#
                             Expert_Net_Benefit = expert_net_benefit,#
                             MelAInoma_Net_Benefit = mel_net_benefit)#
print(result_table)
# Load necessary libraries#
library(readxl)#
if (!require(rmda)) {#
  install.packages("rmda")#
  library(rmda)#
}#
#
# Define the file path (make sure the path is correct on your system)#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Would refer yes or no.xlsx"#
#
# Read the data from the Excel file#
dat <- read_excel(file_path)#
#
# Identify which lesions are melanomas: lesions 4, 9, 13, 22, and 24#
melanoma_ids <- c(4, 9, 13, 22, 24)#
# Create a new column indicating lesion type based on expert evaluation (ocular oncologists)#
dat$Type <- ifelse(dat$Lesion %in% melanoma_ids, "Melanoma", "Nevus")#
#
# ---- Analysis based on expert (optician/optometrist) decisions ----#
#
# Assuming columns 2 to 30 contain the individual decisions#
# Convert the decision columns to numeric (if not already) and create a new data frame#
decision_data <- as.data.frame(lapply(dat[, 2:30], as.numeric))#
#
# Sum referral decisions per lesion (each row) and record the total number of decisions per lesion (should be 29)#
dat$ReferralSum <- rowSums(decision_data)#
dat$TotalDecisions <- ncol(decision_data)#
#
# Aggregate referral counts and total decisions by lesion type#
agg <- aggregate(cbind(ReferralSum, TotalDecisions) ~ Type, data = dat, sum)#
print(agg)#
#
# Compare the referral proportions between melanomas and nevi using prop.test#
referral_test <- prop.test(x = agg$ReferralSum, n = agg$TotalDecisions)#
print(referral_test)#
#
# Fit a logistic regression model using the aggregated data:#
glm_model <- glm(cbind(ReferralSum, TotalDecisions - ReferralSum) ~ Type,#
                 data = agg, family = binomial)#
# Display the model summary#
summary(glm_model)#
#
# ---- Analysis based on MelAInoma score decisions ----#
# Here, the "MelAInoma" column already contains a binary referral decision (0 = "would not refer", 1 = "would refer")#
# For the MelAInoma-based analysis, each lesion contributes one decision.#
dat$TotalMelAInoma <- 1#
#
# Aggregate MelAInoma-based referral counts by lesion type#
agg_mel <- aggregate(cbind(MelAInoma, TotalMelAInoma) ~ Type, data = dat, sum)#
print(agg_mel)#
#
# Compare the referral proportions for MelAInoma-based decisions using prop.test#
mel_test <- prop.test(x = agg_mel$MelAInoma, n = agg_mel$TotalMelAInoma)#
print(mel_test)#
#
# Fit a logistic regression model for the MelAInoma-based decision:#
glm_model_mel <- glm(cbind(MelAInoma, TotalMelAInoma - MelAInoma) ~ Type,#
                     data = agg_mel, family = binomial)#
# Display the model summary for MelAInoma-based decisions#
summary(glm_model_mel)#
#
# ---- Decision Curve Analysis ----#
# Create a binary outcome for true lesion type (1 = Melanoma, 0 = Nevus) based on the expert gold standard#
dat$outcome <- ifelse(dat$Type == "Melanoma", 1, 0)#
# For expert decisions, calculate the predicted probability as the proportion of referrals per lesion#
dat$expert_prob <- dat$ReferralSum / dat$TotalDecisions#
#
# Perform decision curve analysis for the expert-based model over a range of threshold probabilities (0 to 1 by 0.01)#
dca_expert <- decision_curve(outcome ~ expert_prob, data = dat, family = binomial(link = "logit"),#
                             thresholds = seq(0, 1, by = 0.01), confidence.intervals = FALSE,#
                             study.design = "cohort", policy = "opt-in")#
#
# Plot the decision curves comparing the net benefit of expert decisions vs. MelAInoma-based decisions#
# (For the MelAInoma-based strategy, we will calculate net benefit manually below)#
plot_decision_curve(list(dca_expert),#
                    curve.names = c("Expert Decision"),#
                    xlab = "Threshold probability",#
                    legend.position = "bottomright")#
#
# Extract net benefit values for the expert-based model at thresholds 10, 20, ..., 90%#
thresholds <- c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)#
expert_net_benefit <- sapply(thresholds, function(t) {#
  dca_expert$net_benefit[which.min(abs(dca_expert$threshold - t))]#
})#
#
# --- Manual calculation of net benefit for the MelAInoma-based decision rule ---#
# From the aggregated MelAInoma decisions we have:#
# For melanomas: TP = 4 (referred out of 5)#
# For nevi: FP = 2 (referred out of 20)#
n <- nrow(dat)#
TP <- sum(dat$outcome == 1 & dat$MelAInoma == 1)  # expected 4#
FP <- sum(dat$outcome == 0 & dat$MelAInoma == 1)  # expected 2#
#
# For each threshold, net benefit (NB) is calculated as:#
# NB = (TP/n) - (FP/n) * (threshold/(1-threshold))#
mel_net_benefit <- sapply(thresholds, function(t) {#
  (TP / n) - (FP / n) * (t / (1 - t))#
})#
#
# Create a table of results#
result_table <- data.frame(Threshold = thresholds,#
                             Expert_Net_Benefit = expert_net_benefit,#
                             MelAInoma_Net_Benefit = mel_net_benefit)#
print(result_table)
# Extract net benefit values for the expert-based model at thresholds 10, 20, ..., 90%#
thresholds <- c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)#
expert_net_benefit <- sapply(thresholds, function(t) {#
  dca_expert$net_benefit[which.min(abs(dca_expert$thresholds - t))]#
})#
#
# --- Manual calculation of net benefit for the MelAInoma-based decision rule ---#
# From the aggregated MelAInoma decisions we have:#
# For melanomas: TP = 4 (referred out of 5)#
# For nevi: FP = 2 (referred out of 20)#
n <- nrow(dat)#
TP <- sum(dat$outcome == 1 & dat$MelAInoma == 1)  # expected 4#
FP <- sum(dat$outcome == 0 & dat$MelAInoma == 1)  # expected 2#
#
# Calculate net benefit (NB) using the standard formula:#
# NB = (TP/n) - (FP/n) * (threshold/(1-threshold))#
mel_net_benefit <- sapply(thresholds, function(t) {#
  (TP / n) - (FP / n) * (t / (1 - t))#
})#
#
# Create a table of results#
result_table <- data.frame(Threshold = thresholds,#
                             Expert_Net_Benefit = expert_net_benefit,#
                             MelAInoma_Net_Benefit = mel_net_benefit)#
print(result_table)
# Load necessary libraries#
library(readxl)#
if (!require(rmda)) {#
  install.packages("rmda")#
  library(rmda)#
}#
#
# Define the file path#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Would refer yes or no.xlsx"#
#
# Read the data from the Excel file#
dat <- read_excel(file_path)#
#
# Convert MelAInoma column to numeric (if it’s not already)#
dat$MelAInoma <- as.numeric(as.character(dat$MelAInoma))#
#
# Identify which lesions are melanomas: lesions 4, 9, 13, 22, and 24#
melanoma_ids <- c(4, 9, 13, 22, 24)#
dat$Type <- ifelse(dat$Lesion %in% melanoma_ids, "Melanoma", "Nevus")#
#
# ---- Analysis based on expert (optician/optometrist) decisions ----#
decision_data <- as.data.frame(lapply(dat[, 2:30], as.numeric))#
dat$ReferralSum <- rowSums(decision_data)#
dat$TotalDecisions <- ncol(decision_data)#
agg <- aggregate(cbind(ReferralSum, TotalDecisions) ~ Type, data = dat, sum)#
print(agg)#
#
referral_test <- prop.test(x = agg$ReferralSum, n = agg$TotalDecisions)#
print(referral_test)#
#
glm_model <- glm(cbind(ReferralSum, TotalDecisions - ReferralSum) ~ Type,#
                 data = agg, family = binomial)#
summary(glm_model)#
#
# ---- Analysis based on MelAInoma score decisions ----#
dat$TotalMelAInoma <- 1#
agg_mel <- aggregate(cbind(MelAInoma, TotalMelAInoma) ~ Type, data = dat, sum)#
print(agg_mel)#
#
mel_test <- prop.test(x = agg_mel$MelAInoma, n = agg_mel$TotalMelAInoma)#
print(mel_test)#
#
glm_model_mel <- glm(cbind(MelAInoma, TotalMelAInoma - MelAInoma) ~ Type,#
                     data = agg_mel, family = binomial)#
summary(glm_model_mel)#
#
# ---- Decision Curve Analysis ----#
dat$outcome <- ifelse(dat$Type == "Melanoma", 1, 0)#
dat$expert_prob <- dat$ReferralSum / dat$TotalDecisions#
dca_expert <- decision_curve(outcome ~ expert_prob, data = dat, family = binomial(link = "logit"),#
                             thresholds = seq(0, 1, by = 0.01), confidence.intervals = FALSE,#
                             study.design = "cohort", policy = "opt-in")#
#
plot_decision_curve(list(dca_expert),#
                    curve.names = c("Expert Decision"),#
                    xlab = "Threshold probability",#
                    legend.position = "bottomright")#
#
# Extract net benefit values for thresholds of interest (10% to 90%)#
thresholds <- c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)#
#
expert_net_benefit <- sapply(thresholds, function(t) {#
  dca_expert$net_benefit[which.min(abs(dca_expert$thresholds - t))]#
})#
#
# Manual calculation for the MelAInoma-based decision rule:#
n <- nrow(dat)  # total lesions, 25#
TP <- sum(dat$outcome == 1 & dat$MelAInoma == 1)  # expected 4 (TP)#
FP <- sum(dat$outcome == 0 & dat$MelAInoma == 1)  # expected 2 (FP)#
mel_net_benefit <- sapply(thresholds, function(t) {#
  (TP / n) - (FP / n) * (t / (1 - t))#
})#
#
result_table <- data.frame(Threshold = thresholds,#
                             Expert_Net_Benefit = expert_net_benefit,#
                             MelAInoma_Net_Benefit = mel_net_benefit)#
print(result_table)
# Load necessary libraries#
library(readxl)  # Not used here, but loaded for consistency with your workflow#
if (!require(rmda)) {#
  install.packages("rmda")#
  library(rmda)#
}#
#
# Define the data as a string (tab‐separated), including header and 25 rows#
data_str <- "Lesion\t1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12\t13\t14\t15\t16\t17\t18\t19\t20\t21\t22\t23\t24\t25\t26\t27\t28\t29\tMelAInoma#
1\t0\t0\t1\t0\t0\t1\t0\t0\t1\t1\t1\t1\t0\t0\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\t0\t1\t1\t0\t0#
2\t0\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t0#
3\t0\t1\t0\t1\t1\t1\t0\t1\t0\t0\t1\t1\t1\t1\t1\t0\t1\t0\t0\t0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0#
4\t0\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t0\t1#
5\t0\t1\t1\t1\t1\t1\t0\t1\t1\t0\t1\t1\t1\t0\t1\t1\t0\t0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t1\t0\t0#
6\t0\t1\t1\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t1\t1\t1\t1\t0\t0#
7\t0\t0\t0\t1\t0\t0\t0\t1\t1\t0\t1\t1\t0\t0\t1\t0\t1\t0\t1\t0\t0\t0\t1\t0\t0\t1\t1\t1\t0\t0#
8\t1\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t1\t0\t1\t0\t1\t1\t1\t1\t1\t0#
9\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1#
10\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\t1\t1\t1\t1\t1\t0\t1\t0\t1\t0\t0\t1\t1\t1\t1\t1\t0\t1\t0\t0#
11\t0\t0\t0\t1\t0\t1\t0\t0\t0\t0\t1\t1\t0\t0\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\t1\t0\t1\t1\t0\t0#
12\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t1\t1\t0\t0#
13\t0\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t0#
14\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t1\t1\t0#
15\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\t0\t1\t0\t1\t0\t0\t0\t0\t1\t0\t0#
16\t0\t0\t1\t0\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t1\t0\t1\t1\t0\t1\t1\t0\t0#
17\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t0#
18\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t0#
19\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0#
20\t1\t1\t1\t1\t0\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t0\t0\t1\t1\t1\t0\t1\t1\t1#
21\t1\t1\t1\t1\t0\t1\t1\t1\t1\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t1\t1#
22\t0\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t0\t1\t1\t0\t1\t1\t1\t1\t1\t1#
23\t1\t1\t1\t1\t0\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t0#
24\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1#
25\t0\t1\t0\t1\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t0\t1\t0\t0\t1\t0\t1\t1\t1\t1\t0\t0"#
#
# Read data from the string#
dat <- read.table(text = data_str, header = TRUE, sep = "\t", stringsAsFactors = FALSE)#
#
# Convert necessary columns to numeric.#
# Lesion, columns "1" to "29", and MelAInoma should all be numeric.#
dat$Lesion <- as.numeric(dat$Lesion)#
# The column names "1" to "29" are non-syntactic names, so we use backticks.#
for(col in as.character(1:29)){#
  dat[[col]] <- as.numeric(dat[[col]])#
}#
dat$MelAInoma <- as.numeric(dat$MelAInoma)#
#
# Convert MelAInoma to numeric (if not already)#
# (This should now be numeric based on the loop above.)#
#
# Identify which lesions are melanomas based on expert evaluation.#
# According to your instructions, lesions 4, 9, 13, 22, and 24 are melanomas.#
melanoma_ids <- c(4, 9, 13, 22, 24)#
dat$Type <- ifelse(dat$Lesion %in% melanoma_ids, "Melanoma", "Nevus")#
#
# ---- Analysis based on expert (optician/optometrist) decisions ----#
# Columns 2 to 30 (the columns named "1" to "29") contain the individual decisions.#
decision_data <- dat[, as.character(1:29)]#
# Ensure decisions are numeric.#
decision_data[] <- lapply(decision_data, as.numeric)#
#
# Calculate row-wise sums (number of "would refer" votes) and record total decisions (should be 29).#
dat$ReferralSum <- rowSums(decision_data)#
dat$TotalDecisions <- ncol(decision_data)#
#
# Aggregate referral counts and total decisions by lesion type.#
agg <- aggregate(cbind(ReferralSum, TotalDecisions) ~ Type, data = dat, sum)#
print(agg)#
#
referral_test <- prop.test(x = agg$ReferralSum, n = agg$TotalDecisions)#
print(referral_test)#
#
glm_model <- glm(cbind(ReferralSum, TotalDecisions - ReferralSum) ~ Type,#
                 data = agg, family = binomial)#
summary(glm_model)#
#
# ---- Analysis based on MelAInoma score decisions ----#
# For the MelAInoma-based analysis, each lesion contributes one decision.#
dat$TotalMelAInoma <- 1#
agg_mel <- aggregate(cbind(MelAInoma, TotalMelAInoma) ~ Type, data = dat, sum)#
print(agg_mel)#
#
mel_test <- prop.test(x = agg_mel$MelAInoma, n = agg_mel$TotalMelAInoma)#
print(mel_test)#
#
glm_model_mel <- glm(cbind(MelAInoma, TotalMelAInoma - MelAInoma) ~ Type,#
                     data = agg_mel, family = binomial)#
summary(glm_model_mel)#
#
# ---- Decision Curve Analysis ----#
# Create a binary outcome for true lesion type (1 = Melanoma, 0 = Nevus), based on expert gold standard.#
dat$outcome <- ifelse(dat$Type == "Melanoma", 1, 0)#
# For expert decisions, compute the predicted probability as the proportion of referrals.#
dat$expert_prob <- dat$ReferralSum / dat$TotalDecisions#
#
# Perform decision curve analysis for the expert-based model.#
dca_expert <- decision_curve(outcome ~ expert_prob, #
                             data = dat, #
                             family = binomial(link = "logit"),#
                             thresholds = seq(0, 1, by = 0.01), #
                             confidence.intervals = FALSE,#
                             study.design = "cohort", #
                             policy = "opt-in")#
#
plot_decision_curve(list(dca_expert),#
                    curve.names = c("Expert Decision"),#
                    xlab = "Threshold probability",#
                    legend.position = "bottomright")#
#
# Extract net benefit values for thresholds of interest (10% to 90%).#
thresholds <- c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)#
#
expert_net_benefit <- sapply(thresholds, function(t) {#
  dca_expert$net_benefit[which.min(abs(dca_expert$thresholds - t))]#
})#
#
# --- Manual calculation for the MelAInoma-based decision rule ---#
# From the aggregated MelAInoma decisions, for melanomas (TP) and nevi (FP):#
n <- nrow(dat)  # total lesions, here 25#
TP <- sum(dat$outcome == 1 & dat$MelAInoma == 1)  # expected 4#
FP <- sum(dat$outcome == 0 & dat$MelAInoma == 1)  # expected 2#
#
mel_net_benefit <- sapply(thresholds, function(t) {#
  (TP / n) - (FP / n) * (t / (1 - t))#
})#
#
# Create a results table.#
result_table <- data.frame(Threshold = thresholds,#
                             Expert_Net_Benefit = expert_net_benefit,#
                             MelAInoma_Net_Benefit = mel_net_benefit)#
print(result_table)
# Assuming dca_expert was computed using thresholds = seq(0,1, by=0.01)#
# Define the thresholds of interest.#
thresholds <- c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)#
#
# Extract the expert net benefit values by using the known index.#
expert_net_benefit <- sapply(thresholds, function(t) {#
  idx <- round(t * 100 + 1)  # e.g., for t=0.1, idx = 11#
  dca_expert$net_benefit[idx]#
})#
#
# --- Manual calculation for the MelAInoma-based decision rule ---#
# According to our data:#
# For melanomas (TP): 4 (out of 5)#
# For nevi (FP): 2 (out of 20)#
n <- nrow(dat)  # total lesions; here 25#
TP <- sum(dat$outcome == 1 & dat$MelAInoma == 1)  # expected 4#
FP <- sum(dat$outcome == 0 & dat$MelAInoma == 1)  # expected 2#
#
# Calculate net benefit manually for each threshold:#
mel_net_benefit <- sapply(thresholds, function(t) {#
  (TP / n) - (FP / n) * (t / (1 - t))#
})#
#
# Create the results table.#
result_table <- data.frame(Threshold = thresholds,#
                             Expert_Net_Benefit = expert_net_benefit,#
                             MelAInoma_Net_Benefit = mel_net_benefit)#
print(result_table)
# Define the thresholds of interest.#
thresholds <- c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)#
#
# Extract the expert net benefit values using dca_expert$results.#
# The decision_curve analysis was run with thresholds = seq(0,1, by = 0.01)#
# so we know that 0 corresponds to index 1, 0.01 to index 2, etc.#
expert_net_benefit <- sapply(thresholds, function(t) {#
  idx <- round(t * 100 + 1)  # for example, t = 0.1 => idx = 11#
  dca_expert$results$net_benefit[idx]#
})#
#
# --- Manual calculation for the MelAInoma-based decision rule ---#
# According to our data:#
# For melanomas (TP): 4 (out of 5)#
# For nevi (FP): 2 (out of 20)#
n <- nrow(dat)  # total lesions; here 25#
TP <- sum(dat$outcome == 1 & dat$MelAInoma == 1)  # expected 4#
FP <- sum(dat$outcome == 0 & dat$MelAInoma == 1)  # expected 2#
#
# Calculate net benefit manually for each threshold:#
mel_net_benefit <- sapply(thresholds, function(t) {#
  (TP / n) - (FP / n) * (t / (1 - t))#
})#
#
# Create the results table:#
result_table <- data.frame(Threshold = thresholds,#
                             Expert_Net_Benefit = expert_net_benefit,#
                             MelAInoma_Net_Benefit = mel_net_benefit)#
print(result_table)
# Load required packages#
library(readxl)    # For reading Excel files#
library(dplyr)     # For data manipulation#
library(tidyr)     # For reshaping data#
#
# -------------------------------#
# 1. Read Data and Preprocess#
# -------------------------------#
#
# Change the file path if necessary#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Would refer yes or no.xlsx"#
#
# Read the Excel file (assumes the data is in the first sheet)#
df <- read_excel(file_path)#
#
# Inspect the first few rows of the data#
head(df)#
#
# We assume the first column is "Lesion", the next 29 columns (columns 2 to 30) are the referral responses (0/1) from different opticians/optometrists,#
# and the last column (column 31) is "MelAInoma" (the AI-based decision).#
#
# Create expert-based lesion classification based on lesion number.#
# According to the provided information, lesions 4, 9, 13, 22 and 24 are melanomas.#
melanoma_ids <- c(4, 9, 13, 22, 24)#
df <- df %>%#
  mutate(Expert = ifelse(Lesion %in% melanoma_ids, "Melanoma", "Nevus"))#
#
# -------------------------------#
# 2. Expert Referral Decisions Analysis#
# -------------------------------#
#
# Calculate the total number of "would refer" votes for each lesion from the 29 raters.#
# Assuming that the columns for the raters are the 2nd through 30th columns.#
df <- df %>%#
  mutate(Expert_referrals = rowSums(select(., 2:30)),#
         Expert_proportion = Expert_referrals / 29)#
#
# Compute group-level referral counts:#
# For melanomas:#
melanoma_df <- df %>% filter(Expert == "Melanoma")#
nevus_df   <- df %>% filter(Expert == "Nevus")#
#
# Total decisions: each lesion is decided upon by 29 opticians/optometrists.#
total_melanoma_decisions <- nrow(melanoma_df) * 29#
total_nevus_decisions    <- nrow(nevus_df) * 29#
#
# Total referrals:#
referrals_melanoma <- sum(melanoma_df$Expert_referrals)#
referrals_nevus    <- sum(nevus_df$Expert_referrals)#
#
cat("Expert-based Decisions:\n")#
cat("Melanomas: ", referrals_melanoma, " referrals out of ", total_melanoma_decisions,#
    sprintf(" (%.1f%%)\n", 100 * referrals_melanoma/total_melanoma_decisions))#
cat("Nevi: ", referrals_nevus, " referrals out of ", total_nevus_decisions,#
    sprintf(" (%.1f%%)\n\n", 100 * referrals_nevus/total_nevus_decisions))#
#
# Two-sample test for equality of proportions (Chi-squared test)#
prop_test_expert <- prop.test(x = c(referrals_melanoma, referrals_nevus),#
                                n = c(total_melanoma_decisions, total_nevus_decisions))#
print(prop_test_expert)#
#
# Reshape data to long format for logistic regression analysis.#
# Each row is a decision of one optician for one lesion.#
expert_long <- df %>%#
  pivot_longer(cols = 2:30, names_to = "Rater", values_to = "Referral") %>%#
  mutate(Referral = as.numeric(Referral),#
         Expert = factor(Expert, levels = c("Nevus", "Melanoma")))  # Set "Nevus" as the reference#
#
# Logistic regression: predicting referral decision based on expert classification.#
model_expert <- glm(Referral ~ Expert, data = expert_long, family = binomial)#
summary(model_expert)#
#
# -------------------------------#
# 3. MelAInoma-based Referral Decisions Analysis#
# -------------------------------#
#
# For the MelAInoma-based decision, each lesion has a single decision stored in the "MelAInoma" column.#
# Convert MelAInoma to numeric if needed.#
df$MelAInoma <- as.numeric(df$MelAInoma)#
#
# Count referrals by MelAInoma stratified by expert classification.#
referrals_melanoma_AI <- sum(df$MelAInoma[df$Expert == "Melanoma"])#
referrals_nevus_AI    <- sum(df$MelAInoma[df$Expert == "Nevus"])#
total_melanoma_AI     <- nrow(df %>% filter(Expert == "Melanoma"))#
total_nevus_AI        <- nrow(df %>% filter(Expert == "Nevus"))#
#
cat("MelAInoma-based Decisions:\n")#
cat("Melanomas: ", referrals_melanoma_AI, " referrals out of ", total_melanoma_AI,#
    sprintf(" (%.1f%%)\n", 100 * referrals_melanoma_AI/total_melanoma_AI))#
cat("Nevi: ", referrals_nevus_AI, " referrals out of ", total_nevus_AI,#
    sprintf(" (%.1f%%)\n\n", 100 * referrals_nevus_AI/total_nevus_AI))#
#
# Two-sample test for equality of proportions for MelAInoma-based decisions#
prop_test_AI <- prop.test(x = c(referrals_melanoma_AI, referrals_nevus_AI),#
                          n = c(total_melanoma_AI, total_nevus_AI))#
print(prop_test_AI)#
#
# Logistic regression for MelAInoma decisions (note: one decision per lesion)#
df$Expert <- factor(df$Expert, levels = c("Nevus", "Melanoma")) # Ensure correct reference order#
model_AI <- glm(MelAInoma ~ Expert, data = df, family = binomial)#
summary(model_AI)#
#
# -------------------------------#
# 4. Decision Curve Analysis#
# -------------------------------#
#
# Define a function to compute net benefit given a set of predictions and the true expert classification.#
# NB = (TP/n) - (FP/n) * (pt/(1 - pt))#
compute_net_benefit <- function(predicted, expert_class, threshold, total_n) {#
  # predicted: binary vector of referral decisions (0/1)#
  # expert_class: vector with "Melanoma" or "Nevus" (truth)#
  TP <- sum(predicted == 1 & expert_class == "Melanoma")#
  FP <- sum(predicted == 1 & expert_class == "Nevus")#
  nb <- (TP / total_n) - (FP / total_n) * (threshold/(1 - threshold))#
  return(nb)#
}#
#
# Set up thresholds in decimal (0.1, 0.2, …, 0.9)#
thresholds <- seq(0.1, 0.9, by = 0.1)#
n_lesions <- nrow(df)#
#
# Initialize vectors to store net benefit for each strategy#
NB_expert <- numeric(length(thresholds))#
NB_AI     <- numeric(length(thresholds))#
#
# Loop over thresholds#
for (i in seq_along(thresholds)) {#
  pt <- thresholds[i]#
  # Expert-based strategy:#
  # For each lesion, use the proportion of opticians who would refer as the predicted referral probability.#
  # Apply the threshold: if proportion >= pt, then classify as referral.#
  expert_pred <- ifelse(df$Expert_proportion >= pt, 1, 0)#
  NB_expert[i] <- compute_net_benefit(expert_pred, df$Expert, pt, n_lesions)#
  # MelAInoma-based strategy:#
  # The decision is fixed (given by the AI: 0 or 1)#
  AI_pred <- df$MelAInoma#
  NB_AI[i] <- compute_net_benefit(AI_pred, df$Expert, pt, n_lesions)#
}#
#
# Create a table of net benefits#
decision_curve_table <- data.frame(#
  Threshold = thresholds * 100,  # convert to percentages#
  Expert_NB = NB_expert,#
  MelAInoma_NB = NB_AI#
)#
#
cat("Net Clinical Benefit at Various Threshold Probabilities:\n")#
print(decision_curve_table)#
#
# Optionally, you can save the table as a CSV file:#
# write.csv(decision_curve_table, "decision_curve_table.csv", row.names = FALSE)
# Load required packages#
library(readxl)    # For reading Excel files#
library(dplyr)     # For data manipulation#
library(tidyr)     # For reshaping data#
library(openxlsx)  # For exporting XLSX files#
#
# -------------------------------#
# 1. Read Data and Preprocess#
# -------------------------------#
#
# Change the file path if necessary#
file_path <- "/Users/gussta/Library/Mobile Documents/com~apple~CloudDocs/Forskning/Pågående pek/Synologenprojektet/Data/Would refer yes or no.xlsx"#
#
# Read the Excel file (assumes the data is in the first sheet)#
df <- read_excel(file_path)#
#
# Inspect the first few rows of the data#
head(df)#
#
# We assume the first column is "Lesion", the next 29 columns (columns 2 to 30) are the referral responses (0/1) from different opticians/optometrists,#
# and the last column (column 31) is "MelAInoma" (the AI-based decision).#
#
# Create expert-based lesion classification based on lesion number.#
# According to the provided information, lesions 4, 9, 13, 22 and 24 are melanomas.#
melanoma_ids <- c(4, 9, 13, 22, 24)#
df <- df %>%#
  mutate(Expert = ifelse(Lesion %in% melanoma_ids, "Melanoma", "Nevus"))#
#
# -------------------------------#
# 2. Expert Referral Decisions Analysis#
# -------------------------------#
#
# Calculate the total number of "would refer" votes for each lesion from the 29 raters.#
# Assuming that the columns for the raters are the 2nd through 30th columns.#
df <- df %>%#
  mutate(Expert_referrals = rowSums(select(., 2:30)),#
         Expert_proportion = Expert_referrals / 29)#
#
# Compute group-level referral counts:#
# For melanomas:#
melanoma_df <- df %>% filter(Expert == "Melanoma")#
nevus_df   <- df %>% filter(Expert == "Nevus")#
#
# Total decisions: each lesion is decided upon by 29 opticians/optometrists.#
total_melanoma_decisions <- nrow(melanoma_df) * 29#
total_nevus_decisions    <- nrow(nevus_df) * 29#
#
cat("Expert-based Decisions:\n")#
cat("Melanomas: ", referrals_melanoma <- sum(melanoma_df$Expert_referrals), " referrals out of ", total_melanoma_decisions,#
    sprintf(" (%.1f%%)\n", 100 * referrals_melanoma/total_melanoma_decisions))#
cat("Nevi: ", referrals_nevus <- sum(nevus_df$Expert_referrals), " referrals out of ", total_nevus_decisions,#
    sprintf(" (%.1f%%)\n\n", 100 * referrals_nevus/total_nevus_decisions))#
#
# Two-sample test for equality of proportions (Chi-squared test)#
prop_test_expert <- prop.test(x = c(referrals_melanoma, referrals_nevus),#
                                n = c(total_melanoma_decisions, total_nevus_decisions))#
print(prop_test_expert)#
#
# Reshape data to long format for logistic regression analysis.#
# Each row is a decision of one optician for one lesion.#
expert_long <- df %>%#
  pivot_longer(cols = 2:30, names_to = "Rater", values_to = "Referral") %>%#
  mutate(Referral = as.numeric(Referral),#
         Expert = factor(Expert, levels = c("Nevus", "Melanoma")))  # Set "Nevus" as the reference#
#
# Logistic regression: predicting referral decision based on expert classification.#
model_expert <- glm(Referral ~ Expert, data = expert_long, family = binomial)#
summary(model_expert)#
#
# -------------------------------#
# 3. MelAInoma-based Referral Decisions Analysis#
# -------------------------------#
#
# For the MelAInoma-based decision, each lesion has a single decision stored in the "MelAInoma" column.#
# Convert MelAInoma to numeric if needed.#
df$MelAInoma <- as.numeric(df$MelAInoma)#
#
# Count referrals by MelAInoma stratified by expert classification.#
referrals_melanoma_AI <- sum(df$MelAInoma[df$Expert == "Melanoma"])#
referrals_nevus_AI    <- sum(df$MelAInoma[df$Expert == "Nevus"])#
total_melanoma_AI     <- nrow(df %>% filter(Expert == "Melanoma"))#
total_nevus_AI        <- nrow(df %>% filter(Expert == "Nevus"))#
#
cat("MelAInoma-based Decisions:\n")#
cat("Melanomas: ", referrals_melanoma_AI, " referrals out of ", total_melanoma_AI,#
    sprintf(" (%.1f%%)\n", 100 * referrals_melanoma_AI/total_melanoma_AI))#
cat("Nevi: ", referrals_nevus_AI, " referrals out of ", total_nevus_AI,#
    sprintf(" (%.1f%%)\n\n", 100 * referrals_nevus_AI/total_nevus_AI))#
#
# Two-sample test for equality of proportions for MelAInoma-based decisions#
prop_test_AI <- prop.test(x = c(referrals_melanoma_AI, referrals_nevus_AI),#
                          n = c(total_melanoma_AI, total_nevus_AI))#
print(prop_test_AI)#
#
# Logistic regression for MelAInoma decisions (note: one decision per lesion)#
df$Expert <- factor(df$Expert, levels = c("Nevus", "Melanoma")) # Ensure correct reference order#
model_AI <- glm(MelAInoma ~ Expert, data = df, family = binomial)#
summary(model_AI)#
#
# -------------------------------#
# 4. Decision Curve Analysis#
# -------------------------------#
#
# Define a function to compute net benefit given a set of predictions and the true expert classification.#
# NB = (TP/n) - (FP/n) * (pt/(1 - pt))#
compute_net_benefit <- function(predicted, expert_class, threshold, total_n) {#
  # predicted: binary vector of referral decisions (0/1)#
  # expert_class: vector with "Melanoma" or "Nevus" (truth)#
  TP <- sum(predicted == 1 & expert_class == "Melanoma")#
  FP <- sum(predicted == 1 & expert_class == "Nevus")#
  nb <- (TP / total_n) - (FP / total_n) * (threshold/(1 - threshold))#
  return(nb)#
}#
#
# Set up thresholds in decimal (0.1, 0.2, …, 0.9)#
thresholds <- seq(0.1, 0.9, by = 0.1)#
n_lesions <- nrow(df)#
#
# Initialize vectors to store net benefit for each strategy#
NB_expert <- numeric(length(thresholds))#
NB_AI     <- numeric(length(thresholds))#
#
# Loop over thresholds#
for (i in seq_along(thresholds)) {#
  pt <- thresholds[i]#
  # Expert-based strategy:#
  # For each lesion, use the proportion of opticians who would refer as the predicted referral probability.#
  # Apply the threshold: if proportion >= pt, then classify as referral.#
  expert_pred <- ifelse(df$Expert_proportion >= pt, 1, 0)#
  NB_expert[i] <- compute_net_benefit(expert_pred, df$Expert, pt, n_lesions)#
  # MelAInoma-based strategy:#
  # The decision is fixed (given by the AI: 0 or 1)#
  AI_pred <- df$MelAInoma#
  NB_AI[i] <- compute_net_benefit(AI_pred, df$Expert, pt, n_lesions)#
}#
#
# Create a table of net benefits#
decision_curve_table <- data.frame(#
  Threshold = thresholds * 100,  # convert to percentages#
  Expert_NB = NB_expert,#
  MelAInoma_NB = NB_AI#
)#
#
cat("Net Clinical Benefit at Various Threshold Probabilities:\n")#
print(decision_curve_table)#
#
# -------------------------------#
# 5. Export Decision Curve Table to Desktop as XLSX#
# -------------------------------#
#
# Define the output file path to Desktop#
output_file <- file.path(Sys.getenv("HOME"), "Desktop", "decision_curve_table.xlsx")#
write.xlsx(decision_curve_table, file = output_file)#
cat("Decision curve table exported to: ", output_file, "\n")
# install.packages("pROC")#
library(pROC)#
#
# Suppose you have a data.frame `df` with:#
#   df$truth       : 0/1 (nevus=0, melanoma=1)#
#   df$mel_score   : MelAInoma score for each of the 25 cases#
#   df$moles_1 ... df$moles_29 : MOLES scores from the 29 raters#
#
# Create ROC objects#
roc_mel <- roc(df$truth, df$mel_score, ci=FALSE)#
#
# Function to run DeLong test against MelAInoma#
compare_rater <- function(rater_scores) {#
  roc_human <- roc(df$truth, rater_scores, ci=FALSE)#
  test   <- roc.test(roc_human, roc_mel, method="delong", paired=TRUE)#
  return(test$p.value)#
}#
#
# Apply to all 29 raters#
pvals <- sapply(1:29, function(i) {#
  compare_rater(df[[paste0("moles_", i)]])#
})#
#
# Assemble results#
results <- data.frame(#
  Rater        = paste0("Optician_", 1:29),#
  Human_AUC    = sapply(1:29, function(i) roc(df$truth, df[[paste0("moles_", i)]])$auc),#
  MelAUC       = as.numeric(roc_mel$auc),#
  p_value      = pvals,#
  p_adj_Bonf   = p.adjust(pvals, method="bonferroni")#
)#
#
print(results)
# install.packages("pROC")   # uncomment if needed#
library(pROC)#
#
# 1. Paste your data here as a multiline string.#
#    Columns are: truth (0=nevus,1=melanoma),#
#    mel_score (continuous), above_thresh (0/1), then moles_1…moles_29.#
txt <- "#
0 26 0 0 0 4 1 0 1 1 0 5 0 3 0 1 3 0 0 0 0 0 0 1 0 3 4 0 1 2 1 0#
0 63 0 1 3 4 5 6 3 2 3 6 3 8 2 6 4 4 6 2 1 2 5 4 4 2 5 3 2 4 2 2#
0 40 0 1 2 3 2 2 1 1 1 3 1 6 0 4 3 2 1 2 0 0 2 1 1 5 3 2 4 0 0 0#
1 90 1 1 4 8 5 5 1 2 2 8 2 10 2 6 5 3 5 4 0 0 4 6 2 4 6 8 5 5 1 1#
0 15 0 0 2 5 6 2 1 1 2 3 0 8 2 2 1 1 2 2 0 0 2 2 2 4 5 4 7 6 2 0#
0 48 0 0 3 3 3 1 0 1 1 4 1 6 0 2 3 2 3 2 0 2 3 1 0 2 6 2 6 3 1 0#
0 12 0 0 1 2 4 2 1 1 1 3 0 6 1 3 0 1 0 2 0 2 2 2 0 4 3 0 3 2 1 0#
0 44 0 2 4 4 5 1 1 1 0 6 0 5 1 2 0 0 0 2 0 1 4 2 0 4 3 4 5 4 1 3#
1 99 1 2 6 8 7 5 4 2 2 8 6 10 3 7 5 4 6 6 4 5 7 4 6 5 7 8 8 7 3 3#
0 12 0 0 6 4 3 3 1 2 0 4 0 6 3 3 3 2 0 3 0 2 2 1 2 4 6 6 5 0 2 0#
0 31 0 0 1 3 3 1 1 1 0 2 1 4 1 2 0 0 1 2 0 3 3 1 0 4 3 4 5 3 1 0#
0 33 0 2 4 6 3 3 1 1 2 7 2 10 2 5 4 2 2 2 1 2 4 2 5 3 2 2 6 3 3 1#
1 4  0 1 6 8 6 2 2 1 2 5 3 5 2 5 5 3 5 2 1 2 4 4 1 5 6 6 6 5 4 2#
0 10 0 2 6 7 5 4 2 1 2 7 2 7 2 9 2 3 5 4 1 4 4 4 4 3 7 6 8 6 3 3#
0 13 0 0 0 3 2 0 1 0 0 4 0 2 0 2 2 0 1 2 0 0 2 2 0 5 4 1 0 3 1 0#
0 26 0 1 2 5 3 4 1 1 3 6 3 6 2 6 4 3 5 3 1 2 4 2 1 3 6 6 2 4 3 1#
0 12 0 3 6 8 7 5 2 2 4 8 8 10 6 9 7 5 8 7 2 6 7 7 7 5 8 8 8 8 4 4#
0 47 0 2 5 6 6 3 3 1 2 7 3 10 4 7 7 3 7 7 2 3 5 3 4 5 7 8 8 8 3 3#
0 13 0 0 0 2 0 0 0 0 0 1 0 8 0 0 2 0 0 2 0 0 1 1 0 0 3 2 1 0 0 0#
0 70 1 2 4 3 1 1 1 1 1 5 0 6 0 3 4 2 3 3 0 0 3 2 0 2 5 4 3 2 1 2#
0 76 1 2 3 7 6 1 2 1 1 6 1 7 2 3 3 1 5 3 1 1 6 3 2 5 4 6 6 6 3 2#
1 88 1 1 5 4 4 1 1 2 0 5 3 8 4 5 4 4 2 6 2 1 4 1 1 5 3 2 5 8 1 4#
0 50 0 3 4 8 7 1 4 2 2 5 5 10 4 5 4 4 6 6 3 5 5 4 4 5 7 6 5 6 1 4#
1 82 1 4 6 9 7 3 4 3 5 8 6 10 6 10 6 5 9 6 4 5 6 8 7 5 8 8 8 8 6 4#
0 48 0 1 1 2 6 0 1 1 1 4 3 10 2 4 1 6 1 2 1 0 2 2 0 3 3 2 5 8 1 1#
"#
#
# 2. Read into a data.frame and name columns#
df <- read.table(text = txt, header = FALSE)#
colnames(df) <- c("truth", "mel_score", "above_th",#
                  paste0("moles_", 1:29))#
#
# 3. Make sure truth is a factor#
df$truth <- factor(df$truth, levels = c(0,1),#
                   labels = c("nevus","melanoma"))#
#
# 4. Compute the ROC objects and AUCs#
roc_mel  <- roc(df$truth, df$mel_score,  ci = FALSE)#
human_rocs <- lapply(1:29, function(i)#
  roc(df$truth, df[[paste0("moles_",i)]], ci = FALSE))#
human_auc  <- sapply(human_rocs, `[[`, "auc")#
#
# 5. DeLong’s paired test vs MelAInoma#
p_delong <- sapply(human_rocs,#
  function(rroc) roc.test(rroc, roc_mel,#
                         method="delong", paired=TRUE)$p.value)#
#
# 6. Bonferroni‑adjust#
p_adj    <- p.adjust(p_delong, method="bonferroni")#
#
# 7. Summarize#
res <- data.frame(#
  Rater     = paste0("Optician_", 1:29),#
  Human_AUC = round(human_auc,3),#
  MelAUC    = round(as.numeric(roc_mel$auc),3),#
  P_value   = signif(p_delong,3),#
  P_Bonf    = signif(p_adj,3)#
)#
print(res)#
#
# 8. (Optional) Wilcoxon signed‑rank test (paired) #
#    comparing the 29 human AUCs to the single MelAInoma AUC:#
wilcox.test(human_auc,#
            rep(as.numeric(roc_mel$auc), 29),#
            paired = TRUE)
# ---------- prerequisites ----------#
# install.packages("httr2")   # if not already present#
# install.packages("jsonlite")#
#
library(httr2)#
library(jsonlite)#
#
# ---------- 1) put your API key here ----------#
api_key <- Sys.getenv("WOS_API_KEY")  # or just#
#
# ---------- 2) helper: get Times Cited for one identifier ----------#
get_tc <- function(id, id_type = c("PMID", "DO")) {#
  id_type <- match.arg(id_type)#
  query <- paste0(id_type, ":", id)#
  req <- request(#
    paste0("https://api.clarivate.com/api/woslite/v1/articles?databaseId=WOK&usrQuery=", #
           URLencode(query), "&count=1&firstRecord=1")#
  ) |> #
    req_headers(`X-ApiKey` = api_key) |>#
    req_perform()#
  if (status_code(req) != 200) return(NA_integer_)#
  res <- resp_body_json(req, simplifyVector = TRUE)#
  if (length(res$`Data`$`Records`$`records`)) {#
    as.integer(res$`Data`$`Records`$`records`[[1]]$`dynamic_data`$`citation_related`$`tc_list`$`silo_tc`$`local_count`)#
  } else {#
    NA_integer_#
  }#
}#
#
# ---------- 3) main loop ----------#
pmids <- scan("pmid_list.txt", what = character())  # one PMID per line#
#
times_cited <- integer(length(pmids))#
for (i in seq_along(pmids)) {#
  tc <- get_tc(pmids[i], "PMID")#
  # fallback on DOI if missing#
  if (is.na(tc)) {#
    # retrieve DOI via NCBI E-utilities#
    efetch <- jsonlite::fromJSON(paste0(#
      "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?",#
      "db=pubmed&id=", pmids[i], "&retmode=json"#
    ))#
    doi <- unlist(efetch$result[[pmids[i]]]$articleids)#
    doi <- doi[grep("^10\\.", doi)][1]#
    if (!is.na(doi)) tc <- get_tc(doi, "DO")#
  }#
  times_cited[i] <- tc#
  message(sprintf("[%d/%d] PMID %s  →  TC = %s", #
                  i, length(pmids), pmids[i], times_cited[i]))#
}#
#
results <- data.frame(#
  PMID        = pmids,#
  TimesCited  = times_cited,#
  stringsAsFactors = FALSE#
)#
#
write.csv(results, "wos_times_cited.csv", row.names = FALSE)
# Install required packages if necessary#
if (!requireNamespace("httr2", quietly = TRUE)) {#
  install.packages("httr2")#
}#
if (!requireNamespace("jsonlite", quietly = TRUE)) {#
  install.packages("jsonlite")#
}#
#
library(httr2)#
library(jsonlite)#
#
# 1) Set your Web of Science API key (store in environment or paste here)#
api_key <- Sys.getenv("WOS_API_KEY")#
if (identical(api_key, "")) {#
  stop("Please set your WOS_API_KEY environment variable, e.g.\n  Sys.setenv(WOS_API_KEY='your_api_key')")#
}#
#
# 2) Read the list of PMIDs (one per line) from pmid_list.txt in your working directory#
pmid_file <- "pmid_list.txt"#
if (!file.exists(pmid_file)) {#
  stop(sprintf("File '%s' not found. Create it with one PMID per line.", pmid_file))#
}#
pmids <- trimws(readLines(pmid_file))#
if (length(pmids) == 0) stop("No PMIDs found in pmid_list.txt")#
#
# 3) Helper function: query Times Cited for a PMID or DOI#
get_tc <- function(id, id_type = c("PMID", "DO")) {#
  id_type <- match.arg(id_type)#
  q <- paste0(id_type, ":", id)#
  url <- paste0(#
    "https://api.clarivate.com/api/woslite/v1/articles?",#
    "databaseId=WOK&usrQuery=", URLencode(q),#
    "&count=1&firstRecord=1"#
  )#
  res <- request(url) |>#
    req_headers(`X-ApiKey` = api_key) |>#
    req_perform()#
  if (res$status_code != 200) return(NA_integer_)#
  body <- resp_body_json(res, simplifyVector = TRUE)#
  recs <- body$Data$Records$records#
  if (length(recs) == 0) return(NA_integer_)#
  silo <- recs[[1]]$dynamic_data$citation_related$tc_list$silo_tc#
  as.integer(silo$local_count)#
}#
#
# 4) Loop over PMIDs, falling back to DOI lookup if needed#
times_cited <- integer(length(pmids))#
for (i in seq_along(pmids)) {#
  pmid <- pmids[i]#
  tc <- get_tc(pmid, "PMID")#
  if (is.na(tc)) {#
    # fetch DOI via NCBI E-utilities#
    esum <- fromJSON(paste0(#
      "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?",#
      "db=pubmed&id=", pmid, "&retmode=json"#
    ))#
    ids <- esum$result[[pmid]]$articleids#
    doi <- ids[grep("^10\\.", ids)][1]#
    if (!is.na(doi)) {#
      tc <- get_tc(doi, "DO")#
    }#
  }#
  times_cited[i] <- tc#
  message(sprintf("[%d/%d] PMID %s → Times Cited: %s",#
                  i, length(pmids), pmid,#
                  ifelse(is.na(tc), "NA", tc)))#
}#
#
# 5) Compile and save results#
results <- data.frame(#
  PMID       = pmids,#
  TimesCited = times_cited,#
  stringsAsFactors = FALSE#
)#
write.csv(results, "wos_times_cited.csv", row.names = FALSE)#
message("Saved citation counts to wos_times_cited.csv")
install.packages(c("devtools", "roxygen2"))#
setwd("path/to/EARS")#
devtools::document()#
devtools::install()#
library(EARS)
# If you don’t already have one, this creates ~/.Rprofile#
if (!file.exists("~/.Rprofile")) file.create("~/.Rprofile")#
#
# Append the Umeå mirror setting#
cat(#
  "# Use CRAN mirror at Umeå, Sweden\n",#
  "options(repos = c(CRAN = \"https://cran.its.umu.se/\"))\n",#
  file = "~/.Rprofile",#
  append = TRUE#
)#
#
# Reload .Rprofile to apply immediately#
source("~/.Rprofile")
library(EARS)#
#
df2 <- data.frame(#
  grp      = rep(c("A","B"), each=5),#
  duration = 1:10,#
  status   = sample(0:1, 10, TRUE)#
)#
#
res2 <- ears_test(df2, "Group", "Time", "Event")#
# You should see exactly three prompts like the one above,#
# then immediately:#
# Event-Adjusted Rank Sum (EARS) test with censoring-adjusted P value#
# Kruskal-Wallis chi-squared = X.XX#
# Adjusted P value = Y.YY
library(EARS)#
#
df2 <- data.frame(#
  grp      = rep(c("A","B"), each=5),#
  duration = 1:10,#
  status   = sample(0:1, 10, TRUE)#
)#
#
# This will now show exactly one prompt per missing arg:#
res2 <- ears_test(df2, "Group", "Time", "Event")#
# Then immediately:#
# Event-Adjusted Rank Sum (EARS) test with censoring-adjusted P value#
# Kruskal-Wallis chi-squared = X.XX#
# Adjusted P value = Y.YY
pkg_dir <- path.expand("~/Desktop/EARS")#
setwd(pkg_dir)#
#
# regenerate man/ and NAMESPACE#
roxygen2::roxygenise(pkg_dir)#
#
# reinstall the package#
devtools::install(pkg_dir)
